import asyncio
import functools
import logging
import multiprocessing
import os
import signal
import socket
import sys  # Added import for sys.exit
import time
import traceback
import uuid
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from dataclasses import dataclass, field
from datetime import datetime, timezone, timedelta
from enum import Enum, auto
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Set, Union, Tuple
from collections import defaultdict
import threading
from contextlib import contextmanager
import random
import asyncio # Ensure asyncio is imported for PriorityQueue

import networkx as nx
import psutil
import sqlalchemy as sa
from sqlalchemy.ext.asyncio import AsyncSession

from ..database.db import get_db_session
from ..database.models import StatusEnum
from ..database.models import Workflow as WorkflowModel
from ..database.models import WorkflowRun as WorkflowRunModel
from ..database.models import WorkflowStepDefinition as WorkflowStepDefinitionModel
from ..database.models import WorkflowStepRun as WorkflowStepRunModel
from .distributed import DistributedTaskManager
from .metrics_collector import MetricsCollector

logger = logging.getLogger(__name__)


class StepExecutionStatus(Enum):
    """Status of a workflow step execution"""

    PENDING = auto()
    QUEUED = auto()
    RUNNING = auto()
    COMPLETED = auto()
    FAILED = auto()
    SKIPPED = auto()
    CANCELLED = auto()
    TIMEOUT = auto()

    def to_db_status(self) -> StatusEnum:
        """Convert manager's StepExecutionStatus to database's StatusEnum"""
        try:
            return StatusEnum[self.name]
        except KeyError:
            logger.warning(f"Unsupported StepExecutionStatus for DB conversion: {self.name}. Defaulting to FAILED.")
            return StatusEnum.FAILED

    @staticmethod
    def from_db_status(db_status: StatusEnum) -> "StepExecutionStatus":
        """Convert database's StatusEnum to manager's StepExecutionStatus"""
        try:
            return StepExecutionStatus[db_status.name]
        except KeyError:
            logger.warning(
                f"Unsupported DB StatusEnum for manager conversion: {db_status.name}. Defaulting to FAILED."
            )
            return StepExecutionStatus.FAILED


@dataclass
class WorkflowContext:
    """Context for a workflow execution with enhanced metadata"""

    workflow_db_id: uuid.UUID
    run_db_id: uuid.UUID
    workflow_id: str
    run_id: str
    parameters: Dict[str, Any] = field(default_factory=dict)
    results: Dict[str, Any] = field(default_factory=dict)
    artifacts: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    logs: List[str] = field(default_factory=list)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    status: str = "PENDING"
    error: Optional[str] = None

    # Enhanced fields for better tracking
    resource_usage: Dict[str, List[float]] = field(
        default_factory=lambda: {"cpu_percent": [], "memory_percent": [], "memory_mb": [], "timestamp": []}
    )
    step_statuses: Dict[str, StepExecutionStatus] = field(default_factory=dict)
    step_times: Dict[str, Dict[str, float]] = field(default_factory=dict)

    def add_result(self, key: str, value: Any) -> None:
        """Add a result to the context"""
        self.results[key] = value

    def add_artifact(self, key: str, path: str) -> None:
        """Add an artifact path to the context"""
        self.artifacts[key] = path

    def get_result(self, key: str, default: Any = None) -> Any:
        """Get a result from the context"""
        return self.results.get(key, default)

    def get_artifact(self, key: str, default: str = None) -> str:
        """Get an artifact path from the context"""
        return self.artifacts.get(key, default)

    def add_log(self, message: str) -> None:
        """Add a log message to the context"""
        timestamp = datetime.now().isoformat()
        self.logs.append(f"[{timestamp}] {message}")

    def update_resource_usage(self) -> None:
        """Update resource usage statistics"""
        try:
            # Get current process
            process = psutil.Process(os.getpid())

            # Add metrics
            self.resource_usage["cpu_percent"].append(process.cpu_percent())
            memory_info = process.memory_info()
            memory_percent = process.memory_percent()
            self.resource_usage["memory_percent"].append(memory_percent)
            self.resource_usage["memory_mb"].append(memory_info.rss / (1024 * 1024))
            self.resource_usage["timestamp"].append(time.time())
        except Exception as e:
            logger.warning(f"Could not update resource usage: {str(e)}")

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        serializable_step_statuses = {step_id: status.name for step_id, status in self.step_statuses.items()}
        return {
            "workflow_db_id": str(self.workflow_db_id),
            "run_db_id": str(self.run_db_id),
            "workflow_id": self.workflow_id,
            "run_id": self.run_id,
            "parameters": self.parameters,
            "results": self.results,
            "artifacts": self.artifacts,
            "metadata": self.metadata,
            "logs": self.logs,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "status": self.status,
            "error": self.error,
            "resource_usage": self.resource_usage,
            "step_statuses": serializable_step_statuses,
            "step_times": self.step_times,
        }


@dataclass
class WorkflowStepDefinition:
    """A step definition in a workflow with enhanced capabilities"""

    id: str
    name: str
    description: str
    function_identifier: str
    parameters: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)
    timeout_seconds: Optional[int] = None
    retry_count: int = 0
    retry_delay_seconds: int = 60
    priority: int = 0
    estimated_memory_mb: int = 1000
    use_gpu: bool = False
    use_process: bool = False
    _resolved_function: Optional[Callable] = field(default=None, repr=False, compare=False)


class ExecutionMode(Enum):
    """Execution modes for the workflow manager"""

    SEQUENTIAL = auto()  # Execute steps one after another
    PARALLEL = auto()  # Execute steps in parallel when possible
    DISTRIBUTED = auto()  # Execute steps across distributed workers


class WorkflowManager:
    """Enhanced workflow manager with distributed execution capabilities and DB integration"""

    def __init__(
        self,
        execution_mode: ExecutionMode = ExecutionMode.PARALLEL,
        max_workers: int = None,
        metrics_collector: Optional[MetricsCollector] = None,
        distributed_manager: Optional[DistributedTaskManager] = None,
        resource_monitor_interval: int = 5, # Interval for general resource monitoring
        gpu_devices: List[int] = None,
        workflow_dir: str = "./workflows_runtime_artifacts",
        enable_caching: bool = True,
        enable_prometheus: bool = False, 
        prometheus_port: int = 8000,
        use_optimization: bool = True,
        # New parameters for resource-aware scheduling
        max_queued_tasks_per_run: int = 10, # Max tasks a single workflow run can queue waiting for resources
        system_cpu_threshold_percent: float = 85.0, # System CPU threshold for queuing
        system_memory_threshold_percent: float = 90.0, # System memory threshold for queuing
        resource_check_interval_seconds: float = 2.0, # How often to check resource-limited queue
    ):
        """
        Initialize an enhanced workflow manager with extensive options

        Args:
            execution_mode: Mode of execution (SEQUENTIAL, PARALLEL, DISTRIBUTED)
            max_workers: Maximum number of parallel workers, defaults to CPU count
            metrics_collector: Optional metrics collector for monitoring
            distributed_manager: Optional distributed task manager for remote execution
            resource_monitor_interval: Interval in seconds for monitoring resource usage
            gpu_devices: List of GPU device IDs to use
            workflow_dir: Directory to store temporary workflow runtime artifacts (logs, temp files not in DB).
        """
        # Enhanced initialization with validation
        self._validate_initialization(execution_mode, max_workers, resource_monitor_interval)
        self._setup_telemetry()
        self._configure_heartbeat()
        
        # Add validation
        if max_workers is not None and max_workers <= 0:
            raise ValueError("max_workers must be positive")
            
        if resource_monitor_interval < 1:
            raise ValueError("resource_monitor_interval must be at least 1 second")
            
        # Enhanced initialization
        self._setup_directories(workflow_dir)
        self._validate_gpu_devices(gpu_devices)
        
        self.execution_mode = execution_mode
        self.max_workers = max_workers or min(32, multiprocessing.cpu_count() if multiprocessing.cpu_count() else 1)
        self.metrics_collector = metrics_collector
        self.distributed_manager = distributed_manager
        self.resource_monitor_interval = resource_monitor_interval
        self.gpu_devices = gpu_devices or []
        self.workflow_artifacts_dir = Path(workflow_dir)
        self.workflow_artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        # Store configuration options
        self.enable_caching = enable_caching
        self.enable_prometheus = enable_prometheus
        self.prometheus_port = prometheus_port
        self.use_optimization = use_optimization

        # Resource-aware scheduling config
        self.max_queued_tasks_per_run = max_queued_tasks_per_run
        self.system_cpu_threshold_percent = system_cpu_threshold_percent
        self.system_memory_threshold_percent = system_memory_threshold_percent
        self.resource_check_interval_seconds = resource_check_interval_seconds

        self.thread_executor: Optional[ThreadPoolExecutor] = None
        self.process_executor: Optional[ProcessPoolExecutor] = None
        # Active tasks in executors, mapping future to (run_db_id, step_id)
        self._active_executor_tasks: Dict[asyncio.Future, Tuple[uuid.UUID, str]] = {} 


        self.registered_workflows: Dict[str, WorkflowModel] = {}
        self.workflow_definitions_cache: Dict[uuid.UUID, List[WorkflowStepDefinitionModel]] = {}

        self.step_pid_map: Dict[str, int] = {}

        self.available_memory_mb = psutil.virtual_memory().total / (1024 * 1024) if psutil.virtual_memory() else 0
        self.available_gpu_memory = self._get_available_gpu_memory()

        self._function_resolver: Optional[Callable[[str], Callable]] = None

        self._setup_monitoring()
        
        # Initialize GPU scheduler for advanced GPU management
        if self.gpu_devices:
            self._setup_gpu_scheduler()
            
        # Setup Prometheus metrics if enabled
        if getattr(self, 'enable_prometheus', False):
            self.setup_prometheus_metrics()

        # Resource-limited queue for steps waiting for system resources
        # Stores tuples: (priority, timestamp, step_id, step_dataclass, context_repr, future, loop)
        # Timestamp is used as a tie-breaker for priority queue if priorities are equal
        self._resource_limited_queues: Dict[uuid.UUID, asyncio.PriorityQueue] = defaultdict(asyncio.PriorityQueue)
        self._resource_queue_processor_task: Optional[asyncio.Task] = None
        if self.execution_mode == ExecutionMode.PARALLEL: # Or always start it if it handles distributed too
            # Ensure an event loop is running or get the current one if in async context
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError: # No running event loop
                # This case is tricky. If WorkflowManager is initialized outside an async context,
                # creating tasks needs careful handling. For now, assume it's in an async context
                # or that the user will manage the loop for starting the processor.
                # A more robust solution might involve a dedicated thread for the asyncio loop
                # if the manager can be used in a purely synchronous way.
                logger.warning("WorkflowManager initialized outside a running asyncio event loop. "
                               "Resource queue processor might not start automatically if not in async context.")
                loop = asyncio.new_event_loop() # Temporary loop, might not be ideal
                asyncio.set_event_loop(loop)


            self._resource_queue_processor_task = asyncio.create_task(self._process_resource_limited_queues())
            logger.info("Resource-limited queue processor task created.")


        logger.info(
            f"Initialized workflow manager in {self.execution_mode.name} mode with {self.max_workers} workers."
        )
        logger.info(f"Workflow runtime artifacts will be stored in: {self.workflow_artifacts_dir}")
        if self.gpu_devices:
            logger.info(f"Using GPU devices: {self.gpu_devices}")

        # Add new performance tracking
        self.performance_stats = {
            'total_workflows': 0,
            'successful_workflows': 0,
            'failed_workflows': 0,
            'avg_duration': 0,
            'resource_usage': []
        }

    def _validate_initialization(self, execution_mode, max_workers, monitor_interval):
        """Validate all initialization parameters with precise checks"""
        if not isinstance(execution_mode, ExecutionMode):
            raise TypeError(f"execution_mode must be ExecutionMode enum, got {type(execution_mode)}")
        if max_workers is not None and (not isinstance(max_workers, int) or max_workers <= 0):
            raise ValueError("max_workers must be positive integer or None")
        if not isinstance(monitor_interval, (int, float)) or monitor_interval < 0.1:
            raise ValueError("resource_monitor_interval must be number >= 0.1")

    def _setup_telemetry(self):
        """Configure detailed performance telemetry"""
        self.telemetry = {
            'executions': [],
            'resource_usage': [],
            'step_performance': defaultdict(list),
            'error_stats': defaultdict(int)
        }
        self._last_telemetry_flush = time.time()
        
    def _configure_heartbeat(self):
        """Setup periodic health reporting"""
        self.heartbeat_interval = 60  # seconds
        self._heartbeat_thread = threading.Thread(
            target=self._run_heartbeat,
            daemon=True,
            name="WorkflowManagerHeartbeat"
        )
        self._heartbeat_thread.start()

    def _run_heartbeat(self):
        """Continuous health monitoring"""
        while True:
            try:
                self._report_health()
                time.sleep(self.heartbeat_interval)
            except Exception as e:
                logger.error(f"Heartbeat error: {str(e)}", exc_info=True)
                time.sleep(min(5, self.heartbeat_interval))  # Brief pause after error

    def _report_health(self):
        """Generate comprehensive health report"""
        report = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'active_workflows': len(self.active_workflows),
            'resource_usage': self._get_current_resources(),
            'performance': self._calculate_performance_metrics(),
            'errors': dict(self.telemetry['error_stats'])
        }
        if self.metrics_collector:
            self.metrics_collector.gauge('active_workflows', report['active_workflows'])
            self.metrics_collector.record('health_report', report)

    def set_function_resolver(self, resolver: Callable[[str], Callable]):
        """Set the function to resolve string identifiers to callable functions."""
        self._function_resolver = resolver

    def _resolve_function(self, identifier: str) -> Callable:
        """Resolve a string identifier to a callable function."""
        if not self._function_resolver:
            raise ValueError("Function resolver has not been set. Use set_function_resolver().")
        try:
            return self._function_resolver(identifier)
        except Exception as e:
            logger.error(f"Failed to resolve function identifier '{identifier}': {e}")
            raise

    def _setup_monitoring(self):
        """Set up resource monitoring and signal handling"""
        # Set up signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._handle_shutdown)
        signal.signal(signal.SIGTERM, self._handle_shutdown)

        # Start resource monitoring if enabled
        if self.resource_monitor_interval > 0:
            self._start_resource_monitor()

    def _start_resource_monitor(self):
        """Start a background thread for resource monitoring"""
        import threading

        def monitor_resources():
            while True:
                try:
                    # Update available resources
                    self.available_memory_mb = psutil.virtual_memory().available / (1024 * 1024)
                    self.available_gpu_memory = self._get_available_gpu_memory()

                    # Update resource usage for active workflows
                    # This needs to be adapted for async context if get_workflow_status becomes async
                    # For now, this is a placeholder for how it might be triggered.
                    # active_run_ids = [] # Logic to get active run_ids from DB or internal tracking needed
                    # for run_id in active_run_ids:
                    #     asyncio.run(self.get_workflow_status(run_id)) # This is problematic in a sync signal handler

                    # Report metrics if collector is available
                    if self.metrics_collector:
                        self.metrics_collector.gauge("workflow_manager_available_memory_mb", self.available_memory_mb)
                        self.metrics_collector.gauge(
                            "workflow_manager_active_workflows", len(self.registered_workflows)
                        )

                    time.sleep(self.resource_monitor_interval)
                except Exception as e:
                    logger.error(f"Error in resource monitor: {str(e)}")
                    time.sleep(self.resource_monitor_interval)

        monitor_thread = threading.Thread(target=monitor_resources, daemon=True)
        monitor_thread.start()
        logger.info(f"Started resource monitoring thread with interval {self.resource_monitor_interval}s")

    def _get_available_gpu_memory(self) -> Dict[int, int]:
        """Get available GPU memory in MB for each device"""
        gpu_memory = {}

        if not self.gpu_devices:
            return gpu_memory

        try:
            try:
                import torch
            except ImportError:
                logger.warning("PyTorch is not installed. GPU memory monitoring disabled.")
                return gpu_memory

            if torch.cuda.is_available():
                for device_id in self.gpu_devices:
                    # Get device properties
                    props = torch.cuda.get_device_properties(device_id)
                    # Get memory usage
                    torch.cuda.set_device(device_id)
                    torch.cuda.empty_cache()
                    allocated = torch.cuda.memory_allocated(device_id) / (1024 * 1024)
                    total = props.total_memory / (1024 * 1024)
                    available = total - allocated
                    gpu_memory[device_id] = available
        except Exception as e:
            logger.warning(f"Could not get GPU memory: {str(e)}")

        return gpu_memory
    def _handle_shutdown(self, signum, frame):
        logger.info(f"Shutdown signal {signum} received. Cleaning up...")

        if self.thread_executor:
            self.thread_executor.shutdown(wait=False, cancel_futures=True)
        if self.process_executor:
            self.process_executor.shutdown(wait=False, cancel_futures=True)

        if self._resource_queue_processor_task and not self._resource_queue_processor_task.done():
            self._resource_queue_processor_task.cancel()
            logger.info("Cancelled resource queue processor task.")
            # Optionally, await the task with a timeout to allow cleanup,
            # but signal handlers should be quick.
            # try:
            #     await asyncio.wait_for(self._resource_queue_processor_task, timeout=5.0)
            # except (asyncio.CancelledError, asyncio.TimeoutError):
            #     pass


        logger.info("Workflow manager shutdown complete.")
        sys.exit(0)

    async def register_workflow_definition(
        self,
        name: str,
        steps_definitions: List[WorkflowStepDefinition],
        version: str = "1.0.0",
        description: Optional[str] = None,
        default_parameters: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
        owner_id: Optional[str] = None,
        enabled: bool = True,
    ) -> uuid.UUID:
        """
        Registers a new workflow definition in the database or updates an existing one
        based on name and version.
        """
        if not steps_definitions:
            raise ValueError("Workflow must have at least one step definition.")

        async with get_db_session() as session:
            existing_workflow_stmt = sa.select(WorkflowModel).where(
                WorkflowModel.name == name, WorkflowModel.version == version
            )
            result = await session.execute(existing_workflow_stmt)
            workflow_db_model = result.scalar_one_or_none()

            if workflow_db_model:
                workflow_db_model.description = description
                workflow_db_model.step_count = len(steps_definitions)
                workflow_db_model.default_parameters = default_parameters
                workflow_db_model.tags = tags
                workflow_db_model.owner_id = owner_id
                workflow_db_model.enabled = enabled
                workflow_db_model.updated_at = datetime.now(timezone.utc)
                logger.info(f"Updating existing workflow definition: {name} v{version} (ID: {workflow_db_model.id})")

                delete_steps_stmt = sa.delete(WorkflowStepDefinitionModel).where(
                    WorkflowStepDefinitionModel.workflow_id == workflow_db_model.id
                )
                await session.execute(delete_steps_stmt)
            else:
                workflow_db_model = WorkflowModel(
                    name=name,
                    description=description,
                    version=version,
                    step_count=len(steps_definitions),
                    enabled=enabled,
                    default_parameters=default_parameters,
                    tags=tags,
                    owner_id=owner_id,
                )
                session.add(workflow_db_model)
                await session.flush()
                logger.info(f"Registered new workflow definition: {name} v{version} (ID: {workflow_db_model.id})")

            db_step_definitions = []
            for _i, step_def_dataclass in enumerate(steps_definitions):
                if not step_def_dataclass.function_identifier:
                    raise ValueError(f"Step '{step_def_dataclass.name}' must have a function_identifier.")

                db_step = WorkflowStepDefinitionModel(
                    workflow_id=workflow_db_model.id,
                    step_id_in_workflow=step_def_dataclass.id,
                    name=step_def_dataclass.name,
                    description=step_def_dataclass.description,
                    function_identifier=step_def_dataclass.function_identifier,
                    default_parameters=step_def_dataclass.parameters,
                    dependencies=step_def_dataclass.dependencies,
                    retry_count=step_def_dataclass.retry_count,
                    timeout_seconds=step_def_dataclass.timeout_seconds,
                    estimated_memory_mb=step_def_dataclass.estimated_memory_mb,
                    estimated_duration_seconds=None,
                    priority=step_def_dataclass.priority,
                    use_gpu=step_def_dataclass.use_gpu,
                )
                db_step_definitions.append(db_step)

            session.add_all(db_step_definitions)
            await session.commit()

            self.registered_workflows[f"{name}__{version}"] = workflow_db_model
            self.workflow_definitions_cache[workflow_db_model.id] = db_step_definitions

            return workflow_db_model.id

    async def get_workflow_definition_models(
        self, workflow_identifier: Union[str, uuid.UUID], version: Optional[str] = None
    ) -> Optional[WorkflowModel]:
        """Retrieve a workflow definition model from DB by its ID or name/version."""
        async with get_db_session() as session:
            if isinstance(workflow_identifier, uuid.UUID):
                stmt = sa.select(WorkflowModel).where(WorkflowModel.id == workflow_identifier)
            elif version:  # Name and version provided
                stmt = sa.select(WorkflowModel).where(
                    WorkflowModel.name == workflow_identifier, WorkflowModel.version == version
                )
            else:  # Only name provided, try to get the latest or a default version (e.g., enabled with highest version number)
                # This logic might need refinement based on how versions are handled (e.g., 'latest' tag or semantic versioning order)
                stmt = (
                    sa.select(WorkflowModel)
                    .where(WorkflowModel.name == workflow_identifier, WorkflowModel.enabled)
                    .order_by(WorkflowModel.version.desc())
                )

            result = await session.execute(stmt)
            workflow_db_model = result.scalar_one_or_none()
            return workflow_db_model

    async def get_step_definition_models(self, workflow_db_id: uuid.UUID, session: AsyncSession = None) -> List[WorkflowStepDefinitionModel]:
        """
        Retrieve step definition models for a given workflow DB ID.
        Accepts an optional session parameter for reusing an existing database session.
        """
        # Check cache first for better performance
        if workflow_db_id in self.workflow_definitions_cache:
            return self.workflow_definitions_cache[workflow_db_id]

        # Use provided session or create a new one
        if session is not None:
            # Use existing session
            stmt = (
                sa.select(WorkflowStepDefinitionModel)
                .where(WorkflowStepDefinitionModel.workflow_id == workflow_db_id)
                .order_by(WorkflowStepDefinitionModel.id)
            )
            result = await session.execute(stmt)
            step_definitions_db = result.scalars().all()
            self.workflow_definitions_cache[workflow_db_id] = step_definitions_db
            return step_definitions_db
        else:
            # Create new session
            async with get_db_session() as new_session:
                stmt = (
                    sa.select(WorkflowStepDefinitionModel)
                    .where(WorkflowStepDefinitionModel.workflow_id == workflow_db_id)
                    .order_by(WorkflowStepDefinitionModel.id)
                )
                result = await new_session.execute(stmt)
                step_definitions_db = result.scalars().all()
                self.workflow_definitions_cache[workflow_db_id] = step_definitions_db
                return step_definitions_db

    async def create_workflow_run(
        self,
        workflow_identifier: Union[str, uuid.UUID],  # Name or DB ID of the workflow definition
        parameters: Dict[str, Any],
        version: Optional[str] = None,  # Specify if workflow_identifier is a name
        triggered_by: Optional[str] = None,
        custom_metadata: Optional[Dict[str, Any]] = None,
    ) -> uuid.UUID:  # Returns the DB ID of the WorkflowRun
        """Creates a new workflow run record in the database."""
        async with get_db_session() as session:
            workflow_db_model = await self.get_workflow_definition_models(workflow_identifier, version)
            if not workflow_db_model:
                id_str = str(workflow_identifier) + (f" v{version}" if version else "")
                raise ValueError(f"Workflow definition '{id_str}' not found or not unique.")

            run_db_model = WorkflowRunModel(
                workflow_id=workflow_db_model.id,
                parameters=parameters,
                status=StatusEnum.PENDING,
                triggered_by=triggered_by,
                custom_metadata=custom_metadata,
                # created_at, updated_at by TimestampMixin. id by default.
            )
            session.add(run_db_model)
            await session.commit()
            await session.refresh(run_db_model)  # To get the generated ID and defaults
            logger.info(
                f"Created workflow run with DB ID: {run_db_model.id} for workflow '{workflow_db_model.name}' v{workflow_db_model.version}"
            )
            return run_db_model.id

    def _build_workflow_graph(self, steps_definitions_dataclasses: List[WorkflowStepDefinition]) -> nx.DiGraph:
        """Builds a directed acyclic graph (DAG) from workflow step dataclasses."""
        G = nx.DiGraph()
        step_map = {step.id: step for step in steps_definitions_dataclasses}

        for step_dataclass in steps_definitions_dataclasses:
            G.add_node(step_dataclass.id, step_dataclass=step_dataclass)

        for step_dataclass in steps_definitions_dataclasses:
            for dep_id in step_dataclass.dependencies:
                if dep_id not in step_map:
                    raise ValueError(f"Step '{step_dataclass.id}' has an unknown dependency: '{dep_id}'.")
                G.add_edge(dep_id, step_dataclass.id)

        if not nx.is_directed_acyclic_graph(G):
            cycles = list(nx.simple_cycles(G))
            raise ValueError(f"Workflow contains cyclic dependencies: {cycles}")
        return G

    async def execute_workflow(self, run_db_id: uuid.UUID, use_cache: bool = None) -> WorkflowContext:
        """Enhanced execution with detailed performance tracking and optional caching"""
        start_time = time.time()
        
        # Determine if caching should be used
        if use_cache is None:
            use_cache = self.enable_caching
        
        try:
            # Track resource usage at start
            initial_resources = self._get_system_resources()
            
            async with get_db_session() as session:  # type: AsyncSession
                run_db_model = await session.get(WorkflowRunModel, run_db_id)
                if not run_db_model:
                    raise ValueError(f"Workflow run with DB ID '{run_db_id}' not found.")

                if run_db_model.status not in [
                    StatusEnum.PENDING,
                    StatusEnum.QUEUED,
                    StatusEnum.FAILED,
                ]:  # Allow re-running FAILED ones
                    logger.warning(
                        f"Workflow run '{run_db_id}' has status {run_db_model.status.name} and cannot be started."
                    )
                    # Potentially return a context based on the current DB state or raise error
                    # For now, let's create a basic context and return
                    workflow_def_model = await session.get(WorkflowModel, run_db_model.workflow_id)
                    return WorkflowContext(
                        workflow_db_id=run_db_model.workflow_id,
                        run_db_id=run_db_model.id,
                        workflow_id=workflow_def_model.name if workflow_def_model else "UnknownWorkflow",
                        run_id=str(run_db_model.id),  # Using DB ID as the run_id for context if not otherwise set
                        parameters=run_db_model.parameters or {},
                        status=StepExecutionStatus.from_db_status(run_db_model.status).name,
                        start_time=run_db_model.started_at,
                        end_time=run_db_model.completed_at,
                        error=run_db_model.error_message,
                    )

                workflow_db_model = await session.get(WorkflowModel, run_db_model.workflow_id)
                if not workflow_db_model:
                    error_msg = f"Workflow definition (ID: {run_db_model.workflow_id}) associated with run '{run_db_id}' not found."
                    run_db_model.status = StatusEnum.FAILED
                    run_db_model.error_message = error_msg
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    await session.commit()
                    raise ValueError(error_msg)

                step_definitions_db = await self.get_step_definition_models(workflow_db_model.id)
                if not step_definitions_db:
                    error_msg = (
                        f"No step definitions found for workflow '{workflow_db_model.name}' (ID: {workflow_db_model.id})."
                    )
                    run_db_model.status = StatusEnum.FAILED
                    run_db_model.error_message = error_msg
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    await session.commit()
                    raise ValueError(error_msg)

                # Convert DB step definition models to manager's WorkflowStepDefinition dataclasses
                steps_dataclasses = []
                for step_db in step_definitions_db:
                    steps_dataclasses.append(
                        WorkflowStepDefinition(
                            id=step_db.step_id_in_workflow,
                            name=step_db.name,
                            description=step_db.description or "",
                            function_identifier=step_db.function_identifier or "",  # Ensure not None
                            parameters=step_db.default_parameters or {},
                            dependencies=step_db.dependencies or [],
                            timeout_seconds=step_db.timeout_seconds,
                            retry_count=step_db.retry_count or 0,
                            priority=step_db.priority or 0,
                            estimated_memory_mb=step_db.estimated_memory_mb or 1000,
                            use_gpu=step_db.use_gpu or False,
                            # _resolved_function is set later before execution
                        )
                    )

                # Build workflow graph using the dataclasses
                try:
                    graph = self._build_workflow_graph(steps_dataclasses)
                except ValueError as e:
                    error_msg = f"Failed to build workflow graph for run '{run_db_id}': {e}"
                    run_db_model.status = StatusEnum.FAILED
                    run_db_model.error_message = error_msg
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    await session.commit()
                    raise ValueError(error_msg) from e

                # Initialize WorkflowContext for this run
                # The `run_id` in context was originally a string; here we use the DB UUID string.
                # The `workflow_id` in context was also a string; using the workflow definition name.
                context = WorkflowContext(
                    workflow_db_id=workflow_db_model.id,
                    run_db_id=run_db_model.id,
                    workflow_id=workflow_db_model.name,
                    run_id=str(run_db_model.id),
                    parameters=run_db_model.parameters or {},
                    metadata=run_db_model.custom_metadata or {},
                    start_time=datetime.now(timezone.utc),
                    status=StepExecutionStatus.RUNNING.name,  # Set context status
                )

                # Update DB: WorkflowRun status to RUNNING and started_at
                run_db_model.status = StatusEnum.RUNNING
                run_db_model.started_at = context.start_time
                await session.commit()

                logger.info(
                    f"Executing workflow run {run_db_id} for workflow '{workflow_db_model.name}' (Definition ID: {workflow_db_model.id})"
                )

                # Make step_dataclasses accessible to execution methods, perhaps by passing directly
                # or by associating them with the context in a structured way if context is passed around.
                # For now, passing them directly to the execution strategy methods.
                step_dataclasses_map = {s.id: s for s in steps_dataclasses}

                # Fetch workflow-level default parameters
                workflow_default_params = workflow_db_model.default_parameters or {}
                context.metadata["workflow_default_params"] = workflow_default_params  # Store for _execute_step

                final_context: WorkflowContext
                try:
                    if self.execution_mode == ExecutionMode.SEQUENTIAL:
                        final_context = await self._execute_sequential(run_db_id, graph, context, step_dataclasses_map)
                    elif self.execution_mode == ExecutionMode.PARALLEL:
                        final_context = await self._execute_parallel(run_db_id, graph, context, step_dataclasses_map)
                    elif self.execution_mode == ExecutionMode.DISTRIBUTED:
                        if not self.distributed_manager:
                            raise ValueError("DistributedTaskManager not configured for DISTRIBUTED mode.")
                        final_context = await self._execute_distributed(run_db_id, graph, context, step_dataclasses_map)
                    else:
                        raise ValueError(f"Unsupported execution mode: {self.execution_mode}")
                except Exception as e:
                    logger.error(f"Core execution error for run '{run_db_id}': {e}", exc_info=True)
                    context.status = StepExecutionStatus.FAILED.name
                    context.error = traceback.format_exc()
                    final_context = context  # Use the current context which has the error

                # Finalize WorkflowRun in DB
                run_db_model = await session.get(
                    WorkflowRunModel, run_db_id
                )  # Re-fetch to avoid detached instance issues potentially
                if run_db_model:
                    run_db_model.status = StatusEnum[final_context.status]  # Convert back from context string status
                    run_db_model.results = final_context.results
                    run_db_model.artifacts = final_context.artifacts
                    run_db_model.logs_summary = "\n".join(final_context.logs[-100:])  # Store last 100 log lines as summary
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    run_db_model.duration_seconds = (
                        (run_db_model.completed_at - run_db_model.started_at).total_seconds()
                        if run_db_model.started_at
                        else None
                    )
                    run_db_model.error_message = final_context.error
                    
                    # Add workflow metrics to custom_metadata
                    if not run_db_model.custom_metadata:
                        run_db_model.custom_metadata = {}
                        
                    run_db_model.custom_metadata["performance"] = {
                        "start_time": start_time,
                        "end_time": time.time(),
                        "total_duration": time.time() - start_time,
                        "avg_step_duration": sum(d.get("duration", 0) for d in final_context.step_times.values()) / len(final_context.step_times) if final_context.step_times else None,
                        "peak_memory_mb": max(final_context.resource_usage.get("memory_mb", [0])) if final_context.resource_usage.get("memory_mb") else None,
                        "avg_cpu_percent": sum(final_context.resource_usage.get("cpu_percent", [0])) / len(final_context.resource_usage.get("cpu_percent", [])) if final_context.resource_usage.get("cpu_percent") else None
                    }
                    
                    # If successful and caching enabled, mark as cacheable
                    if use_cache and final_context.status == StepExecutionStatus.COMPLETED.name:
                        await self.cache_workflow_result(run_db_id, workflow_db_model.name, run_db_model.parameters or {})
                    
                    # Update Prometheus metrics if enabled
                    if self.enable_prometheus and hasattr(self, 'prom_workflow_durations'):
                        self.prom_workflow_durations.labels(
                            workflow_name=workflow_db_model.name,
                            status=final_context.status
                        ).observe(run_db_model.duration_seconds or 0)
                        
                    # error_traceback could be stored if context has it explicitly
                    await session.commit()
                    logger.info(f"Workflow run {run_db_id} finished with status: {run_db_model.status.name}")
                else:
                    logger.error(f"Could not find WorkflowRun {run_db_id} in DB for final update.")

                # Track successful completion
                self.performance_stats['successful_workflows'] += 1
                return context
            
        except Exception as e:
            # Enhanced error tracking
            self.performance_stats['failed_workflows'] += 1
            if self.metrics_collector:
                self.metrics_collector.event(
                    "workflow_failed",
                    {"run_id": str(run_db_id), "error": str(e)}
                )
            raise
            
        finally:
            # Update performance metrics
            duration = time.time() - start_time
            self.performance_stats['total_workflows'] += 1
            self.performance_stats['avg_duration'] = (
                (self.performance_stats['avg_duration'] * (self.performance_stats['total_workflows'] - 1) + duration) 
                / self.performance_stats['total_workflows']
            )

    def _get_system_resources(self) -> Dict[str, Any]:
        """Get detailed system resource metrics"""
        return {
            'timestamp': time.time(),
            'cpu': psutil.cpu_percent(percpu=True),
            'memory': psutil.virtual_memory()._asdict(),
            'disk': psutil.disk_usage('/')._asdict(),
            'network': psutil.net_io_counters()._asdict()
        }

    async def _execute_sequential(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        """
        Execute workflow steps sequentially in dependency order

        Args:
            run_db_id: ID of the workflow run
            graph: Directed acyclic graph of steps
            context: Workflow context
            steps_map: Map of step_id_in_workflow to WorkflowStepDefinition dataclass

        Returns:
            Updated workflow context
        """
        logger.info(f"Executing workflow run {run_db_id} in sequential mode")

        for step_id_in_workflow in nx.topological_sort(graph):
            step_dataclass = graph.nodes[step_id_in_workflow]["step_dataclass"]

            # Check for cancellation
            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"Workflow run {run_db_id} cancelled. Skipping step {step_id_in_workflow}.")
                # Persist cancellation for this step if not already done
                async with get_db_session() as session:
                    await self._create_or_update_step_run(
                        session,
                        run_db_id,
                        step_dataclass,
                        StatusEnum.CANCELLED,
                        message="Workflow cancelled by higher context",
                        completed_at=datetime.now(timezone.utc),
                    )
                    await session.commit()
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus.CANCELLED
                continue

            # Check dependencies
            dep_failed = False
            for dep_id in graph.predecessors(step_id_in_workflow):
                if context.step_statuses.get(dep_id) == StepExecutionStatus.FAILED:
                    context.step_statuses[step_id_in_workflow] = StepExecutionStatus.SKIPPED
                    logger.info(
                        f"Skipping step {step_id_in_workflow} due to failed dependency {dep_id} in run {run_db_id}"
                    )
                    async with get_db_session() as session:
                        await self._create_or_update_step_run(
                            session,
                            run_db_id,
                            step_dataclass,
                            StatusEnum.SKIPPED,
                            message=f"Skipped due to failed dependency: {dep_id}",
                            completed_at=datetime.now(timezone.utc),
                        )
                        await session.commit()
                    dep_failed = True
                    break
            if dep_failed:
                continue

            # Execute step
            try:
                step_result_dict = await self._execute_step(run_db_id, step_dataclass, context)

                # Update context with step result
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus[step_result_dict["status"]]
                if "result" in step_result_dict:
                    context.results[step_id_in_workflow] = step_result_dict["result"]
                if step_result_dict["status"] == StepExecutionStatus.FAILED.name:
                    logger.error(
                        f"Step {step_id_in_workflow} failed in run {run_db_id}: {step_result_dict.get('error')}"
                    )
                    if not context.parameters.get("continue_on_failure", False):
                        context.status = StepExecutionStatus.FAILED.name
                        context.error = f"Step {step_id_in_workflow} failed. {step_result_dict.get('error')}"
                        # Persist overall failure to WorkflowRun
                        async with get_db_session() as session:
                            run_model = await session.get(WorkflowRunModel, run_db_id)
                            if run_model:
                                run_model.status = StatusEnum.FAILED
                                run_model.error_message = context.error
                                run_model.completed_at = datetime.now(timezone.utc)
                                await session.commit()
                        return context  # Stop further execution
            except Exception as e:
                logger.error(
                    f"Error during execution of step {step_id_in_workflow} (run {run_db_id}): {e}", exc_info=True
                )
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus.FAILED
                context.status = StepExecutionStatus.FAILED.name
                context.error = (
                    f"Step {step_id_in_workflow} encountered an unhandled exception: {traceback.format_exc()}"
                )
                async with get_db_session() as session:
                    # Error is logged by _execute_step, here we update the overall run
                    # _execute_step handles its own DB update for the step run
                    run_model = await session.get(WorkflowRunModel, run_db_id)
                    await self._create_or_update_step_run(
                        session,
                        run_db_id,
                        step_dataclass,
                        StatusEnum.FAILED,
                        error_message=str(e),
                        error_traceback=traceback.format_exc(),
                        completed_at=datetime.now(timezone.utc),
                    )
                    await session.commit()

                return context  # Stop further execution

        if context.status == StepExecutionStatus.RUNNING.name:  # If no step failed and not cancelled
            context.status = StepExecutionStatus.COMPLETED.name
        return context

    async def _execute_parallel(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        logger.info(f"Executing workflow run {run_db_id} in parallel mode with {self.max_workers} workers.")

        if self.thread_executor is None:
            self.thread_executor = ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix="WfThread")
        if self.process_executor is None:
            self.process_executor = ProcessPoolExecutor(max_workers=self.max_workers)

        finalized_steps_for_graph: Set[str] = set()
        submitted_step_futures: Dict[str, asyncio.Future] = {} 
        # _active_executor_tasks is managed by adding when submitting via executor, removing when future is processed.

        while len(finalized_steps_for_graph) < len(graph.nodes):
            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"Run {run_db_id} (Parallel): Workflow cancelled. Halting step submission & attempting to cancel active.")
                for f_node_id, f_task in list(submitted_step_futures.items()): 
                    if not f_task.done(): f_task.cancel()
                # Also attempt to clear any tasks for this run_db_id from _resource_limited_queues
                if run_db_id in self._resource_limited_queues:
                    while not self._resource_limited_queues[run_db_id].empty():
                        try:
                            _, _, _, _, _, queued_future, _ = self._resource_limited_queues[run_db_id].get_nowait()
                            if not queued_future.done(): queued_future.cancel("Workflow cancelled")
                        except asyncio.QueueEmpty:
                            break
                        except Exception as e_q_cancel:
                            logger.warning(f"Error cancelling queued task during workflow cancellation for {run_db_id}: {e_q_cancel}")
                    del self._resource_limited_queues[run_db_id]
                break 
            
            if context.status == StepExecutionStatus.FAILED.name and not context.parameters.get("continue_on_failure", False):
                logger.info(f"Run {run_db_id} (Parallel): Workflow failed and not 'continue_on_failure'. Halting & cancelling active.")
                for f_node_id, f_task in list(submitted_step_futures.items()):
                    if not f_task.done(): f_task.cancel()
                # Clear resource queue for this run as well
                if run_db_id in self._resource_limited_queues:
                    while not self._resource_limited_queues[run_db_id].empty():
                        try:
                            _, _, _, _, _, queued_future, _ = self._resource_limited_queues[run_db_id].get_nowait()
                            if not queued_future.done(): queued_future.cancel("Workflow failed")
                        except asyncio.QueueEmpty:
                            break
                        except Exception as e_q_fail_cancel:
                            logger.warning(f"Error cancelling queued task during workflow failure for {run_db_id}: {e_q_fail_cancel}")
                    del self._resource_limited_queues[run_db_id]
                break

            ready_to_submit_node_ids = []
            for node_id in graph.nodes:
                if node_id in finalized_steps_for_graph or node_id in submitted_step_futures or \
                   any(item[2] == node_id for queue in self._resource_limited_queues.values() for item in queue._queue): # Check if in any resource queue
                    continue 

                dependencies = set(graph.predecessors(node_id))
                if dependencies.issubset(finalized_steps_for_graph): 
                    dep_unsuccessful = False
                    for dep_id in dependencies:
                        dep_final_status = context.step_statuses.get(dep_id)
                        if dep_final_status not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                            log_prefix_par_skip = f"ParallelRunner (Run: {run_db_id}, Step: {node_id})"
                            logger.info(f"{log_prefix_par_skip}: Skipping due to unsuccessful dependency '{dep_id}' (status: {dep_final_status.name if dep_final_status else 'Unknown'}).")
                            context.step_statuses[node_id] = StepExecutionStatus.SKIPPED
                            async with get_db_session() as session:
                                step_dc_skip = graph.nodes[node_id]["step_dataclass"]
                                await self._create_or_update_step_run(
                                    session, run_db_id, step_dc_skip, StatusEnum.SKIPPED,
                                    message=f"Skipped due to unsuccessful dependency: {dep_id} (status: {dep_final_status.name if dep_final_status else 'Unknown'}).",
                                    completed_at=datetime.now(timezone.utc)
                                )
                                await session.commit()
                            finalized_steps_for_graph.add(node_id) 
                            dep_unsuccessful = True
                            break
                    if not dep_unsuccessful:
                        ready_to_submit_node_ids.append(node_id)
            
            for node_id_to_submit in ready_to_submit_node_ids:
                if context.status == StepExecutionStatus.CANCELLED.name or \
                   (context.status == StepExecutionStatus.FAILED.name and not context.parameters.get("continue_on_failure", False)):
                    break

                step_dataclass_to_submit = graph.nodes[node_id_to_submit]["step_dataclass"]
                loop = asyncio.get_running_loop()
                asyncio_task_future = loop.create_future()
                # Storing future temporarily; will add to submitted_step_futures only if directly submitted or successfully queued

                if await self._are_resources_available_for_step(step_dataclass_to_submit):
                    logger.debug(f"Resources available for step {node_id_to_submit}. Submitting directly to executor.")
                    context_repr = context if not step_dataclass_to_submit.use_process else context.to_dict()
                    chosen_executor = self.process_executor if step_dataclass_to_submit.use_process else self.thread_executor

                    if chosen_executor is None: 
                        critical_msg = f"Run {run_db_id}: Executor for step {node_id_to_submit} (process: {step_dataclass_to_submit.use_process}) is None. This is a bug."
                        logger.critical(critical_msg)
                        if not asyncio_task_future.done(): asyncio_task_future.set_exception(RuntimeError(critical_msg))
                        # This failure will be caught when processing futures.
                        submitted_step_futures[node_id_to_submit] = asyncio_task_future 
                        self._active_executor_tasks[asyncio_task_future] = (run_db_id, node_id_to_submit) # Track it as active, though it will fail
                        continue

                    chosen_executor.submit(
                        self._run_step_task_for_executor, 
                        run_db_id, 
                        step_dataclass_to_submit, 
                        context_repr, 
                        asyncio_task_future, 
                        loop
                    )
                    submitted_step_futures[node_id_to_submit] = asyncio_task_future
                    self._active_executor_tasks[asyncio_task_future] = (run_db_id, node_id_to_submit) # Add to active tasks
                    
                    context.step_statuses[node_id_to_submit] = StepExecutionStatus.QUEUED # Or PENDING, _execute_step_core will set RUNNING
                    context.add_log(f"Step '{node_id_to_submit}' (Run: {run_db_id}): Submitted directly to executor.")
                else:
                    # Resources not available, try to queue it
                    current_q_size = self._resource_limited_queues[run_db_id].qsize()
                    if current_q_size < self.max_queued_tasks_per_run:
                        priority = step_dataclass_to_submit.priority # Lower number is higher priority
                        timestamp = time.monotonic() # Tie-breaker for same priority
                        context_repr_for_q = context if not step_dataclass_to_submit.use_process else context.to_dict()
                        
                        queue_item = (
                            priority, timestamp, node_id_to_submit, step_dataclass_to_submit, 
                            context_repr_for_q, asyncio_task_future, loop
                        )
                        self._resource_limited_queues[run_db_id].put_nowait(queue_item)
                        submitted_step_futures[node_id_to_submit] = asyncio_task_future # Track future even if queued
                        # DO NOT add to _active_executor_tasks yet, it's not in an executor.
                        # The _process_resource_limited_queues will add it when it submits.

                        context.step_statuses[node_id_to_submit] = StepExecutionStatus.QUEUED # Or a new status like RESOURCE_WAIT
                        logger.info(f"Step '{node_id_to_submit}' (Run: {run_db_id}): Queued due to resource limits (CPU/Mem/GPU/Executor). Queue size: {current_q_size + 1}")
                        context.add_log(f"Step '{node_id_to_submit}': Queued, waiting for resources.")
                    else:
                        logger.warning(f"Step '{node_id_to_submit}' (Run: {run_db_id}): Resources unavailable AND resource queue for this run is full ({current_q_size}/{self.max_queued_tasks_per_run}). Step will be re-evaluated.")
                        # Future is not added to submitted_step_futures, step remains unsubmitted this cycle.
                        # No change to context.step_statuses, it remains PENDING or whatever it was.
                        if not asyncio_task_future.done():
                           # Set a specific exception or cancel to avoid hanging if not handled elsewhere
                           asyncio_task_future.set_exception(RuntimeError(f"Step {node_id_to_submit} could not be queued due to full resource queue and no available resources."))
                           submitted_step_futures[node_id_to_submit] = asyncio_task_future # Track to process its failure

            if not submitted_step_futures:
                # Check if there are items in resource queues for ANY run_id. If so, _process_resource_limited_queues will handle them.
                # If all _resource_limited_queues are empty AND submitted_step_futures is empty, but not all steps are finalized, then sleep. 
                all_resource_queues_empty = all(q.empty() for q in self._resource_limited_queues.values())
                if len(finalized_steps_for_graph) < len(graph.nodes) and all_resource_queues_empty:
                    await asyncio.sleep(0.05) 
                continue

            done_futures, _ = await asyncio.wait(
                submitted_step_futures.values(), 
                return_when=asyncio.FIRST_COMPLETED, 
                timeout=self.resource_check_interval_seconds / 2 # Shorter timeout to remain responsive
            )

            for completed_future in done_futures:
                processed_node_id = None
                for sid, fut_val in list(submitted_step_futures.items()): 
                    if fut_val == completed_future:
                        processed_node_id = sid
                        break
                
                if processed_node_id is None: 
                    logger.warning(f"Run {run_db_id} (Parallel): A future {completed_future} completed but couldn't find its step ID.")
                    if completed_future in self._active_executor_tasks: # Cleanup if it was somehow active
                        del self._active_executor_tasks[completed_future]
                    continue 
                
                # Remove from active executor tasks as it has completed processing
                if completed_future in self._active_executor_tasks:
                    del self._active_executor_tasks[completed_future]
                else:
                    # This might happen if the future was for a task that was resource-queued and then processed,
                    # or if it failed before being added to _active_executor_tasks (e.g. executor was None).
                    logger.debug(f"Future {completed_future} for step {processed_node_id} was not in _active_executor_tasks upon completion.")

                try:
                    step_actual_result = completed_future.result() 
                    if context.step_statuses.get(processed_node_id) == StepExecutionStatus.COMPLETED:
                         context.results[processed_node_id] = step_actual_result
                         logger.info(f"Step '{processed_node_id}' (Run: {run_db_id}): Future resolved successfully. Result stored.")
                    else: 
                        logger.warning(f"Step '{processed_node_id}' (Run: {run_db_id}): Future completed, but context status is {context.step_statuses.get(processed_node_id)} (expected COMPLETED). Result (preview): {str(step_actual_result)[:100]}. Trusting _execute_step_core's DB update.")

                except asyncio.CancelledError:
                    logger.info(f"Step '{processed_node_id}' (Run: {run_db_id}): Future was cancelled.")
                    if context.step_statuses.get(processed_node_id) not in StepExecutionStatus._member_map_.values():
                         context.step_statuses[processed_node_id] = StepExecutionStatus.CANCELLED
                    
                    async with get_db_session() as session:
                        step_run_db_id_for_cancel = context.metadata.get(f"step_run_db_id_{processed_node_id}") # Assuming _execute_step_core stores its DB ID here
                        await self._create_or_update_step_run(
                            session, run_db_id, graph.nodes[processed_node_id]["step_dataclass"], StatusEnum.CANCELLED,
                            step_run_db_id=step_run_db_id_for_cancel,
                            message=f"Step '{processed_node_id}' cancelled via future.",
                            completed_at=datetime.now(timezone.utc)
                        )
                        await session.commit()

                except Exception as e_future_exc: 
                    logger.error(f"Step '{processed_node_id}' (Run: {run_db_id}): Future resolved with definitive failure: {e_future_exc}", exc_info=False)
                    
                    if not context.parameters.get("continue_on_failure", False):
                        if context.status != StepExecutionStatus.FAILED.name: 
                            logger.info(f"Run {run_db_id} (Parallel): Failing overall due to step '{processed_node_id}' and not 'continue_on_failure'.")
                            context.status = StepExecutionStatus.FAILED.name 
                            context.error = context.error or f"Step '{processed_node_id}' failed: {str(e_future_exc)}"
                        for f_node_id_cancel, f_task_cancel in list(submitted_step_futures.items()):
                            if not f_task_cancel.done() and f_node_id_cancel != processed_node_id : 
                                f_task_cancel.cancel()
                
                finalized_steps_for_graph.add(processed_node_id)
                if processed_node_id in submitted_step_futures:
                    del submitted_step_futures[processed_node_id] 

        # Final workflow status determination (same as before)
        if context.status == StepExecutionStatus.RUNNING.name: 
            if len(finalized_steps_for_graph) == len(graph.nodes):
                all_steps_ok_or_skipped = True
                final_error_message_overall = context.error # Preserve earlier error if any
                for sid_check in graph.nodes:
                    s_final_stat = context.step_statuses.get(sid_check)
                    if s_final_stat not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                        all_steps_ok_or_skipped = False
                        final_error_message_overall = final_error_message_overall or f"Workflow finished with non-successful steps (parallel, e.g., step '{sid_check}' status: {s_final_stat.name if s_final_stat else 'Unknown'})."
                        logger.warning(f"Workflow run {run_db_id} determined as FAILED overall (parallel) due to step '{sid_check}' (status: {s_final_stat.name if s_final_stat else 'Unknown'}).")
                        break
                if all_steps_ok_or_skipped: 
                    context.status = StepExecutionStatus.COMPLETED.name
                else:
                    context.status = StepExecutionStatus.FAILED.name
                    context.error = final_error_message_overall
            else: 
                if context.status == StepExecutionStatus.RUNNING.name: 
                    context.status = StepExecutionStatus.FAILED.name 
                    context.error = context.error or "Workflow did not complete all steps (parallel execution)."
                    logger.warning(f"Workflow run {run_db_id} (Parallel): Exited main loop with {len(finalized_steps_for_graph)}/{len(graph.nodes)} steps finalized. Overall status set to {context.status}, Error: {context.error}")
        
        logger.info(f"Parallel execution for run {run_db_id} finished with overall status: {context.status}. Final error (if any): {context.error}")
        return context

    def _run_step_task_for_executor(
        self,
        run_db_id: uuid.UUID,
        step_dataclass: WorkflowStepDefinition,
        # Pass a serializable version of context for processes, full context for threads
        context_repr: Union[WorkflowContext, Dict[str, Any]], 
        asyncio_future: asyncio.Future,
        loop: asyncio.AbstractEventLoop,
    ):
        """
        Helper function to run _execute_step_core and set result/exception on an asyncio.Future.
        This is called by the thread/process pool executor.
        Handles context reconstruction for process-based execution.
        """
        current_pid = os.getpid()
        log_prefix = f"[ExecutorTask PID:{current_pid} Step:{step_dataclass.id} Run:{run_db_id}]"
        logger.info(f"{log_prefix} Starting task execution.")

        reconstructed_context: Optional[WorkflowContext] = None
        original_context_passed_to_process = False

        try:
            if isinstance(context_repr, WorkflowContext):
                # This branch is for ThreadPoolExecutor
                reconstructed_context = context_repr
                logger.debug(f"{log_prefix} Using direct WorkflowContext in thread.")
            elif isinstance(context_repr, dict) and step_dataclass.use_process:
                # This branch is for ProcessPoolExecutor
                original_context_passed_to_process = True
                logger.info(f"{log_prefix} Reconstructing WorkflowContext from dict for process execution.")
                try:
                    # Essential fields for _execute_step_core and _call_function
                    workflow_db_id_uuid = uuid.UUID(context_repr["workflow_db_id"])
                    run_db_id_uuid = uuid.UUID(context_repr["run_db_id"])
                    
                    reconstructed_context = WorkflowContext(
                        workflow_db_id=workflow_db_id_uuid,
                        run_db_id=run_db_id_uuid,
                        workflow_id=context_repr.get("workflow_id", "UnknownWorkflow"),
                        run_id=context_repr.get("run_id", str(run_db_id_uuid)),
                        parameters=context_repr.get("parameters", {}),
                        results=context_repr.get("results", {}), # Crucial for dependency resolution
                        artifacts=context_repr.get("artifacts", {}),
                        metadata=context_repr.get("metadata", {}),
                        logs=[], # Logs are specific to this execution context, don't inherit old ones
                        # status, start_time, end_time, error are managed by _execute_step_core
                        resource_usage=defaultdict(list), # Fresh resource usage for this step/process
                        step_statuses= {k: StepExecutionStatus[v] for k, v in context_repr.get("step_statuses", {}).items()},
                        step_times=context_repr.get("step_times", {})
                    )
                    logger.info(f"{log_prefix} Successfully reconstructed context.")
                except KeyError as ke:
                    logger.error(f"{log_prefix} Missing essential key for context reconstruction: {ke}", exc_info=True)
                    # Cannot proceed without a valid context if reconstruction fails
                    # Set exception on future and return
                    if loop.is_running() and not asyncio_future.done():
                        loop.call_soon_threadsafe(asyncio_future.set_exception, ValueError(f"Context reconstruction failed: missing {ke}"))
                    return 
                except Exception as e_recon:
                    logger.error(f"{log_prefix} General error during context reconstruction: {e_recon}", exc_info=True)
                    if loop.is_running() and not asyncio_future.done():
                        loop.call_soon_threadsafe(asyncio_future.set_exception, ValueError(f"Context reconstruction error: {e_recon}"))
                    return
            else:
                err_msg = f"{log_prefix} Invalid context_repr type: {type(context_repr)}. Expected WorkflowContext or dict (for process)."
                logger.error(err_msg)
                if loop.is_running() and not asyncio_future.done():
                    loop.call_soon_threadsafe(asyncio_future.set_exception, TypeError(err_msg))
                return

            if reconstructed_context is None:
                # Should have been caught above, but as a safeguard
                err_msg = f"{log_prefix} Context could not be established."
                logger.error(err_msg)
                if loop.is_running() and not asyncio_future.done():
                    loop.call_soon_threadsafe(asyncio_future.set_exception, ValueError(err_msg))
                return

            # Setup Process-Specific Function Resolver Cache (if in a process)
            # The main manager's _function_resolver isn't directly available in a new process.
            # If _resolve_function is called within the process, it needs a way to get the resolver.
            # This is a complex problem if the resolver itself isn't easily picklable or globally accessible.
            # For now, assuming _resolve_function (if called by step func) handles this, or function is pre-resolved.
            # The current design pre-resolves function in _execute_step_core *before* sending to executor if possible.
            
            # Execute the core step logic
            # asyncio.run() creates a new event loop, which is necessary if this task runs in a separate process
            # or a thread not managed by the main asyncio loop where `self.execute_workflow` runs.
            result = asyncio.run(self._execute_step_core(run_db_id, step_dataclass, reconstructed_context))
            
            logger.info(f"{log_prefix} Execution finished. Setting result on future.")
            if loop.is_running() and not asyncio_future.done():
                # If the step produced a new context (e.g. in a process), we might want to return parts of it.
                # However, WorkflowContext itself is not easily picklable if it contains complex objects.
                # The primary result is the step's output, context changes are typically updated in DB by _execute_step_core.
                loop.call_soon_threadsafe(asyncio_future.set_result, result)
            elif not loop.is_running() and not asyncio_future.done():
                 logger.warning(f"{log_prefix} Main loop is not running. Setting result directly (might be problematic).")
                 asyncio_future.set_result(result)
            else:
                logger.warning(f"{log_prefix} Future already done or loop closed. Cannot set result.")

        except Exception as e:
            logger.error(f"{log_prefix} Unhandled error in executor task: {e}", exc_info=True)
            error_info = {
                'status': StepExecutionStatus.FAILED.name,
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            # Attempt to update the DB for this step to FAILED if an unexpected error occurs here.
            # This is a best-effort basis as we are outside the main async flow of _execute_step_core.
            try:
                # Create a synchronous session or a new async event loop for this isolated DB update.
                # This is complex. For now, we rely on _execute_step_core to handle its DB states mostly.
                # The primary goal here is to set the exception on the future.
                logger.error(f"{log_prefix} Critical error in executor task wrapper for step {step_dataclass.id}. DB update for this specific error is not yet implemented here.")
            except Exception as db_err:
                logger.error(f"{log_prefix} Failed to update DB for critical error: {db_err}")
            
            if loop.is_running() and not asyncio_future.done():
                # Propagate the exception to the main asyncio loop that is awaiting the future.
                # The `e` here is the exception from the `try` block above.
                loop.call_soon_threadsafe(asyncio_future.set_exception, e) 
            elif not asyncio_future.done():
                logger.warning(f"{log_prefix} Main loop not running. Setting exception directly.")
                asyncio_future.set_exception(e)
            else:
                logger.warning(f"{log_prefix} Future already done or loop closed. Cannot set exception.")

    async def _create_or_update_step_run(
        self,
        session: AsyncSession,
        run_db_id: uuid.UUID,
        step_dataclass: WorkflowStepDefinition,
        status: StatusEnum,
        step_run_db_id: Optional[uuid.UUID] = None,
        parameters_used: Optional[Dict[str, Any]] = None,
        result: Optional[Any] = None,
        artifacts_produced: Optional[Dict[str, str]] = None,
        logs: Optional[str] = None,
        started_at: Optional[datetime] = None,
        completed_at: Optional[datetime] = None,
        duration_seconds: Optional[float] = None,
        error_message: Optional[str] = None,
        error_traceback: Optional[str] = None,
        retry_attempt: Optional[int] = None,
        host_machine: Optional[str] = None,
        message: Optional[str] = None,
        resource_metrics: Optional[Dict[str, Any]] = None, 
    ) -> WorkflowStepRunModel:
        step_run_model: Optional[WorkflowStepRunModel] = None
        log_entry = message or (
            f"Status update: {status.name}" + (f" | Error: {error_message}" if error_message else "")
        )
        final_logs = logs
        if not final_logs and log_entry:
            final_logs = log_entry
        elif final_logs and log_entry and final_logs != log_entry: # Avoid duplicate log entry
            final_logs = final_logs + "\\\\n" + log_entry

        if step_run_db_id:
            step_run_model = await session.get(WorkflowStepRunModel, step_run_db_id)
            if not step_run_model:
                logger.warning(
                    f"Tried to update WorkflowStepRun DB ID {step_run_db_id} but not found (run {run_db_id}, step {step_dataclass.id}). Will try to find/create."
                )
                step_run_db_id = None 

        if not step_run_model: 
            stmt = (
                sa.select(WorkflowStepRunModel)
                .where(
                    WorkflowStepRunModel.workflow_run_id == run_db_id,
                    WorkflowStepRunModel.step_id_in_workflow == step_dataclass.id,
                )
                .order_by(WorkflowStepRunModel.created_at.desc()) 
            )
            existing_runs = (await session.execute(stmt)).scalars().all()
            if existing_runs:
                 if step_run_db_id and any(sr.id == step_run_db_id for sr in existing_runs):
                     step_run_model = next(sr for sr in existing_runs if sr.id == step_run_db_id)
                 else: 
                     step_run_model = existing_runs[0]


        if step_run_model:  # Update existing
            step_run_model.status = status
            if parameters_used is not None: step_run_model.parameters_used = parameters_used
            if result is not None: step_run_model.result = result
            if artifacts_produced is not None: step_run_model.artifacts_produced = artifacts_produced
            if final_logs:
                if step_run_model.logs and step_run_model.logs != final_logs:
                    new_logs = step_run_model.logs + "\n" + final_logs
                else:
                    new_logs = final_logs
                step_run_model.logs = new_logs[-20000:] if new_logs else None
                step_run_model.started_at = started_at
            if completed_at is not None: step_run_model.completed_at = completed_at
            if duration_seconds is not None: step_run_model.duration_seconds = duration_seconds
            
            if error_message is not None: step_run_model.error_message = error_message
            elif status not in [StatusEnum.FAILED, StatusEnum.TIMEOUT]: step_run_model.error_message = None
            if error_traceback is not None: step_run_model.error_traceback = error_traceback
            elif status not in [StatusEnum.FAILED, StatusEnum.TIMEOUT]: step_run_model.error_traceback = None
                
            if retry_attempt is not None: step_run_model.retry_attempt = retry_attempt
            if host_machine is not None: step_run_model.host_machine = host_machine
            
            if resource_metrics:
                if not step_run_model.custom_metadata: step_run_model.custom_metadata = {}
                attempt_key = f"attempt_{retry_attempt if retry_attempt is not None else 0}_metrics"
                step_run_model.custom_metadata[attempt_key] = resource_metrics
                
                if hasattr(step_run_model, 'peak_memory_mb') and 'peak_memory_mb' in resource_metrics:
                    step_run_model.peak_memory_mb = resource_metrics['peak_memory_mb']
                if hasattr(step_run_model, 'average_cpu_percent') and 'average_cpu_percent' in resource_metrics:
                    step_run_model.average_cpu_percent = resource_metrics['average_cpu_percent']

            step_run_model.updated_at = datetime.now(timezone.utc)
            logger.debug(f"Updating StepRun {step_run_model.id} for step {step_dataclass.id}, status {status.name}, attempt {retry_attempt}")
        else:  
            run_model_for_workflow_id = await session.get(WorkflowRunModel, run_db_id)
            if not run_model_for_workflow_id:
                raise ValueError(f"Run {run_db_id} not found, cannot create step run for {step_dataclass.id}")

            step_def_id_stmt = sa.select(WorkflowStepDefinitionModel.id).where(
                WorkflowStepDefinitionModel.workflow_id == run_model_for_workflow_id.workflow_id,
                WorkflowStepDefinitionModel.step_id_in_workflow == step_dataclass.id,
            )
            step_definition_db_id = (await session.execute(step_def_id_stmt)).scalar_one_or_none()
            if not step_definition_db_id:
                raise ValueError(f"Step def for {step_dataclass.id} in workflow {run_model_for_workflow_id.workflow_id} not found.")

            current_custom_metadata = {}
            if resource_metrics:
                current_custom_metadata[f"attempt_{retry_attempt if retry_attempt is not None else 0}_metrics"] = resource_metrics

            step_run_model = WorkflowStepRunModel(
                workflow_run_id=run_db_id,
                step_definition_id=step_definition_db_id,
                step_id_in_workflow=step_dataclass.id,
                status=status,
                parameters_used=(parameters_used or step_dataclass.parameters),
                result=result, artifacts_produced=artifacts_produced, logs=final_logs,
                started_at=started_at, completed_at=completed_at, duration_seconds=duration_seconds,
                error_message=error_message, error_traceback=error_traceback,
                retry_attempt=(retry_attempt or 0),
                host_machine=(host_machine or socket.gethostname() if hasattr(socket, "gethostname") else None),
                custom_metadata=current_custom_metadata,
                peak_memory_mb=resource_metrics.get('peak_memory_mb') if resource_metrics and hasattr(WorkflowStepRunModel, 'peak_memory_mb') else None,
                average_cpu_percent=resource_metrics.get('average_cpu_percent') if resource_metrics and hasattr(WorkflowStepRunModel, 'average_cpu_percent') else None,
            )
            session.add(step_run_model)
            await session.flush()
            logger.debug(f"Created new StepRun {step_run_model.id} for step {step_dataclass.id}, status {status.name}, attempt {retry_attempt}")
        
        await session.flush()
        return step_run_model

    async def _execute_step_core(
        self, 
        run_db_id: uuid.UUID, 
        step_dataclass: WorkflowStepDefinition, 
        context: WorkflowContext
    ) -> Any: # Returns the functional result of the step
        step_id = step_dataclass.id
        function_identifier = step_dataclass.function_identifier
        step_run_db_rec_id: Optional[uuid.UUID] = None 

        logger.info(f"Initiating step '{step_id}' for run '{run_db_id}' using function '{function_identifier}'")
        context.add_log(f"Step '{step_id}': Starting with function '{function_identifier}'.")

        async with get_db_session() as session:
            temp_step_run_model = await self._create_or_update_step_run(
                session, run_db_id, step_dataclass, StatusEnum.PENDING,
                parameters_used=step_dataclass.parameters, 
                message=f"Step '{step_id}': Initialized to PENDING."
            )
            await session.commit()
            step_run_db_rec_id = temp_step_run_model.id
            logger.debug(f"Step '{step_id}': DB record ID {step_run_db_rec_id} initialized for attempt tracking.")

        retry_count = 0
        max_retries = step_dataclass.retry_count or 0
        step_timeout_seconds = step_dataclass.timeout_seconds if step_dataclass.timeout_seconds is not None and step_dataclass.timeout_seconds > 0 else None
        
        current_process = psutil.Process(os.getpid())

        while retry_count <= max_retries:
            attempt_start_time = time.time()
            attempt_error_message: Optional[str] = None
            attempt_error_traceback: Optional[str] = None
            attempt_status: StatusEnum = StatusEnum.RUNNING 
            attempt_resource_metrics: Dict[str, Any] = {}
            step_function_result: Any = None

            log_prefix = f"Step '{step_id}' (Run: {run_db_id}, Attempt {retry_count + 1}/{max_retries + 1})"
            logger.info(f"{log_prefix}: Starting.")
            context.add_log(f"{log_prefix}: Starting.")

            initial_cpu_times = current_process.cpu_times()
            initial_memory_info = current_process.memory_info()
            initial_io_counters = current_process.io_counters()
            
            async with get_db_session() as session:
                await self._create_or_update_step_run(
                    session, run_db_id, step_dataclass, StatusEnum.RUNNING,
                    step_run_db_id=step_run_db_rec_id, 
                    started_at=datetime.now(timezone.utc),
                    retry_attempt=retry_count,
                    message=f"{log_prefix}: Marked as RUNNING."
                )
                await session.commit()

            try:
                if not step_dataclass._resolved_function:
                    try:
                        step_dataclass._resolved_function = self._resolve_function(function_identifier)
                        logger.debug(f"{log_prefix}: Resolved function '{function_identifier}'.")
                    except Exception as e_resolve:
                        logger.error(f"{log_prefix}: Failed to resolve function '{function_identifier}': {e_resolve}", exc_info=True)
                        attempt_error_message = f"Function resolution failed: {str(e_resolve)}"
                        attempt_error_traceback = traceback.format_exc()
                        attempt_status = StatusEnum.FAILED
                        break 

                actual_parameters = {**(context.metadata.get("workflow_default_params", {})), **(step_dataclass.parameters or {})}
                executable_task = self._call_function(step_dataclass._resolved_function, actual_parameters, context)

                if step_timeout_seconds:
                    logger.info(f"{log_prefix}: Executing with timeout: {step_timeout_seconds}s.")
                    step_function_result = await asyncio.wait_for(executable_task, timeout=step_timeout_seconds)
                else:
                    logger.info(f"{log_prefix}: Executing without specific timeout.")
                    step_function_result = await executable_task
                
                attempt_status = StatusEnum.COMPLETED

            except asyncio.TimeoutError:
                attempt_error_message = f"Execution timed out after {step_timeout_seconds}s"
                logger.warning(f"{log_prefix}: {attempt_error_message}")
                attempt_status = StatusEnum.TIMEOUT
            except Exception as e_exec:
                attempt_error_message = f"Execution failed: {str(e_exec)}"
                attempt_error_traceback = traceback.format_exc()
                logger.error(f"{log_prefix}: {attempt_error_message}", exc_info=True)
                attempt_status = StatusEnum.FAILED
            
            attempt_duration = time.time() - attempt_start_time

            try:
                final_cpu_times = current_process.cpu_times()
                final_memory_info = current_process.memory_info()
                final_io_counters = current_process.io_counters()

                cpu_user_delta = final_cpu_times.user - initial_cpu_times.user
                cpu_system_delta = final_cpu_times.system - initial_cpu_times.system
                total_cpu_time_for_attempt = cpu_user_delta + cpu_system_delta
                
                avg_cpu_percent = (total_cpu_time_for_attempt / attempt_duration) * 100 if attempt_duration > 0.01 else 0.0
                
                attempt_resource_metrics = {
                    'duration_seconds': round(attempt_duration, 3),
                    'cpu_time_user_seconds': round(cpu_user_delta, 3),
                    'cpu_time_system_seconds': round(cpu_system_delta, 3),
                    'average_cpu_percent': round(avg_cpu_percent, 2),
                    'initial_memory_rss_mb': round(initial_memory_info.rss / (1024*1024), 2),
                    'final_memory_rss_mb': round(final_memory_info.rss / (1024*1024), 2),
                    'peak_memory_mb': round(final_memory_info.rss / (1024*1024), 2), 
                    'io_read_count': final_io_counters.read_count - initial_io_counters.read_count,
                    'io_write_count': final_io_counters.write_count - initial_io_counters.write_count,
                    'io_read_bytes': final_io_counters.read_bytes - initial_io_counters.read_bytes,
                    'io_write_bytes': final_io_counters.write_bytes - initial_io_counters.write_bytes,
                }
            except psutil.Error as e_psutil:
                logger.warning(f"{log_prefix}: Failed to collect full resource metrics: {e_psutil}")
                attempt_resource_metrics['error'] = f"psutil error: {str(e_psutil)}"
            
            async with get_db_session() as session:
                await self._create_or_update_step_run(
                    session, run_db_id, step_dataclass, attempt_status,
                    step_run_db_id=step_run_db_rec_id, 
                    result=step_function_result if attempt_status == StatusEnum.COMPLETED else None,
                    error_message=attempt_error_message, error_traceback=attempt_error_traceback,
                    completed_at=datetime.now(timezone.utc), 
                    duration_seconds=attempt_duration, 
                    retry_attempt=retry_count,
                    resource_metrics=attempt_resource_metrics
                )
                await session.commit()

            if attempt_status == StatusEnum.COMPLETED:
                logger.info(f"{log_prefix}: Succeeded. Result: {str(step_function_result)[:100]}... Metrics: {attempt_resource_metrics}")
                context.add_log(f"{log_prefix}: Succeeded.")
                context.step_statuses[step_id] = StepExecutionStatus.COMPLETED
                context.step_times[step_id] = {
                    "start_time": attempt_start_time, "end_time": time.time(), 
                    "duration": attempt_duration, "attempt": retry_count + 1,
                    "metrics": attempt_resource_metrics
                }
                return step_function_result

            logger.warning(f"{log_prefix}: Attempt failed with status {attempt_status.name}. Error: {attempt_error_message}. Metrics: {attempt_resource_metrics}")
            context.add_log(f"{log_prefix}: Attempt failed with status {attempt_status.name}. Error: {attempt_error_message}")

            retry_count += 1
            if retry_count <= max_retries:
                retry_delay_config = step_dataclass.retry_delay_seconds or 60
                actual_retry_delay = (retry_delay_config * (2 ** (retry_count - 1))) + (random.uniform(-0.1, 0.1) * retry_delay_config)
                actual_retry_delay = max(1.0, min(actual_retry_delay, 3600.0)) 
                
                logger.info(f"{log_prefix}: Scheduling retry {retry_count}/{max_retries} after {actual_retry_delay:.2f}s.")
                context.add_log(f"{log_prefix}: Will retry ({retry_count}/{max_retries}) after {actual_retry_delay:.2f}s.")
                
                async with get_db_session() as session: 
                     await self._create_or_update_step_run(
                        session, run_db_id, step_dataclass, StatusEnum.PENDING,
                        step_run_db_id=step_run_db_rec_id,
                        message=f"{log_prefix}: Queued for retry {retry_count}.",
                        error_message=None, error_traceback=None, 
                        started_at=None, completed_at=None, duration_seconds=None,
                        resource_metrics=None 
                    )
                     await session.commit()
                await asyncio.sleep(actual_retry_delay)
            else: 
                final_failure_message = f"{log_prefix}: Failed definitively after {max_retries} retries. Last status: {attempt_status.name}. Last error: {attempt_error_message}"
                logger.error(final_failure_message)
                context.add_log(final_failure_message)
                context.step_statuses[step_id] = StepExecutionStatus.from_db_status(attempt_status) 
                context.step_times[step_id] = {
                    "start_time_initial": "N/A (multiple attempts)", "end_time_final": time.time(), 
                    "total_attempts_duration": "N/A (see individual attempt metrics)", "error": attempt_error_message,
                    "attempts_made": retry_count, "final_attempt_metrics": attempt_resource_metrics
                }
                if attempt_status == StatusEnum.TIMEOUT:
                    raise asyncio.TimeoutError(attempt_error_message) 
                else: 
                    raise RuntimeError(attempt_error_message)
        
        final_unhandled_message = f"{log_prefix}: Exited execution loop unexpectedly. Last status {attempt_status.name if 'attempt_status' in locals() else 'UNKNOWN'}."
        logger.error(final_unhandled_message)
        raise RuntimeError(final_unhandled_message)


    async def _execute_sequential(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        logger.info(f"Executing workflow run {run_db_id} in sequential mode")

        for step_id_in_workflow in nx.topological_sort(graph):
            step_dataclass = graph.nodes[step_id_in_workflow]["step_dataclass"]
            log_prefix_seq = f"SequentialRunner (Run: {run_db_id}, Step: {step_id_in_workflow})"

            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"{log_prefix_seq}: Workflow cancelled. Skipping step.")
                async with get_db_session() as session:
                    await self._create_or_update_step_run(
                        session, run_db_id, step_dataclass, StatusEnum.CANCELLED,
                        message="Workflow cancelled by higher context.",
                        completed_at=datetime.now(timezone.utc)
                    )
                    await session.commit()
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus.CANCELLED
                continue

            dep_failed_or_skipped = False
            for dep_id in graph.predecessors(step_id_in_workflow):
                dep_status = context.step_statuses.get(dep_id)
                if dep_status in [StepExecutionStatus.FAILED, StepExecutionStatus.TIMEOUT, StepExecutionStatus.CANCELLED]:
                    logger.info(f"{log_prefix_seq}: Skipping due to unsuccessful dependency '{dep_id}' (status: {dep_status}).")
                    context.step_statuses[step_id_in_workflow] = StepExecutionStatus.SKIPPED
                    async with get_db_session() as session:
                        await self._create_or_update_step_run(
                            session, run_db_id, step_dataclass, StatusEnum.SKIPPED,
                            message=f"Skipped due to unsuccessful dependency: {dep_id} (status: {dep_status.name if dep_status else 'Unknown'}).",
                            completed_at=datetime.now(timezone.utc)
                        )
                        await session.commit()
                    dep_failed_or_skipped = True
                    break
            if dep_failed_or_skipped:
                continue

            try:
                logger.info(f"{log_prefix_seq}: Calling _execute_step_core.")
                step_result = await self._execute_step_core(run_db_id, step_dataclass, context)
                context.results[step_id_in_workflow] = step_result
                logger.info(f"{log_prefix_seq}: Successfully completed. Result: {str(step_result)[:100]}...")

            except Exception as e: 
                logger.error(f"{log_prefix_seq}: Definitive failure after retries: {e}", exc_info=False) 
                if not context.parameters.get("continue_on_failure", False):
                    context.status = StepExecutionStatus.FAILED.name 
                    context.error = f"Step '{step_id_in_workflow}' failed: {str(e)}"
                    logger.error(f"Workflow run {run_db_id} failing due to step '{step_id_in_workflow}'.")
                    return context 

        if context.status == StepExecutionStatus.RUNNING.name: 
            all_steps_processed_ok = True
            for node_id_check in graph.nodes:
                final_status = context.step_statuses.get(node_id_check)
                if final_status not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                    all_steps_processed_ok = False
                    context.status = StepExecutionStatus.FAILED.name 
                    context.error = context.error or f"Workflow finished with non-successful steps (e.g., step '{node_id_check}' status: {final_status})."
                    logger.warning(f"Workflow run {run_db_id} determined as FAILED overall due to step '{node_id_check}' (status: {final_status}).")
                    break
            if all_steps_processed_ok:
                 context.status = StepExecutionStatus.COMPLETED.name
                 logger.info(f"Workflow run {run_db_id} completed successfully (sequential execution).")
        return context

    async def _execute_parallel(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        logger.info(f"Executing workflow run {run_db_id} in parallel mode with {self.max_workers} workers.")

        if self.thread_executor is None:
            self.thread_executor = ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix="WfThread")
        if self.process_executor is None:
            self.process_executor = ProcessPoolExecutor(max_workers=self.max_workers)

        finalized_steps_for_graph: Set[str] = set()
        submitted_step_futures: Dict[str, asyncio.Future] = {} 
        # _active_executor_tasks is managed by adding when submitting via executor, removing when future is processed.

        while len(finalized_steps_for_graph) < len(graph.nodes):
            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"Run {run_db_id} (Parallel): Workflow cancelled. Halting step submission & attempting to cancel active.")
                for f_node_id, f_task in list(submitted_step_futures.items()): 
                    if not f_task.done(): f_task.cancel()
                # Also attempt to clear any tasks for this run_db_id from _resource_limited_queues
                if run_db_id in self._resource_limited_queues:
                    while not self._resource_limited_queues[run_db_id].empty():
                        try:
                            _, _, _, _, _, queued_future, _ = self._resource_limited_queues[run_db_id].get_nowait()
                            if not queued_future.done(): queued_future.cancel("Workflow cancelled")
                        except asyncio.QueueEmpty:
                            break
                        except Exception as e_q_cancel:
                            logger.warning(f"Error cancelling queued task during workflow cancellation for {run_db_id}: {e_q_cancel}")
                    del self._resource_limited_queues[run_db_id]
                break 
            
            if context.status == StepExecutionStatus.FAILED.name and not context.parameters.get("continue_on_failure", False):
                logger.info(f"Run {run_db_id} (Parallel): Workflow failed and not 'continue_on_failure'. Halting & cancelling active.")
                for f_node_id, f_task in list(submitted_step_futures.items()):
                    if not f_task.done(): f_task.cancel()
                # Clear resource queue for this run as well
                if run_db_id in self._resource_limited_queues:
                    while not self._resource_limited_queues[run_db_id].empty():
                        try:
                            _, _, _, _, _, queued_future, _ = self._resource_limited_queues[run_db_id].get_nowait()
                            if not queued_future.done(): queued_future.cancel("Workflow failed")
                        except asyncio.QueueEmpty:
                            break
                        except Exception as e_q_fail_cancel:
                            logger.warning(f"Error cancelling queued task during workflow failure for {run_db_id}: {e_q_fail_cancel}")
                    del self._resource_limited_queues[run_db_id]
                break

            ready_to_submit_node_ids = []
            for node_id in graph.nodes:
                if node_id in finalized_steps_for_graph or node_id in submitted_step_futures or \
                   any(item[2] == node_id for queue in self._resource_limited_queues.values() for item in queue._queue): # Check if in any resource queue
                    continue 

                dependencies = set(graph.predecessors(node_id))
                if dependencies.issubset(finalized_steps_for_graph): 
                    dep_unsuccessful = False
                    for dep_id in dependencies:
                        dep_final_status = context.step_statuses.get(dep_id)
                        if dep_final_status not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                            log_prefix_par_skip = f"ParallelRunner (Run: {run_db_id}, Step: {node_id})"
                            logger.info(f"{log_prefix_par_skip}: Skipping due to unsuccessful dependency '{dep_id}' (status: {dep_final_status.name if dep_final_status else 'Unknown'}).")
                            context.step_statuses[node_id] = StepExecutionStatus.SKIPPED
                            async with get_db_session() as session:
                                step_dc_skip = graph.nodes[node_id]["step_dataclass"]
                                await self._create_or_update_step_run(
                                    session, run_db_id, step_dc_skip, StatusEnum.SKIPPED,
                                    message=f"Skipped due to unsuccessful dependency: {dep_id} (status: {dep_final_status.name if dep_final_status else 'Unknown'}).",
                                    completed_at=datetime.now(timezone.utc)
                                )
                                await session.commit()
                            finalized_steps_for_graph.add(node_id) 
                            dep_unsuccessful = True
                            break
                    if not dep_unsuccessful:
                        ready_to_submit_node_ids.append(node_id)
            
            for node_id_to_submit in ready_to_submit_node_ids:
                if context.status == StepExecutionStatus.CANCELLED.name or \
                   (context.status == StepExecutionStatus.FAILED.name and not context.parameters.get("continue_on_failure", False)):
                    break

                step_dataclass_to_submit = graph.nodes[node_id_to_submit]["step_dataclass"]
                loop = asyncio.get_running_loop()
                asyncio_task_future = loop.create_future()
                # Storing future temporarily; will add to submitted_step_futures only if directly submitted or successfully queued

                if await self._are_resources_available_for_step(step_dataclass_to_submit):
                    logger.debug(f"Resources available for step {node_id_to_submit}. Submitting directly to executor.")
                    context_repr = context if not step_dataclass_to_submit.use_process else context.to_dict()
                    chosen_executor = self.process_executor if step_dataclass_to_submit.use_process else self.thread_executor

                    if chosen_executor is None: 
                        critical_msg = f"Run {run_db_id}: Executor for step {node_id_to_submit} (process: {step_dataclass_to_submit.use_process}) is None. This is a bug."
                        logger.critical(critical_msg)
                        if not asyncio_task_future.done(): asyncio_task_future.set_exception(RuntimeError(critical_msg))
                        # This failure will be caught when processing futures.
                        submitted_step_futures[node_id_to_submit] = asyncio_task_future 
                        self._active_executor_tasks[asyncio_task_future] = (run_db_id, node_id_to_submit) # Track it as active, though it will fail
                        continue

                    chosen_executor.submit(
                        self._run_step_task_for_executor, 
                        run_db_id, 
                        step_dataclass_to_submit, 
                        context_repr, 
                        asyncio_task_future, 
                        loop
                    )
                    submitted_step_futures[node_id_to_submit] = asyncio_task_future
                    self._active_executor_tasks[asyncio_task_future] = (run_db_id, node_id_to_submit) # Add to active tasks
                    
                    context.step_statuses[node_id_to_submit] = StepExecutionStatus.QUEUED # Or PENDING, _execute_step_core will set RUNNING
                    context.add_log(f"Step '{node_id_to_submit}' (Run: {run_db_id}): Submitted directly to executor.")
                else:
                    # Resources not available, try to queue it
                    current_q_size = self._resource_limited_queues[run_db_id].qsize()
                    if current_q_size < self.max_queued_tasks_per_run:
                        priority = step_dataclass_to_submit.priority # Lower number is higher priority
                        timestamp = time.monotonic() # Tie-breaker for same priority
                        context_repr_for_q = context if not step_dataclass_to_submit.use_process else context.to_dict()
                        
                        queue_item = (
                            priority, timestamp, node_id_to_submit, step_dataclass_to_submit, 
                            context_repr_for_q, asyncio_task_future, loop
                        )
                        self._resource_limited_queues[run_db_id].put_nowait(queue_item)
                        submitted_step_futures[node_id_to_submit] = asyncio_task_future # Track future even if queued
                        # DO NOT add to _active_executor_tasks yet, it's not in an executor.
                        # The _process_resource_limited_queues will add it when it submits.

                        context.step_statuses[node_id_to_submit] = StepExecutionStatus.QUEUED # Or a new status like RESOURCE_WAIT
                        logger.info(f"Step '{node_id_to_submit}' (Run: {run_db_id}): Queued due to resource limits (CPU/Mem/GPU/Executor). Queue size: {current_q_size + 1}")
                        context.add_log(f"Step '{node_id_to_submit}': Queued, waiting for resources.")
                    else:
                        logger.warning(f"Step '{node_id_to_submit}' (Run: {run_db_id}): Resources unavailable AND resource queue for this run is full ({current_q_size}/{self.max_queued_tasks_per_run}). Step will be re-evaluated.")
                        # Future is not added to submitted_step_futures, step remains unsubmitted this cycle.
                        # No change to context.step_statuses, it remains PENDING or whatever it was.
                        if not asyncio_task_future.done():
                           # Set a specific exception or cancel to avoid hanging if not handled elsewhere
                           asyncio_task_future.set_exception(RuntimeError(f"Step {node_id_to_submit} could not be queued due to full resource queue and no available resources."))
                           submitted_step_futures[node_id_to_submit] = asyncio_task_future # Track to process its failure

            if not submitted_step_futures:
                # Check if there are items in resource queues for ANY run_id. If so, _process_resource_limited_queues will handle them.
                # If all _resource_limited_queues are empty AND submitted_step_futures is empty, but not all steps are finalized, then sleep. 
                all_resource_queues_empty = all(q.empty() for q in self._resource_limited_queues.values())
                if len(finalized_steps_for_graph) < len(graph.nodes) and all_resource_queues_empty:
                    await asyncio.sleep(0.05) 
                continue

            done_futures, _ = await asyncio.wait(
                submitted_step_futures.values(), 
                return_when=asyncio.FIRST_COMPLETED, 
                timeout=self.resource_check_interval_seconds / 2 # Shorter timeout to remain responsive
            )

            for completed_future in done_futures:
                processed_node_id = None
                for sid, fut_val in list(submitted_step_futures.items()): 
                    if fut_val == completed_future:
                        processed_node_id = sid
                        break
                
                if processed_node_id is None: 
                    logger.warning(f"Run {run_db_id} (Parallel): A future {completed_future} completed but couldn't find its step ID.")
                    if completed_future in self._active_executor_tasks: # Cleanup if it was somehow active
                        del self._active_executor_tasks[completed_future]
                    continue 
                
                # Remove from active executor tasks as it has completed processing
                if completed_future in self._active_executor_tasks:
                    del self._active_executor_tasks[completed_future]
                else:
                    # This might happen if the future was for a task that was resource-queued and then processed,
                    # or if it failed before being added to _active_executor_tasks (e.g. executor was None).
                    logger.debug(f"Future {completed_future} for step {processed_node_id} was not in _active_executor_tasks upon completion.")

                try:
                    step_actual_result = completed_future.result() 
                    if context.step_statuses.get(processed_node_id) == StepExecutionStatus.COMPLETED:
                         context.results[processed_node_id] = step_actual_result
                         logger.info(f"Step '{processed_node_id}' (Run: {run_db_id}): Future resolved successfully. Result stored.")
                    else: 
                        logger.warning(f"Step '{processed_node_id}' (Run: {run_db_id}): Future completed, but context status is {context.step_statuses.get(processed_node_id)} (expected COMPLETED). Result (preview): {str(step_actual_result)[:100]}. Trusting _execute_step_core's DB update.")

                except asyncio.CancelledError:
                    logger.info(f"Step '{processed_node_id}' (Run: {run_db_id}): Future was cancelled.")
                    if context.step_statuses.get(processed_node_id) not in StepExecutionStatus._member_map_.values():
                         context.step_statuses[processed_node_id] = StepExecutionStatus.CANCELLED
                    
                    async with get_db_session() as session:
                        step_run_db_id_for_cancel = context.metadata.get(f"step_run_db_id_{processed_node_id}") # Assuming _execute_step_core stores its DB ID here
                        await self._create_or_update_step_run(
                            session, run_db_id, graph.nodes[processed_node_id]["step_dataclass"], StatusEnum.CANCELLED,
                            step_run_db_id=step_run_db_id_for_cancel,
                            message=f"Step '{processed_node_id}' cancelled via future.",
                            completed_at=datetime.now(timezone.utc)
                        )
                        await session.commit()

                except Exception as e_future_exc: 
                    logger.error(f"Step '{processed_node_id}' (Run: {run_db_id}): Future resolved with definitive failure: {e_future_exc}", exc_info=False)
                    
                    if not context.parameters.get("continue_on_failure", False):
                        if context.status != StepExecutionStatus.FAILED.name: 
                            logger.info(f"Run {run_db_id} (Parallel): Failing overall due to step '{processed_node_id}' and not 'continue_on_failure'.")
                            context.status = StepExecutionStatus.FAILED.name 
                            context.error = context.error or f"Step '{processed_node_id}' failed: {str(e_future_exc)}"
                        for f_node_id_cancel, f_task_cancel in list(submitted_step_futures.items()):
                            if not f_task_cancel.done() and f_node_id_cancel != processed_node_id : 
                                f_task_cancel.cancel()
                
                finalized_steps_for_graph.add(processed_node_id)
                if processed_node_id in submitted_step_futures:
                    del submitted_step_futures[processed_node_id] 

        # Final workflow status determination (same as before)
        if context.status == StepExecutionStatus.RUNNING.name: 
            if len(finalized_steps_for_graph) == len(graph.nodes):
                all_steps_ok_or_skipped = True
                final_error_message_overall = context.error # Preserve earlier error if any
                for sid_check in graph.nodes:
                    s_final_stat = context.step_statuses.get(sid_check)
                    if s_final_stat not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                        all_steps_ok_or_skipped = False
                        final_error_message_overall = final_error_message_overall or f"Workflow finished with non-successful steps (parallel, e.g., step '{sid_check}' status: {s_final_stat.name if s_final_stat else 'Unknown'})."
                        logger.warning(f"Workflow run {run_db_id} determined as FAILED overall (parallel) due to step '{sid_check}' (status: {s_final_stat.name if s_final_stat else 'Unknown'}).")
                        break
                if all_steps_ok_or_skipped: 
                    context.status = StepExecutionStatus.COMPLETED.name
                else:
                    context.status = StepExecutionStatus.FAILED.name
                    context.error = final_error_message_overall
            else: 
                if context.status == StepExecutionStatus.RUNNING.name: 
                    context.status = StepExecutionStatus.FAILED.name 
                    context.error = context.error or "Workflow did not complete all steps (parallel execution)."
                    logger.warning(f"Workflow run {run_db_id} (Parallel): Exited main loop with {len(finalized_steps_for_graph)}/{len(graph.nodes)} steps finalized. Overall status set to {context.status}, Error: {context.error}")
        
        logger.info(f"Parallel execution for run {run_db_id} finished with overall status: {context.status}. Final error (if any): {context.error}")
        return context

logger = logging.getLogger(__name__)


class StepExecutionStatus(Enum):
    """Status of a workflow step execution"""

    PENDING = auto()
    QUEUED = auto()
    RUNNING = auto()
    COMPLETED = auto()
    FAILED = auto()
    SKIPPED = auto()
    CANCELLED = auto()
    TIMEOUT = auto()

    def to_db_status(self) -> StatusEnum:
        """Convert manager's StepExecutionStatus to database's StatusEnum"""
        try:
            return StatusEnum[self.name]
        except KeyError:
            logger.warning(f"Unsupported StepExecutionStatus for DB conversion: {self.name}. Defaulting to FAILED.")
            return StatusEnum.FAILED

    @staticmethod
    def from_db_status(db_status: StatusEnum) -> "StepExecutionStatus":
        """Convert database's StatusEnum to manager's StepExecutionStatus"""
        try:
            return StepExecutionStatus[db_status.name]
        except KeyError:
            logger.warning(
                f"Unsupported DB StatusEnum for manager conversion: {db_status.name}. Defaulting to FAILED."
            )
            return StepExecutionStatus.FAILED


@dataclass
class WorkflowContext:
    """Context for a workflow execution with enhanced metadata"""

    workflow_db_id: uuid.UUID
    run_db_id: uuid.UUID
    workflow_id: str
    run_id: str
    parameters: Dict[str, Any] = field(default_factory=dict)
    results: Dict[str, Any] = field(default_factory=dict)
    artifacts: Dict[str, str] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)
    logs: List[str] = field(default_factory=list)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    status: str = "PENDING"
    error: Optional[str] = None

    # Enhanced fields for better tracking
    resource_usage: Dict[str, List[float]] = field(
        default_factory=lambda: {"cpu_percent": [], "memory_percent": [], "memory_mb": [], "timestamp": []}
    )
    step_statuses: Dict[str, StepExecutionStatus] = field(default_factory=dict)
    step_times: Dict[str, Dict[str, float]] = field(default_factory=dict)

    def add_result(self, key: str, value: Any) -> None:
        """Add a result to the context"""
        self.results[key] = value

    def add_artifact(self, key: str, path: str) -> None:
        """Add an artifact path to the context"""
        self.artifacts[key] = path

    def get_result(self, key: str, default: Any = None) -> Any:
        """Get a result from the context"""
        return self.results.get(key, default)

    def get_artifact(self, key: str, default: str = None) -> str:
        """Get an artifact path from the context"""
        return self.artifacts.get(key, default)

    def add_log(self, message: str) -> None:
        """Add a log message to the context"""
        timestamp = datetime.now().isoformat()
        self.logs.append(f"[{timestamp}] {message}")

    def update_resource_usage(self) -> None:
        """Update resource usage statistics"""
        try:
            # Get current process
            process = psutil.Process(os.getpid())

            # Add metrics
            self.resource_usage["cpu_percent"].append(process.cpu_percent())
            memory_info = process.memory_info()
            memory_percent = process.memory_percent()
            self.resource_usage["memory_percent"].append(memory_percent)
            self.resource_usage["memory_mb"].append(memory_info.rss / (1024 * 1024))
            self.resource_usage["timestamp"].append(time.time())
        except Exception as e:
            logger.warning(f"Could not update resource usage: {str(e)}")

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization"""
        serializable_step_statuses = {step_id: status.name for step_id, status in self.step_statuses.items()}
        return {
            "workflow_db_id": str(self.workflow_db_id),
            "run_db_id": str(self.run_db_id),
            "workflow_id": self.workflow_id,
            "run_id": self.run_id,
            "parameters": self.parameters,
            "results": self.results,
            "artifacts": self.artifacts,
            "metadata": self.metadata,
            "logs": self.logs,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "end_time": self.end_time.isoformat() if self.end_time else None,
            "status": self.status,
            "error": self.error,
            "resource_usage": self.resource_usage,
            "step_statuses": serializable_step_statuses,
            "step_times": self.step_times,
        }


@dataclass
class WorkflowStepDefinition:
    """A step definition in a workflow with enhanced capabilities"""

    id: str
    name: str
    description: str
    function_identifier: str
    parameters: Dict[str, Any] = field(default_factory=dict)
    dependencies: List[str] = field(default_factory=list)
    timeout_seconds: Optional[int] = None
    retry_count: int = 0
    retry_delay_seconds: int = 60
    priority: int = 0
    estimated_memory_mb: int = 1000
    use_gpu: bool = False
    use_process: bool = False
    _resolved_function: Optional[Callable] = field(default=None, repr=False, compare=False)


class ExecutionMode(Enum):
    """Execution modes for the workflow manager"""

    SEQUENTIAL = auto()  # Execute steps one after another
    PARALLEL = auto()  # Execute steps in parallel when possible
    DISTRIBUTED = auto()  # Execute steps across distributed workers


class WorkflowManager:
    """Enhanced workflow manager with distributed execution capabilities and DB integration"""

    def __init__(
        self,
        execution_mode: ExecutionMode = ExecutionMode.PARALLEL,
        max_workers: int = None,
        metrics_collector: Optional[MetricsCollector] = None,
        distributed_manager: Optional[DistributedTaskManager] = None,
        resource_monitor_interval: int = 5, # Interval for general resource monitoring
        gpu_devices: List[int] = None,
        workflow_dir: str = "./workflows_runtime_artifacts",
        enable_caching: bool = True,
        enable_prometheus: bool = False, 
        prometheus_port: int = 8000,
        use_optimization: bool = True,
        # New parameters for resource-aware scheduling
        max_queued_tasks_per_run: int = 10, # Max tasks a single workflow run can queue waiting for resources
        system_cpu_threshold_percent: float = 85.0, # System CPU threshold for queuing
        system_memory_threshold_percent: float = 90.0, # System memory threshold for queuing
        resource_check_interval_seconds: float = 2.0, # How often to check resource-limited queue
    ):
        """
        Initialize an enhanced workflow manager with extensive options

        Args:
            execution_mode: Mode of execution (SEQUENTIAL, PARALLEL, DISTRIBUTED)
            max_workers: Maximum number of parallel workers, defaults to CPU count
            metrics_collector: Optional metrics collector for monitoring
            distributed_manager: Optional distributed task manager for remote execution
            resource_monitor_interval: Interval in seconds for monitoring resource usage
            gpu_devices: List of GPU device IDs to use
            workflow_dir: Directory to store temporary workflow runtime artifacts (logs, temp files not in DB).
        """
        # Enhanced initialization with validation
        self._validate_initialization(execution_mode, max_workers, resource_monitor_interval)
        self._setup_telemetry()
        self._configure_heartbeat()
        
        # Add validation
        if max_workers is not None and max_workers <= 0:
            raise ValueError("max_workers must be positive")
            
        if resource_monitor_interval < 1:
            raise ValueError("resource_monitor_interval must be at least 1 second")
            
        # Enhanced initialization
        self._setup_directories(workflow_dir)
        self._validate_gpu_devices(gpu_devices)
        
        self.execution_mode = execution_mode
        self.max_workers = max_workers or min(32, multiprocessing.cpu_count() if multiprocessing.cpu_count() else 1)
        self.metrics_collector = metrics_collector
        self.distributed_manager = distributed_manager
        self.resource_monitor_interval = resource_monitor_interval
        self.gpu_devices = gpu_devices or []
        self.workflow_artifacts_dir = Path(workflow_dir)
        self.workflow_artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        # Store configuration options
        self.enable_caching = enable_caching
        self.enable_prometheus = enable_prometheus
        self.prometheus_port = prometheus_port
        self.use_optimization = use_optimization

        # Resource-aware scheduling config
        self.max_queued_tasks_per_run = max_queued_tasks_per_run
        self.system_cpu_threshold_percent = system_cpu_threshold_percent
        self.system_memory_threshold_percent = system_memory_threshold_percent
        self.resource_check_interval_seconds = resource_check_interval_seconds

        self.thread_executor: Optional[ThreadPoolExecutor] = None
        self.process_executor: Optional[ProcessPoolExecutor] = None
        # Active tasks in executors, mapping future to (run_db_id, step_id)
        self._active_executor_tasks: Dict[asyncio.Future, Tuple[uuid.UUID, str]] = {} 


        self.registered_workflows: Dict[str, WorkflowModel] = {}
        self.workflow_definitions_cache: Dict[uuid.UUID, List[WorkflowStepDefinitionModel]] = {}

        self.step_pid_map: Dict[str, int] = {}

        self.available_memory_mb = psutil.virtual_memory().total / (1024 * 1024) if psutil.virtual_memory() else 0
        self.available_gpu_memory = self._get_available_gpu_memory()

        self._function_resolver: Optional[Callable[[str], Callable]] = None

        self._setup_monitoring()
        
        # Initialize GPU scheduler for advanced GPU management
        if self.gpu_devices:
            self._setup_gpu_scheduler()
            
        # Setup Prometheus metrics if enabled
        if getattr(self, 'enable_prometheus', False):
            self.setup_prometheus_metrics()

        # Resource-limited queue for steps waiting for system resources
        # Stores tuples: (priority, timestamp, step_id, step_dataclass, context_repr, future, loop)
        # Timestamp is used as a tie-breaker for priority queue if priorities are equal
        self._resource_limited_queues: Dict[uuid.UUID, asyncio.PriorityQueue] = defaultdict(asyncio.PriorityQueue)
        self._resource_queue_processor_task: Optional[asyncio.Task] = None
        if self.execution_mode == ExecutionMode.PARALLEL: # Or always start it if it handles distributed too
            # Ensure an event loop is running or get the current one if in async context
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError: # No running event loop
                # This case is tricky. If WorkflowManager is initialized outside an async context,
                # creating tasks needs careful handling. For now, assume it's in an async context
                # or that the user will manage the loop for starting the processor.
                # A more robust solution might involve a dedicated thread for the asyncio loop
                # if the manager can be used in a purely synchronous way.
                logger.warning("WorkflowManager initialized outside a running asyncio event loop. "
                               "Resource queue processor might not start automatically if not in async context.")
                loop = asyncio.new_event_loop() # Temporary loop, might not be ideal
                asyncio.set_event_loop(loop)


            self._resource_queue_processor_task = asyncio.create_task(self._process_resource_limited_queues())
            logger.info("Resource-limited queue processor task created.")


        logger.info(
            f"Initialized workflow manager in {self.execution_mode.name} mode with {self.max_workers} workers."
        )
        logger.info(f"Workflow runtime artifacts will be stored in: {self.workflow_artifacts_dir}")
        if self.gpu_devices:
            logger.info(f"Using GPU devices: {self.gpu_devices}")

        # Add new performance tracking
        self.performance_stats = {
            'total_workflows': 0,
            'successful_workflows': 0,
            'failed_workflows': 0,
            'avg_duration': 0,
            'resource_usage': []
        }

    def _validate_initialization(self, execution_mode, max_workers, monitor_interval):
        """Validate all initialization parameters with precise checks"""
        if not isinstance(execution_mode, ExecutionMode):
            raise TypeError(f"execution_mode must be ExecutionMode enum, got {type(execution_mode)}")
        if max_workers is not None and (not isinstance(max_workers, int) or max_workers <= 0):
            raise ValueError("max_workers must be positive integer or None")
        if not isinstance(monitor_interval, (int, float)) or monitor_interval < 0.1:
            raise ValueError("resource_monitor_interval must be number >= 0.1")

    def _setup_telemetry(self):
        """Configure detailed performance telemetry"""
        self.telemetry = {
            'executions': [],
            'resource_usage': [],
            'step_performance': defaultdict(list),
            'error_stats': defaultdict(int)
        }
        self._last_telemetry_flush = time.time()
        
    def _configure_heartbeat(self):
        """Setup periodic health reporting"""
        self.heartbeat_interval = 60  # seconds
        self._heartbeat_thread = threading.Thread(
            target=self._run_heartbeat,
            daemon=True,
            name="WorkflowManagerHeartbeat"
        )
        self._heartbeat_thread.start()

    def _run_heartbeat(self):
        """Continuous health monitoring"""
        while True:
            try:
                self._report_health()
                time.sleep(self.heartbeat_interval)
            except Exception as e:
                logger.error(f"Heartbeat error: {str(e)}", exc_info=True)
                time.sleep(min(5, self.heartbeat_interval))  # Brief pause after error

    def _report_health(self):
        """Generate comprehensive health report"""
        report = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'active_workflows': len(self.active_workflows),
            'resource_usage': self._get_current_resources(),
            'performance': self._calculate_performance_metrics(),
            'errors': dict(self.telemetry['error_stats'])
        }
        if self.metrics_collector:
            self.metrics_collector.gauge('active_workflows', report['active_workflows'])
            self.metrics_collector.record('health_report', report)

    def set_function_resolver(self, resolver: Callable[[str], Callable]):
        """Set the function to resolve string identifiers to callable functions."""
        self._function_resolver = resolver

    def _resolve_function(self, identifier: str) -> Callable:
        """Resolve a string identifier to a callable function."""
        if not self._function_resolver:
            raise ValueError("Function resolver has not been set. Use set_function_resolver().")
        try:
            return self._function_resolver(identifier)
        except Exception as e:
            logger.error(f"Failed to resolve function identifier '{identifier}': {e}")
            raise

    def _setup_monitoring(self):
        """Set up resource monitoring and signal handling"""
        # Set up signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._handle_shutdown)
        signal.signal(signal.SIGTERM, self._handle_shutdown)

        # Start resource monitoring if enabled
        if self.resource_monitor_interval > 0:
            self._start_resource_monitor()

    def _start_resource_monitor(self):
        """Start a background thread for resource monitoring"""
        import threading

        def monitor_resources():
            while True:
                try:
                    # Update available resources
                    self.available_memory_mb = psutil.virtual_memory().available / (1024 * 1024)
                    self.available_gpu_memory = self._get_available_gpu_memory()

                    # Update resource usage for active workflows
                    # This needs to be adapted for async context if get_workflow_status becomes async
                    # For now, this is a placeholder for how it might be triggered.
                    # active_run_ids = [] # Logic to get active run_ids from DB or internal tracking needed
                    # for run_id in active_run_ids:
                    #     asyncio.run(self.get_workflow_status(run_id)) # This is problematic in a sync signal handler

                    # Report metrics if collector is available
                    if self.metrics_collector:
                        self.metrics_collector.gauge("workflow_manager_available_memory_mb", self.available_memory_mb)
                        self.metrics_collector.gauge(
                            "workflow_manager_active_workflows", len(self.registered_workflows)
                        )

                    time.sleep(self.resource_monitor_interval)
                except Exception as e:
                    logger.error(f"Error in resource monitor: {str(e)}")
                    time.sleep(self.resource_monitor_interval)

        monitor_thread = threading.Thread(target=monitor_resources, daemon=True)
        monitor_thread.start()
        logger.info(f"Started resource monitoring thread with interval {self.resource_monitor_interval}s")

    def _get_available_gpu_memory(self) -> Dict[int, int]:
        """Get available GPU memory in MB for each device"""
        gpu_memory = {}

        if not self.gpu_devices:
            return gpu_memory

        try:
            try:
                import torch
            except ImportError:
                logger.warning("PyTorch is not installed. GPU memory monitoring disabled.")
                return gpu_memory

            if torch.cuda.is_available():
                for device_id in self.gpu_devices:
                    # Get device properties
                    props = torch.cuda.get_device_properties(device_id)
                    # Get memory usage
                    torch.cuda.set_device(device_id)
                    torch.cuda.empty_cache()
                    allocated = torch.cuda.memory_allocated(device_id) / (1024 * 1024)
                    total = props.total_memory / (1024 * 1024)
                    available = total - allocated
                    gpu_memory[device_id] = available
        except Exception as e:
            logger.warning(f"Could not get GPU memory: {str(e)}")

        return gpu_memory
    def _handle_shutdown(self, signum, frame):
        logger.info(f"Shutdown signal {signum} received. Cleaning up...")

        if self.thread_executor:
            self.thread_executor.shutdown(wait=False, cancel_futures=True)
        if self.process_executor:
            self.process_executor.shutdown(wait=False, cancel_futures=True)

        if self._resource_queue_processor_task and not self._resource_queue_processor_task.done():
            self._resource_queue_processor_task.cancel()
            logger.info("Cancelled resource queue processor task.")
            # Optionally, await the task with a timeout to allow cleanup,
            # but signal handlers should be quick.
            # try:
            #     await asyncio.wait_for(self._resource_queue_processor_task, timeout=5.0)
            # except (asyncio.CancelledError, asyncio.TimeoutError):
            #     pass


        logger.info("Workflow manager shutdown complete.")
        sys.exit(0)

    async def register_workflow_definition(
        self,
        name: str,
        steps_definitions: List[WorkflowStepDefinition],
        version: str = "1.0.0",
        description: Optional[str] = None,
        default_parameters: Optional[Dict[str, Any]] = None,
        tags: Optional[List[str]] = None,
        owner_id: Optional[str] = None,
        enabled: bool = True,
    ) -> uuid.UUID:
        """
        Registers a new workflow definition in the database or updates an existing one
        based on name and version.
        """
        if not steps_definitions:
            raise ValueError("Workflow must have at least one step definition.")

        async with get_db_session() as session:
            existing_workflow_stmt = sa.select(WorkflowModel).where(
                WorkflowModel.name == name, WorkflowModel.version == version
            )
            result = await session.execute(existing_workflow_stmt)
            workflow_db_model = result.scalar_one_or_none()

            if workflow_db_model:
                workflow_db_model.description = description
                workflow_db_model.step_count = len(steps_definitions)
                workflow_db_model.default_parameters = default_parameters
                workflow_db_model.tags = tags
                workflow_db_model.owner_id = owner_id
                workflow_db_model.enabled = enabled
                workflow_db_model.updated_at = datetime.now(timezone.utc)
                logger.info(f"Updating existing workflow definition: {name} v{version} (ID: {workflow_db_model.id})")

                delete_steps_stmt = sa.delete(WorkflowStepDefinitionModel).where(
                    WorkflowStepDefinitionModel.workflow_id == workflow_db_model.id
                )
                await session.execute(delete_steps_stmt)
            else:
                workflow_db_model = WorkflowModel(
                    name=name,
                    description=description,
                    version=version,
                    step_count=len(steps_definitions),
                    enabled=enabled,
                    default_parameters=default_parameters,
                    tags=tags,
                    owner_id=owner_id,
                )
                session.add(workflow_db_model)
                await session.flush()
                logger.info(f"Registered new workflow definition: {name} v{version} (ID: {workflow_db_model.id})")

            db_step_definitions = []
            for _i, step_def_dataclass in enumerate(steps_definitions):
                if not step_def_dataclass.function_identifier:
                    raise ValueError(f"Step '{step_def_dataclass.name}' must have a function_identifier.")

                db_step = WorkflowStepDefinitionModel(
                    workflow_id=workflow_db_model.id,
                    step_id_in_workflow=step_def_dataclass.id,
                    name=step_def_dataclass.name,
                    description=step_def_dataclass.description,
                    function_identifier=step_def_dataclass.function_identifier,
                    default_parameters=step_def_dataclass.parameters,
                    dependencies=step_def_dataclass.dependencies,
                    retry_count=step_def_dataclass.retry_count,
                    timeout_seconds=step_def_dataclass.timeout_seconds,
                    estimated_memory_mb=step_def_dataclass.estimated_memory_mb,
                    estimated_duration_seconds=None,
                    priority=step_def_dataclass.priority,
                    use_gpu=step_def_dataclass.use_gpu,
                )
                db_step_definitions.append(db_step)

            session.add_all(db_step_definitions)
            await session.commit()

            self.registered_workflows[f"{name}__{version}"] = workflow_db_model
            self.workflow_definitions_cache[workflow_db_model.id] = db_step_definitions

            return workflow_db_model.id

    async def get_workflow_definition_models(
        self, workflow_identifier: Union[str, uuid.UUID], version: Optional[str] = None
    ) -> Optional[WorkflowModel]:
        """Retrieve a workflow definition model from DB by its ID or name/version."""
        async with get_db_session() as session:
            if isinstance(workflow_identifier, uuid.UUID):
                stmt = sa.select(WorkflowModel).where(WorkflowModel.id == workflow_identifier)
            elif version:  # Name and version provided
                stmt = sa.select(WorkflowModel).where(
                    WorkflowModel.name == workflow_identifier, WorkflowModel.version == version
                )
            else:  # Only name provided, try to get the latest or a default version (e.g., enabled with highest version number)
                # This logic might need refinement based on how versions are handled (e.g., 'latest' tag or semantic versioning order)
                stmt = (
                    sa.select(WorkflowModel)
                    .where(WorkflowModel.name == workflow_identifier, WorkflowModel.enabled)
                    .order_by(WorkflowModel.version.desc())
                )

            result = await session.execute(stmt)
            workflow_db_model = result.scalar_one_or_none()
            return workflow_db_model

    async def get_step_definition_models(self, workflow_db_id: uuid.UUID, session: AsyncSession = None) -> List[WorkflowStepDefinitionModel]:
        """
        Retrieve step definition models for a given workflow DB ID.
        Accepts an optional session parameter for reusing an existing database session.
        """
        # Check cache first for better performance
        if workflow_db_id in self.workflow_definitions_cache:
            return self.workflow_definitions_cache[workflow_db_id]

        # Use provided session or create a new one
        if session is not None:
            # Use existing session
            stmt = (
                sa.select(WorkflowStepDefinitionModel)
                .where(WorkflowStepDefinitionModel.workflow_id == workflow_db_id)
                .order_by(WorkflowStepDefinitionModel.id)
            )
            result = await session.execute(stmt)
            step_definitions_db = result.scalars().all()
            self.workflow_definitions_cache[workflow_db_id] = step_definitions_db
            return step_definitions_db
        else:
            # Create new session
            async with get_db_session() as new_session:
                stmt = (
                    sa.select(WorkflowStepDefinitionModel)
                    .where(WorkflowStepDefinitionModel.workflow_id == workflow_db_id)
                    .order_by(WorkflowStepDefinitionModel.id)
                )
                result = await new_session.execute(stmt)
                step_definitions_db = result.scalars().all()
                self.workflow_definitions_cache[workflow_db_id] = step_definitions_db
                return step_definitions_db

    async def create_workflow_run(
        self,
        workflow_identifier: Union[str, uuid.UUID],  # Name or DB ID of the workflow definition
        parameters: Dict[str, Any],
        version: Optional[str] = None,  # Specify if workflow_identifier is a name
        triggered_by: Optional[str] = None,
        custom_metadata: Optional[Dict[str, Any]] = None,
    ) -> uuid.UUID:  # Returns the DB ID of the WorkflowRun
        """Creates a new workflow run record in the database."""
        async with get_db_session() as session:
            workflow_db_model = await self.get_workflow_definition_models(workflow_identifier, version)
            if not workflow_db_model:
                id_str = str(workflow_identifier) + (f" v{version}" if version else "")
                raise ValueError(f"Workflow definition '{id_str}' not found or not unique.")

            run_db_model = WorkflowRunModel(
                workflow_id=workflow_db_model.id,
                parameters=parameters,
                status=StatusEnum.PENDING,
                triggered_by=triggered_by,
                custom_metadata=custom_metadata,
                # created_at, updated_at by TimestampMixin. id by default.
            )
            session.add(run_db_model)
            await session.commit()
            await session.refresh(run_db_model)  # To get the generated ID and defaults
            logger.info(
                f"Created workflow run with DB ID: {run_db_model.id} for workflow '{workflow_db_model.name}' v{workflow_db_model.version}"
            )
            return run_db_model.id

    def _build_workflow_graph(self, steps_definitions_dataclasses: List[WorkflowStepDefinition]) -> nx.DiGraph:
        """Builds a directed acyclic graph (DAG) from workflow step dataclasses."""
        G = nx.DiGraph()
        step_map = {step.id: step for step in steps_definitions_dataclasses}

        for step_dataclass in steps_definitions_dataclasses:
            G.add_node(step_dataclass.id, step_dataclass=step_dataclass)

        for step_dataclass in steps_definitions_dataclasses:
            for dep_id in step_dataclass.dependencies:
                if dep_id not in step_map:
                    raise ValueError(f"Step '{step_dataclass.id}' has an unknown dependency: '{dep_id}'.")
                G.add_edge(dep_id, step_dataclass.id)

        if not nx.is_directed_acyclic_graph(G):
            cycles = list(nx.simple_cycles(G))
            raise ValueError(f"Workflow contains cyclic dependencies: {cycles}")
        return G

    async def execute_workflow(self, run_db_id: uuid.UUID, use_cache: bool = None) -> WorkflowContext:
        """Enhanced execution with detailed performance tracking and optional caching"""
        start_time = time.time()
        
        # Determine if caching should be used
        if use_cache is None:
            use_cache = self.enable_caching
        
        try:
            # Track resource usage at start
            initial_resources = self._get_system_resources()
            
            async with get_db_session() as session:  # type: AsyncSession
                run_db_model = await session.get(WorkflowRunModel, run_db_id)
                if not run_db_model:
                    raise ValueError(f"Workflow run with DB ID '{run_db_id}' not found.")

                if run_db_model.status not in [
                    StatusEnum.PENDING,
                    StatusEnum.QUEUED,
                    StatusEnum.FAILED,
                ]:  # Allow re-running FAILED ones
                    logger.warning(
                        f"Workflow run '{run_db_id}' has status {run_db_model.status.name} and cannot be started."
                    )
                    # Potentially return a context based on the current DB state or raise error
                    # For now, let's create a basic context and return
                    workflow_def_model = await session.get(WorkflowModel, run_db_model.workflow_id)
                    return WorkflowContext(
                        workflow_db_id=run_db_model.workflow_id,
                        run_db_id=run_db_model.id,
                        workflow_id=workflow_def_model.name if workflow_def_model else "UnknownWorkflow",
                        run_id=str(run_db_model.id),  # Using DB ID as the run_id for context if not otherwise set
                        parameters=run_db_model.parameters or {},
                        status=StepExecutionStatus.from_db_status(run_db_model.status).name,
                        start_time=run_db_model.started_at,
                        end_time=run_db_model.completed_at,
                        error=run_db_model.error_message,
                    )

                workflow_db_model = await session.get(WorkflowModel, run_db_model.workflow_id)
                if not workflow_db_model:
                    error_msg = f"Workflow definition (ID: {run_db_model.workflow_id}) associated with run '{run_db_id}' not found."
                    run_db_model.status = StatusEnum.FAILED
                    run_db_model.error_message = error_msg
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    await session.commit()
                    raise ValueError(error_msg)

                step_definitions_db = await self.get_step_definition_models(workflow_db_model.id)
                if not step_definitions_db:
                    error_msg = (
                        f"No step definitions found for workflow '{workflow_db_model.name}' (ID: {workflow_db_model.id})."
                    )
                    run_db_model.status = StatusEnum.FAILED
                    run_db_model.error_message = error_msg
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    await session.commit()
                    raise ValueError(error_msg)

                # Convert DB step definition models to manager's WorkflowStepDefinition dataclasses
                steps_dataclasses = []
                for step_db in step_definitions_db:
                    steps_dataclasses.append(
                        WorkflowStepDefinition(
                            id=step_db.step_id_in_workflow,
                            name=step_db.name,
                            description=step_db.description or "",
                            function_identifier=step_db.function_identifier or "",  # Ensure not None
                            parameters=step_db.default_parameters or {},
                            dependencies=step_db.dependencies or [],
                            timeout_seconds=step_db.timeout_seconds,
                            retry_count=step_db.retry_count or 0,
                            priority=step_db.priority or 0,
                            estimated_memory_mb=step_db.estimated_memory_mb or 1000,
                            use_gpu=step_db.use_gpu or False,
                            # _resolved_function is set later before execution
                        )
                    )

                # Build workflow graph using the dataclasses
                try:
                    graph = self._build_workflow_graph(steps_dataclasses)
                except ValueError as e:
                    error_msg = f"Failed to build workflow graph for run '{run_db_id}': {e}"
                    run_db_model.status = StatusEnum.FAILED
                    run_db_model.error_message = error_msg
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    await session.commit()
                    raise ValueError(error_msg) from e

                # Initialize WorkflowContext for this run
                # The `run_id` in context was originally a string; here we use the DB UUID string.
                # The `workflow_id` in context was also a string; using the workflow definition name.
                context = WorkflowContext(
                    workflow_db_id=workflow_db_model.id,
                    run_db_id=run_db_model.id,
                    workflow_id=workflow_db_model.name,
                    run_id=str(run_db_model.id),
                    parameters=run_db_model.parameters or {},
                    metadata=run_db_model.custom_metadata or {},
                    start_time=datetime.now(timezone.utc),
                    status=StepExecutionStatus.RUNNING.name,  # Set context status
                )

                # Update DB: WorkflowRun status to RUNNING and started_at
                run_db_model.status = StatusEnum.RUNNING
                run_db_model.started_at = context.start_time
                await session.commit()

                logger.info(
                    f"Executing workflow run {run_db_id} for workflow '{workflow_db_model.name}' (Definition ID: {workflow_db_model.id})"
                )

                # Make step_dataclasses accessible to execution methods, perhaps by passing directly
                # or by associating them with the context in a structured way if context is passed around.
                # For now, passing them directly to the execution strategy methods.
                step_dataclasses_map = {s.id: s for s in steps_dataclasses}

                # Fetch workflow-level default parameters
                workflow_default_params = workflow_db_model.default_parameters or {}
                context.metadata["workflow_default_params"] = workflow_default_params  # Store for _execute_step

                final_context: WorkflowContext
                try:
                    if self.execution_mode == ExecutionMode.SEQUENTIAL:
                        final_context = await self._execute_sequential(run_db_id, graph, context, step_dataclasses_map)
                    elif self.execution_mode == ExecutionMode.PARALLEL:
                        final_context = await self._execute_parallel(run_db_id, graph, context, step_dataclasses_map)
                    elif self.execution_mode == ExecutionMode.DISTRIBUTED:
                        if not self.distributed_manager:
                            raise ValueError("DistributedTaskManager not configured for DISTRIBUTED mode.")
                        final_context = await self._execute_distributed(run_db_id, graph, context, step_dataclasses_map)
                    else:
                        raise ValueError(f"Unsupported execution mode: {self.execution_mode}")
                except Exception as e:
                    logger.error(f"Core execution error for run '{run_db_id}': {e}", exc_info=True)
                    context.status = StepExecutionStatus.FAILED.name
                    context.error = traceback.format_exc()
                    final_context = context  # Use the current context which has the error

                # Finalize WorkflowRun in DB
                run_db_model = await session.get(
                    WorkflowRunModel, run_db_id
                )  # Re-fetch to avoid detached instance issues potentially
                if run_db_model:
                    run_db_model.status = StatusEnum[final_context.status]  # Convert back from context string status
                    run_db_model.results = final_context.results
                    run_db_model.artifacts = final_context.artifacts
                    run_db_model.logs_summary = "\n".join(final_context.logs[-100:])  # Store last 100 log lines as summary
                    run_db_model.completed_at = datetime.now(timezone.utc)
                    run_db_model.duration_seconds = (
                        (run_db_model.completed_at - run_db_model.started_at).total_seconds()
                        if run_db_model.started_at
                        else None
                    )
                    run_db_model.error_message = final_context.error
                    
                    # Add workflow metrics to custom_metadata
                    if not run_db_model.custom_metadata:
                        run_db_model.custom_metadata = {}
                        
                    run_db_model.custom_metadata["performance"] = {
                        "start_time": start_time,
                        "end_time": time.time(),
                        "total_duration": time.time() - start_time,
                        "avg_step_duration": sum(d.get("duration", 0) for d in final_context.step_times.values()) / len(final_context.step_times) if final_context.step_times else None,
                        "peak_memory_mb": max(final_context.resource_usage.get("memory_mb", [0])) if final_context.resource_usage.get("memory_mb") else None,
                        "avg_cpu_percent": sum(final_context.resource_usage.get("cpu_percent", [0])) / len(final_context.resource_usage.get("cpu_percent", [])) if final_context.resource_usage.get("cpu_percent") else None
                    }
                    
                    # If successful and caching enabled, mark as cacheable
                    if use_cache and final_context.status == StepExecutionStatus.COMPLETED.name:
                        await self.cache_workflow_result(run_db_id, workflow_db_model.name, run_db_model.parameters or {})
                    
                    # Update Prometheus metrics if enabled
                    if self.enable_prometheus and hasattr(self, 'prom_workflow_durations'):
                        self.prom_workflow_durations.labels(
                            workflow_name=workflow_db_model.name,
                            status=final_context.status
                        ).observe(run_db_model.duration_seconds or 0)
                        
                    # error_traceback could be stored if context has it explicitly
                    await session.commit()
                    logger.info(f"Workflow run {run_db_id} finished with status: {run_db_model.status.name}")
                else:
                    logger.error(f"Could not find WorkflowRun {run_db_id} in DB for final update.")

                # Track successful completion
                self.performance_stats['successful_workflows'] += 1
                return context
            
        except Exception as e:
            # Enhanced error tracking
            self.performance_stats['failed_workflows'] += 1
            if self.metrics_collector:
                self.metrics_collector.event(
                    "workflow_failed",
                    {"run_id": str(run_db_id), "error": str(e)}
                )
            raise
            
        finally:
            # Update performance metrics
            duration = time.time() - start_time
            self.performance_stats['total_workflows'] += 1
            self.performance_stats['avg_duration'] = (
                (self.performance_stats['avg_duration'] * (self.performance_stats['total_workflows'] - 1) + duration) 
                / self.performance_stats['total_workflows']
            )

    def _get_system_resources(self) -> Dict[str, Any]:
        """Get detailed system resource metrics"""
        return {
            'timestamp': time.time(),
            'cpu': psutil.cpu_percent(percpu=True),
            'memory': psutil.virtual_memory()._asdict(),
            'disk': psutil.disk_usage('/')._asdict(),
            'network': psutil.net_io_counters()._asdict()
        }

    async def _execute_sequential(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        """
        Execute workflow steps sequentially in dependency order

        Args:
            run_db_id: ID of the workflow run
            graph: Directed acyclic graph of steps
            context: Workflow context
            steps_map: Map of step_id_in_workflow to WorkflowStepDefinition dataclass

        Returns:
            Updated workflow context
        """
        logger.info(f"Executing workflow run {run_db_id} in sequential mode")

        for step_id_in_workflow in nx.topological_sort(graph):
            step_dataclass = graph.nodes[step_id_in_workflow]["step_dataclass"]

            # Check for cancellation
            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"Workflow run {run_db_id} cancelled. Skipping step {step_id_in_workflow}.")
                # Persist cancellation for this step if not already done
                async with get_db_session() as session:
                    await self._create_or_update_step_run(
                        session,
                        run_db_id,
                        step_dataclass,
                        StatusEnum.CANCELLED,
                        message="Workflow cancelled by higher context",
                        completed_at=datetime.now(timezone.utc),
                    )
                    await session.commit()
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus.CANCELLED
                continue

            # Check dependencies
            dep_failed = False
            for dep_id in graph.predecessors(step_id_in_workflow):
                if context.step_statuses.get(dep_id) == StepExecutionStatus.FAILED:
                    context.step_statuses[step_id_in_workflow] = StepExecutionStatus.SKIPPED
                    logger.info(
                        f"Skipping step {step_id_in_workflow} due to failed dependency {dep_id} in run {run_db_id}"
                    )
                    async with get_db_session() as session:
                        await self._create_or_update_step_run(
                            session,
                            run_db_id,
                            step_dataclass,
                            StatusEnum.SKIPPED,
                            message=f"Skipped due to failed dependency: {dep_id}",
                            completed_at=datetime.now(timezone.utc),
                        )
                        await session.commit()
                    dep_failed = True
                    break
            if dep_failed:
                continue

            # Execute step
            try:
                step_result_dict = await self._execute_step(run_db_id, step_dataclass, context)

                # Update context with step result
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus[step_result_dict["status"]]
                if "result" in step_result_dict:
                    context.results[step_id_in_workflow] = step_result_dict["result"]
                if step_result_dict["status"] == StepExecutionStatus.FAILED.name:
                    logger.error(
                        f"Step {step_id_in_workflow} failed in run {run_db_id}: {step_result_dict.get('error')}"
                    )
                    if not context.parameters.get("continue_on_failure", False):
                        context.status = StepExecutionStatus.FAILED.name
                        context.error = f"Step {step_id_in_workflow} failed. {step_result_dict.get('error')}"
                        # Persist overall failure to WorkflowRun
                        async with get_db_session() as session:
                            run_model = await session.get(WorkflowRunModel, run_db_id)
                            if run_model:
                                run_model.status = StatusEnum.FAILED
                                run_model.error_message = context.error
                                run_model.completed_at = datetime.now(timezone.utc)
                                await session.commit()
                        return context  # Stop further execution
            except Exception as e:
                logger.error(
                    f"Error during execution of step {step_id_in_workflow} (run {run_db_id}): {e}", exc_info=True
                )
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus.FAILED
                context.status = StepExecutionStatus.FAILED.name
                context.error = (
                    f"Step {step_id_in_workflow} encountered an unhandled exception: {traceback.format_exc()}"
                )
                async with get_db_session() as session:
                    # Error is logged by _execute_step, here we update the overall run
                    # _execute_step handles its own DB update for the step run
                    run_model = await session.get(WorkflowRunModel, run_db_id)
                    await self._create_or_update_step_run(
                        session,
                        run_db_id,
                        step_dataclass,
                        StatusEnum.FAILED,
                        error_message=str(e),
                        error_traceback=traceback.format_exc(),
                        completed_at=datetime.now(timezone.utc),
                    )
                    await session.commit()

                return context  # Stop further execution

        if context.status == StepExecutionStatus.RUNNING.name:  # If no step failed and not cancelled
            context.status = StepExecutionStatus.COMPLETED.name
        return context

    async def _execute_parallel(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        """
        Execute workflow steps in parallel where possible

        Args:
            run_db_id: ID of the workflow run
            graph: Directed acyclic graph of steps
            context: Workflow context
            steps_map: Map of step_id_in_workflow to WorkflowStepDefinition dataclass

        Returns:
            Updated workflow context
        """
        logger.info(f"Executing workflow run {run_db_id} in parallel mode")

        if self.thread_executor is None:
            self.thread_executor = ThreadPoolExecutor(
                max_workers=self.max_workers, thread_name_prefix="WorkflowThread"
            )
        if self.process_executor is None:
            self.process_executor = ProcessPoolExecutor(
                max_workers=self.max_workers
            )  # TODO: Consider separate max_workers for processes

        completed_steps: Set[str] = set()
        submitted_steps: Set[str] = set()
        futures: Dict[str, asyncio.Future] = {}  # Maps step_id_in_workflow to asyncio.Future

        while len(completed_steps) < len(graph.nodes):
            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"Workflow run {run_db_id} cancelled. Halting parallel execution.")
                break  # Exit main loop on cancellation

            ready_to_submit_nodes = []
            for node_id in graph.nodes:
                if node_id in completed_steps or node_id in submitted_steps:
                    continue

                dependencies = set(graph.predecessors(node_id))
                if dependencies.issubset(completed_steps):
                    # Check if any dependency failed
                    dep_failed_or_skipped = False
                    for dep_id in dependencies:
                        if context.step_statuses.get(dep_id) in [
                            StepExecutionStatus.FAILED,
                            StepExecutionStatus.SKIPPED,
                        ]:
                            context.step_statuses[node_id] = StepExecutionStatus.SKIPPED
                            logger.info(
                                f"Skipping step {node_id} due to failed/skipped dependency {dep_id} in run {run_db_id}"
                            )
                            async with get_db_session() as session:
                                step_dataclass_for_skip = graph.nodes[node_id]["step_dataclass"]
                                await self._create_or_update_step_run(
                                    session,
                                    run_db_id,
                                    step_dataclass_for_skip,
                                    StatusEnum.SKIPPED,
                                    message=f"Skipped due to failed/skipped dependency: {dep_id}",
                                    completed_at=datetime.now(timezone.utc),
                                )
                                await session.commit()
                            completed_steps.add(node_id)  # Mark as completed for loop progression
                            dep_failed_or_skipped = True
                            break
                    if not dep_failed_or_skipped:
                        ready_to_submit_nodes.append(node_id)

            for node_id in ready_to_submit_nodes:
                if context.status == StepExecutionStatus.CANCELLED.name:
                    break  # Check before submitting new tasks

                step_dataclass = graph.nodes[node_id]["step_dataclass"]

                # Create an asyncio Future to bridge thread/process pool with asyncio
                loop = asyncio.get_running_loop()
                future = loop.create_future()
                futures[node_id] = future

                context_dict_for_process = context.to_dict() if step_dataclass.use_process else None

                if not step_dataclass.use_process:
                    self.thread_executor.submit(
                        self._run_step_task_for_executor, run_db_id, step_dataclass, context, future, loop
                    )
                else:
                    self.process_executor.submit(
                        self._run_step_task_for_executor,
                        run_db_id,
                        step_dataclass,
                        context_dict_for_process,
                        future,
                        loop,  # Pass context_dict
                    )

                submitted_steps.add(node_id)
                context.step_statuses[node_id] = (
                    StepExecutionStatus.QUEUED
                )  # Or directly to RUNNING if resources are immediately available
                context.add_log(f"Submitted step {node_id} for execution in run {run_db_id}")
                async with get_db_session() as session:
                    await self._create_or_update_step_run(
                        session,
                        run_db_id,
                        step_dataclass,
                        StatusEnum.QUEUED,
                        parameters_used=step_dataclass.parameters,
                    )

            if not futures:  # No tasks running
                await asyncio.sleep(0.1)  # Short sleep to prevent busy loop if no tasks are ready yet
                continue

            done, pending = await asyncio.wait(futures.values(), return_when=asyncio.FIRST_COMPLETED, timeout=1.0)

            for future in done:
                node_id = [nid for nid, f in futures.items() if f == future][0]  # Find node_id for this future
                try:
                    step_result_dict = future.result()  # This is the dict from _execute_step
                    context.step_statuses[node_id] = StepExecutionStatus[step_result_dict["status"]]
                    if "result" in step_result_dict:
                        context.results[node_id] = step_result_dict["result"]

                    if step_result_dict["status"] == StepExecutionStatus.FAILED.name:
                        logger.error(f"Step {node_id} failed in run {run_db_id}: {step_result_dict.get('error')}")
                        # Error and status already logged to DB by _execute_step
                        if not context.parameters.get("continue_on_failure", False):
                            context.status = StepExecutionStatus.FAILED.name
                            context.error = f"Step {node_id} failed. {step_result_dict.get('error')}"
                            # Propagate overall failure and cancel other tasks
                            # This needs a mechanism to cancel other futures gracefully
                            # For now, just setting context status
                            # TODO: Implement cancellation of other futures.
                            # return context # Early exit on failure if not continue_on_failure
                except Exception as e:
                    logger.error(f"Error processing result for step {node_id} in run {run_db_id}: {e}", exc_info=True)
                    context.step_statuses[node_id] = StepExecutionStatus.FAILED
                    context.status = StepExecutionStatus.FAILED.name  # Mark overall workflow as failed
                    context.error = f"Error processing result for step {node_id}: {traceback.format_exc()}"
                    async with get_db_session() as session:
                        # Error for step run is handled by _execute_step. This is for overall workflow.
                        # However, if future.result() itself fails with an unexpected error NOT from _execute_step's return dict:
                        step_dataclass_for_fail = graph.nodes[node_id]["step_dataclass"]
                        await self._create_or_update_step_run(
                            session,
                            run_db_id,
                            step_dataclass_for_fail,
                            StatusEnum.FAILED,
                            error_message=str(e),
                            error_traceback=traceback.format_exc(),
                            completed_at=datetime.now(timezone.utc),
                        )
                        await session.commit()

                completed_steps.add(node_id)
                del futures[node_id]  # Remove from active futures

            # If overall context is FAILED or CANCELLED, try to cancel remaining futures.
            if context.status in [StepExecutionStatus.FAILED.name, StepExecutionStatus.CANCELLED.name]:
                for f_node_id, f in futures.items():
                    if not f.done():
                        f.cancel()
                        logger.info(
                            f"Attempting to cancel step {f_node_id} for run {run_db_id} due to overall status: {context.status}"
                        )
                        # Update DB status for these steps being cancelled
                        async with get_db_session() as session:
                            step_dataclass_for_cancel = graph.nodes[f_node_id]["step_dataclass"]
                            await self._create_or_update_step_run(
                                session,
                                run_db_id,
                                step_dataclass_for_cancel,
                                StatusEnum.CANCELLED,
                                message=f"Workflow {context.status.lower()}",
                                completed_at=datetime.now(timezone.utc),
                            )
                            await session.commit()
                if not pending:  # All futures processed or cancelled
                    break

        if context.status == StepExecutionStatus.RUNNING.name:  # If not FAILED or CANCELLED
            if len(completed_steps) == len(graph.nodes):
                all_successful_or_skipped = True
                for step_id in graph.nodes:
                    if context.step_statuses.get(step_id) not in [
                        StepExecutionStatus.COMPLETED,
                        StepExecutionStatus.SKIPPED,
                    ]:
                        all_successful_or_skipped = False
                        break
                if all_successful_or_skipped:
                    context.status = StepExecutionStatus.COMPLETED.name
                else:  # Some steps might still be PENDING/QUEUED if continue_on_failure was true and a failure occurred.
                    context.status = StepExecutionStatus.FAILED.name  # Or a more specific "INCOMPLETE" status
                    if not context.error:
                        context.error = "Workflow finished with non-completed steps."

            else:  # Not all steps completed (e.g. due to cancellation or early exit from loop)
                if context.status == StepExecutionStatus.RUNNING.name:  # If not already set to FAILED/CANCELLED
                    context.status = StepExecutionStatus.FAILED.name  # Or INCOMPLETE
                    if not context.error:
                        context.error = "Workflow did not complete all steps."

        return context

    def _run_step_task_for_executor(
        self,
        run_db_id: uuid.UUID,
        step_dataclass: WorkflowStepDefinition,
        # Pass a serializable version of context for processes, full context for threads
        context_repr: Union[WorkflowContext, Dict[str, Any]], 
        asyncio_future: asyncio.Future,
        loop: asyncio.AbstractEventLoop,
    ):
        """
        Helper function to run _execute_step_core and set result/exception on an asyncio.Future.
        This is called by the thread/process pool executor.
        Handles context reconstruction for process-based execution.
        """
        current_pid = os.getpid()
        log_prefix = f"[ExecutorTask PID:{current_pid} Step:{step_dataclass.id} Run:{run_db_id}]"
        logger.info(f"{log_prefix} Starting task execution.")

        reconstructed_context: Optional[WorkflowContext] = None
        original_context_passed_to_process = False

        try:
            if isinstance(context_repr, WorkflowContext):
                # This branch is for ThreadPoolExecutor
                reconstructed_context = context_repr
                logger.debug(f"{log_prefix} Using direct WorkflowContext in thread.")
            elif isinstance(context_repr, dict) and step_dataclass.use_process:
                # This branch is for ProcessPoolExecutor
                original_context_passed_to_process = True
                logger.info(f"{log_prefix} Reconstructing WorkflowContext from dict for process execution.")
                try:
                    # Essential fields for _execute_step_core and _call_function
                    workflow_db_id_uuid = uuid.UUID(context_repr["workflow_db_id"])
                    run_db_id_uuid = uuid.UUID(context_repr["run_db_id"])
                    
                    reconstructed_context = WorkflowContext(
                        workflow_db_id=workflow_db_id_uuid,
                        run_db_id=run_db_id_uuid,
                        workflow_id=context_repr.get("workflow_id", "UnknownWorkflow"),
                        run_id=context_repr.get("run_id", str(run_db_id_uuid)),
                        parameters=context_repr.get("parameters", {}),
                        results=context_repr.get("results", {}), # Crucial for dependency resolution
                        artifacts=context_repr.get("artifacts", {}),
                        metadata=context_repr.get("metadata", {}),
                        logs=[], # Logs are specific to this execution context, don't inherit old ones
                        # status, start_time, end_time, error are managed by _execute_step_core
                        resource_usage=defaultdict(list), # Fresh resource usage for this step/process
                        step_statuses= {k: StepExecutionStatus[v] for k, v in context_repr.get("step_statuses", {}).items()},
                        step_times=context_repr.get("step_times", {})
                    )
                    logger.info(f"{log_prefix} Successfully reconstructed context.")
                except KeyError as ke:
                    logger.error(f"{log_prefix} Missing essential key for context reconstruction: {ke}", exc_info=True)
                    # Cannot proceed without a valid context if reconstruction fails
                    # Set exception on future and return
                    if loop.is_running() and not asyncio_future.done():
                        loop.call_soon_threadsafe(asyncio_future.set_exception, ValueError(f"Context reconstruction failed: missing {ke}"))
                    return 
                except Exception as e_recon:
                    logger.error(f"{log_prefix} General error during context reconstruction: {e_recon}", exc_info=True)
                    if loop.is_running() and not asyncio_future.done():
                        loop.call_soon_threadsafe(asyncio_future.set_exception, ValueError(f"Context reconstruction error: {e_recon}"))
                    return
            else:
                err_msg = f"{log_prefix} Invalid context_repr type: {type(context_repr)}. Expected WorkflowContext or dict (for process)."
                logger.error(err_msg)
                if loop.is_running() and not asyncio_future.done():
                    loop.call_soon_threadsafe(asyncio_future.set_exception, TypeError(err_msg))
                return

            if reconstructed_context is None:
                # Should have been caught above, but as a safeguard
                err_msg = f"{log_prefix} Context could not be established."
                logger.error(err_msg)
                if loop.is_running() and not asyncio_future.done():
                    loop.call_soon_threadsafe(asyncio_future.set_exception, ValueError(err_msg))
                return

            # Setup Process-Specific Function Resolver Cache (if in a process)
            # The main manager's _function_resolver isn't directly available in a new process.
            # If _resolve_function is called within the process, it needs a way to get the resolver.
            # This is a complex problem if the resolver itself isn't easily picklable or globally accessible.
            # For now, assuming _resolve_function (if called by step func) handles this, or function is pre-resolved.
            # The current design pre-resolves function in _execute_step_core *before* sending to executor if possible.
            
            # Execute the core step logic
            # asyncio.run() creates a new event loop, which is necessary if this task runs in a separate process
            # or a thread not managed by the main asyncio loop where `self.execute_workflow` runs.
            result = asyncio.run(self._execute_step_core(run_db_id, step_dataclass, reconstructed_context))
            
            logger.info(f"{log_prefix} Execution finished. Setting result on future.")
            if loop.is_running() and not asyncio_future.done():
                # If the step produced a new context (e.g. in a process), we might want to return parts of it.
                # However, WorkflowContext itself is not easily picklable if it contains complex objects.
                # The primary result is the step's output, context changes are typically updated in DB by _execute_step_core.
                loop.call_soon_threadsafe(asyncio_future.set_result, result)
            elif not loop.is_running() and not asyncio_future.done():
                 logger.warning(f"{log_prefix} Main loop is not running. Setting result directly (might be problematic).")
                 asyncio_future.set_result(result)
            else:
                logger.warning(f"{log_prefix} Future already done or loop closed. Cannot set result.")

        except Exception as e:
            logger.error(f"{log_prefix} Unhandled error in executor task: {e}", exc_info=True)
            error_info = {
                'status': StepExecutionStatus.FAILED.name,
                'error': str(e),
                'traceback': traceback.format_exc()
            }
            # Attempt to update the DB for this step to FAILED if an unexpected error occurs here.
            # This is a best-effort basis as we are outside the main async flow of _execute_step_core.
            try:
                # Create a synchronous session or a new async event loop for this isolated DB update.
                # This is complex. For now, we rely on _execute_step_core to handle its DB states mostly.
                # The primary goal here is to set the exception on the future.
                logger.error(f"{log_prefix} Critical error in executor task wrapper for step {step_dataclass.id}. DB update for this specific error is not yet implemented here.")
            except Exception as db_err:
                logger.error(f"{log_prefix} Failed to update DB for critical error: {db_err}")
            
            if loop.is_running() and not asyncio_future.done():
                # Propagate the exception to the main asyncio loop that is awaiting the future.
                # The `e` here is the exception from the `try` block above.
                loop.call_soon_threadsafe(asyncio_future.set_exception, e) 
            elif not asyncio_future.done():
                logger.warning(f"{log_prefix} Main loop not running. Setting exception directly.")
                asyncio_future.set_exception(e)
            else:
                logger.warning(f"{log_prefix} Future already done or loop closed. Cannot set exception.")

    async def _create_or_update_step_run(
        self,
        session: AsyncSession,
        run_db_id: uuid.UUID,
        step_dataclass: WorkflowStepDefinition,
        status: StatusEnum,
        step_run_db_id: Optional[uuid.UUID] = None,
        parameters_used: Optional[Dict[str, Any]] = None,
        result: Optional[Any] = None,
        artifacts_produced: Optional[Dict[str, str]] = None,
        logs: Optional[str] = None,
        started_at: Optional[datetime] = None,
        completed_at: Optional[datetime] = None,
        duration_seconds: Optional[float] = None,
        error_message: Optional[str] = None,
        error_traceback: Optional[str] = None,
        retry_attempt: Optional[int] = None,
        host_machine: Optional[str] = None,
        message: Optional[str] = None,
        resource_metrics: Optional[Dict[str, Any]] = None, 
    ) -> WorkflowStepRunModel:
        step_run_model: Optional[WorkflowStepRunModel] = None
        log_entry = message or (
            f"Status update: {status.name}" + (f" | Error: {error_message}" if error_message else "")
        )
        final_logs = logs
        if not final_logs and log_entry:
            final_logs = log_entry
        elif final_logs and log_entry and final_logs != log_entry: # Avoid duplicate log entry
            final_logs = final_logs + "\\\\n" + log_entry

        if step_run_db_id:
            step_run_model = await session.get(WorkflowStepRunModel, step_run_db_id)
            if not step_run_model:
                logger.warning(
                    f"Tried to update WorkflowStepRun DB ID {step_run_db_id} but not found (run {run_db_id}, step {step_dataclass.id}). Will try to find/create."
                )
                step_run_db_id = None 

        if not step_run_model: 
            stmt = (
                sa.select(WorkflowStepRunModel)
                .where(
                    WorkflowStepRunModel.workflow_run_id == run_db_id,
                    WorkflowStepRunModel.step_id_in_workflow == step_dataclass.id,
                )
                .order_by(WorkflowStepRunModel.created_at.desc()) 
            )
            existing_runs = (await session.execute(stmt)).scalars().all()
            if existing_runs:
                 if step_run_db_id and any(sr.id == step_run_db_id for sr in existing_runs):
                     step_run_model = next(sr for sr in existing_runs if sr.id == step_run_db_id)
                 else: 
                     step_run_model = existing_runs[0]


        if step_run_model:  # Update existing
            step_run_model.status = status
            if parameters_used is not None: step_run_model.parameters_used = parameters_used
            if result is not None: step_run_model.result = result
            if artifacts_produced is not None: step_run_model.artifacts_produced = artifacts_produced
            if final_logs:
                if step_run_model.logs and step_run_model.logs != final_logs:
                    new_logs = step_run_model.logs + "\n" + final_logs
                else:
                    new_logs = final_logs
                step_run_model.logs = new_logs[-20000:] if new_logs else None


            if started_at is not None and step_run_model.started_at is None:
                 step_run_model.started_at = started_at
            if completed_at is not None: step_run_model.completed_at = completed_at
            if duration_seconds is not None: step_run_model.duration_seconds = duration_seconds
            
            if error_message is not None: step_run_model.error_message = error_message
            elif status not in [StatusEnum.FAILED, StatusEnum.TIMEOUT]: step_run_model.error_message = None
            if error_traceback is not None: step_run_model.error_traceback = error_traceback
            elif status not in [StatusEnum.FAILED, StatusEnum.TIMEOUT]: step_run_model.error_traceback = None
                
            if retry_attempt is not None: step_run_model.retry_attempt = retry_attempt
            if host_machine is not None: step_run_model.host_machine = host_machine
            
            if resource_metrics:
                if not step_run_model.custom_metadata: step_run_model.custom_metadata = {}
                attempt_key = f"attempt_{retry_attempt if retry_attempt is not None else 0}_metrics"
                step_run_model.custom_metadata[attempt_key] = resource_metrics
                
                if hasattr(step_run_model, 'peak_memory_mb') and 'peak_memory_mb' in resource_metrics:
                    step_run_model.peak_memory_mb = resource_metrics['peak_memory_mb']
                if hasattr(step_run_model, 'average_cpu_percent') and 'average_cpu_percent' in resource_metrics:
                    step_run_model.average_cpu_percent = resource_metrics['average_cpu_percent']

            step_run_model.updated_at = datetime.now(timezone.utc)
            logger.debug(f"Updating StepRun {step_run_model.id} for step {step_dataclass.id}, status {status.name}, attempt {retry_attempt}")
        else:  
            run_model_for_workflow_id = await session.get(WorkflowRunModel, run_db_id)
            if not run_model_for_workflow_id:
                raise ValueError(f"Run {run_db_id} not found, cannot create step run for {step_dataclass.id}")

            step_def_id_stmt = sa.select(WorkflowStepDefinitionModel.id).where(
                WorkflowStepDefinitionModel.workflow_id == run_model_for_workflow_id.workflow_id,
                WorkflowStepDefinitionModel.step_id_in_workflow == step_dataclass.id,
            )
            step_definition_db_id = (await session.execute(step_def_id_stmt)).scalar_one_or_none()
            if not step_definition_db_id:
                raise ValueError(f"Step def for {step_dataclass.id} in workflow {run_model_for_workflow_id.workflow_id} not found.")

            current_custom_metadata = {}
            if resource_metrics:
                current_custom_metadata[f"attempt_{retry_attempt if retry_attempt is not None else 0}_metrics"] = resource_metrics

            step_run_model = WorkflowStepRunModel(
                workflow_run_id=run_db_id,
                step_definition_id=step_definition_db_id,
                step_id_in_workflow=step_dataclass.id,
                status=status,
                parameters_used=(parameters_used or step_dataclass.parameters),
                result=result, artifacts_produced=artifacts_produced, logs=final_logs,
                started_at=started_at, completed_at=completed_at, duration_seconds=duration_seconds,
                error_message=error_message, error_traceback=error_traceback,
                retry_attempt=(retry_attempt or 0),
                host_machine=(host_machine or socket.gethostname() if hasattr(socket, "gethostname") else None),
                custom_metadata=current_custom_metadata,
                peak_memory_mb=resource_metrics.get('peak_memory_mb') if resource_metrics and hasattr(WorkflowStepRunModel, 'peak_memory_mb') else None,
                average_cpu_percent=resource_metrics.get('average_cpu_percent') if resource_metrics and hasattr(WorkflowStepRunModel, 'average_cpu_percent') else None,
            )
            session.add(step_run_model)
            await session.flush()
            logger.debug(f"Created new StepRun {step_run_model.id} for step {step_dataclass.id}, status {status.name}, attempt {retry_attempt}")
        
        await session.flush()
        return step_run_model

    async def _execute_step_core(
        self, 
        run_db_id: uuid.UUID, 
        step_dataclass: WorkflowStepDefinition, 
        context: WorkflowContext
    ) -> Any: # Returns the functional result of the step
        step_id = step_dataclass.id
        function_identifier = step_dataclass.function_identifier
        step_run_db_rec_id: Optional[uuid.UUID] = None 

        logger.info(f"Initiating step '{step_id}' for run '{run_db_id}' using function '{function_identifier}'")
        context.add_log(f"Step '{step_id}': Starting with function '{function_identifier}'.")

        async with get_db_session() as session:
            temp_step_run_model = await self._create_or_update_step_run(
                session, run_db_id, step_dataclass, StatusEnum.PENDING,
                parameters_used=step_dataclass.parameters, 
                message=f"Step '{step_id}': Initialized to PENDING."
            )
            await session.commit()
            step_run_db_rec_id = temp_step_run_model.id
            logger.debug(f"Step '{step_id}': DB record ID {step_run_db_rec_id} initialized for attempt tracking.")

        retry_count = 0
        max_retries = step_dataclass.retry_count or 0
        step_timeout_seconds = step_dataclass.timeout_seconds if step_dataclass.timeout_seconds is not None and step_dataclass.timeout_seconds > 0 else None
        
        current_process = psutil.Process(os.getpid())

        while retry_count <= max_retries:
            attempt_start_time = time.time()
            attempt_error_message: Optional[str] = None
            attempt_error_traceback: Optional[str] = None
            attempt_status: StatusEnum = StatusEnum.RUNNING 
            attempt_resource_metrics: Dict[str, Any] = {}
            step_function_result: Any = None

            log_prefix = f"Step '{step_id}' (Run: {run_db_id}, Attempt {retry_count + 1}/{max_retries + 1})"
            logger.info(f"{log_prefix}: Starting.")
            context.add_log(f"{log_prefix}: Starting.")

            initial_cpu_times = current_process.cpu_times()
            initial_memory_info = current_process.memory_info()
            initial_io_counters = current_process.io_counters()
            
            async with get_db_session() as session:
                await self._create_or_update_step_run(
                    session, run_db_id, step_dataclass, StatusEnum.RUNNING,
                    step_run_db_id=step_run_db_rec_id, 
                    started_at=datetime.now(timezone.utc),
                    retry_attempt=retry_count,
                    message=f"{log_prefix}: Marked as RUNNING."
                )
                await session.commit()

            try:
                if not step_dataclass._resolved_function:
                    try:
                        step_dataclass._resolved_function = self._resolve_function(function_identifier)
                        logger.debug(f"{log_prefix}: Resolved function '{function_identifier}'.")
                    except Exception as e_resolve:
                        logger.error(f"{log_prefix}: Failed to resolve function '{function_identifier}': {e_resolve}", exc_info=True)
                        attempt_error_message = f"Function resolution failed: {str(e_resolve)}"
                        attempt_error_traceback = traceback.format_exc()
                        attempt_status = StatusEnum.FAILED
                        break 

                actual_parameters = {**(context.metadata.get("workflow_default_params", {})), **(step_dataclass.parameters or {})}
                executable_task = self._call_function(step_dataclass._resolved_function, actual_parameters, context)

                if step_timeout_seconds:
                    logger.info(f"{log_prefix}: Executing with timeout: {step_timeout_seconds}s.")
                    step_function_result = await asyncio.wait_for(executable_task, timeout=step_timeout_seconds)
                else:
                    logger.info(f"{log_prefix}: Executing without specific timeout.")
                    step_function_result = await executable_task
                
                attempt_status = StatusEnum.COMPLETED

            except asyncio.TimeoutError:
                attempt_error_message = f"Execution timed out after {step_timeout_seconds}s"
                logger.warning(f"{log_prefix}: {attempt_error_message}")
                attempt_status = StatusEnum.TIMEOUT
            except Exception as e_exec:
                attempt_error_message = f"Execution failed: {str(e_exec)}"
                attempt_error_traceback = traceback.format_exc()
                logger.error(f"{log_prefix}: {attempt_error_message}", exc_info=True)
                attempt_status = StatusEnum.FAILED
            
            attempt_duration = time.time() - attempt_start_time

            try:
                final_cpu_times = current_process.cpu_times()
                final_memory_info = current_process.memory_info()
                final_io_counters = current_process.io_counters()

                cpu_user_delta = final_cpu_times.user - initial_cpu_times.user
                cpu_system_delta = final_cpu_times.system - initial_cpu_times.system
                total_cpu_time_for_attempt = cpu_user_delta + cpu_system_delta
                
                avg_cpu_percent = (total_cpu_time_for_attempt / attempt_duration) * 100 if attempt_duration > 0.01 else 0.0
                
                attempt_resource_metrics = {
                    'duration_seconds': round(attempt_duration, 3),
                    'cpu_time_user_seconds': round(cpu_user_delta, 3),
                    'cpu_time_system_seconds': round(cpu_system_delta, 3),
                    'average_cpu_percent': round(avg_cpu_percent, 2),
                    'initial_memory_rss_mb': round(initial_memory_info.rss / (1024*1024), 2),
                    'final_memory_rss_mb': round(final_memory_info.rss / (1024*1024), 2),
                    'peak_memory_mb': round(final_memory_info.rss / (1024*1024), 2), 
                    'io_read_count': final_io_counters.read_count - initial_io_counters.read_count,
                    'io_write_count': final_io_counters.write_count - initial_io_counters.write_count,
                    'io_read_bytes': final_io_counters.read_bytes - initial_io_counters.read_bytes,
                    'io_write_bytes': final_io_counters.write_bytes - initial_io_counters.write_bytes,
                }
            except psutil.Error as e_psutil:
                logger.warning(f"{log_prefix}: Failed to collect full resource metrics: {e_psutil}")
                attempt_resource_metrics['error'] = f"psutil error: {str(e_psutil)}"
            
            async with get_db_session() as session:
                await self._create_or_update_step_run(
                    session, run_db_id, step_dataclass, attempt_status,
                    step_run_db_id=step_run_db_rec_id, 
                    result=step_function_result if attempt_status == StatusEnum.COMPLETED else None,
                    error_message=attempt_error_message, error_traceback=attempt_error_traceback,
                    completed_at=datetime.now(timezone.utc), 
                    duration_seconds=attempt_duration, 
                    retry_attempt=retry_count,
                    resource_metrics=attempt_resource_metrics
                )
                await session.commit()

            if attempt_status == StatusEnum.COMPLETED:
                logger.info(f"{log_prefix}: Succeeded. Result: {str(step_function_result)[:100]}... Metrics: {attempt_resource_metrics}")
                context.add_log(f"{log_prefix}: Succeeded.")
                context.step_statuses[step_id] = StepExecutionStatus.COMPLETED
                context.step_times[step_id] = {
                    "start_time": attempt_start_time, "end_time": time.time(), 
                    "duration": attempt_duration, "attempt": retry_count + 1,
                    "metrics": attempt_resource_metrics
                }
                return step_function_result

            logger.warning(f"{log_prefix}: Attempt failed with status {attempt_status.name}. Error: {attempt_error_message}. Metrics: {attempt_resource_metrics}")
            context.add_log(f"{log_prefix}: Attempt failed with status {attempt_status.name}. Error: {attempt_error_message}")

            retry_count += 1
            if retry_count <= max_retries:
                retry_delay_config = step_dataclass.retry_delay_seconds or 60
                actual_retry_delay = (retry_delay_config * (2 ** (retry_count - 1))) + (random.uniform(-0.1, 0.1) * retry_delay_config)
                actual_retry_delay = max(1.0, min(actual_retry_delay, 3600.0)) 
                
                logger.info(f"{log_prefix}: Scheduling retry {retry_count}/{max_retries} after {actual_retry_delay:.2f}s.")
                context.add_log(f"{log_prefix}: Will retry ({retry_count}/{max_retries}) after {actual_retry_delay:.2f}s.")
                
                async with get_db_session() as session: 
                     await self._create_or_update_step_run(
                        session, run_db_id, step_dataclass, StatusEnum.PENDING,
                        step_run_db_id=step_run_db_rec_id,
                        message=f"{log_prefix}: Queued for retry {retry_count}.",
                        error_message=None, error_traceback=None, 
                        started_at=None, completed_at=None, duration_seconds=None,
                        resource_metrics=None 
                    )
                     await session.commit()
                await asyncio.sleep(actual_retry_delay)
            else: 
                final_failure_message = f"{log_prefix}: Failed definitively after {max_retries} retries. Last status: {attempt_status.name}. Last error: {attempt_error_message}"
                logger.error(final_failure_message)
                context.add_log(final_failure_message)
                context.step_statuses[step_id] = StepExecutionStatus.from_db_status(attempt_status) 
                context.step_times[step_id] = {
                    "start_time_initial": "N/A (multiple attempts)", "end_time_final": time.time(), 
                    "total_attempts_duration": "N/A (see individual attempt metrics)", "error": attempt_error_message,
                    "attempts_made": retry_count, "final_attempt_metrics": attempt_resource_metrics
                }
                if attempt_status == StatusEnum.TIMEOUT:
                    raise asyncio.TimeoutError(attempt_error_message) 
                else: 
                    raise RuntimeError(attempt_error_message)
        
        final_unhandled_message = f"{log_prefix}: Exited execution loop unexpectedly. Last status {attempt_status.name if 'attempt_status' in locals() else 'UNKNOWN'}."
        logger.error(final_unhandled_message)
        raise RuntimeError(final_unhandled_message)


    async def _execute_sequential(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        logger.info(f"Executing workflow run {run_db_id} in sequential mode")

        for step_id_in_workflow in nx.topological_sort(graph):
            step_dataclass = graph.nodes[step_id_in_workflow]["step_dataclass"]
            log_prefix_seq = f"SequentialRunner (Run: {run_db_id}, Step: {step_id_in_workflow})"

            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"{log_prefix_seq}: Workflow cancelled. Skipping step.")
                async with get_db_session() as session:
                    await self._create_or_update_step_run(
                        session, run_db_id, step_dataclass, StatusEnum.CANCELLED,
                        message="Workflow cancelled by higher context.",
                        completed_at=datetime.now(timezone.utc)
                    )
                    await session.commit()
                context.step_statuses[step_id_in_workflow] = StepExecutionStatus.CANCELLED
                continue

            dep_failed_or_skipped = False
            for dep_id in graph.predecessors(step_id_in_workflow):
                dep_status = context.step_statuses.get(dep_id)
                if dep_status in [StepExecutionStatus.FAILED, StepExecutionStatus.TIMEOUT, StepExecutionStatus.CANCELLED]:
                    logger.info(f"{log_prefix_seq}: Skipping due to unsuccessful dependency '{dep_id}' (status: {dep_status}).")
                    context.step_statuses[step_id_in_workflow] = StepExecutionStatus.SKIPPED
                    async with get_db_session() as session:
                        await self._create_or_update_step_run(
                            session, run_db_id, step_dataclass, StatusEnum.SKIPPED,
                            message=f"Skipped due to unsuccessful dependency: {dep_id} (status: {dep_status.name if dep_status else 'Unknown'}).",
                            completed_at=datetime.now(timezone.utc)
                        )
                        await session.commit()
                    dep_failed_or_skipped = True
                    break
            if dep_failed_or_skipped:
                continue

            try:
                logger.info(f"{log_prefix_seq}: Calling _execute_step_core.")
                step_result = await self._execute_step_core(run_db_id, step_dataclass, context)
                context.results[step_id_in_workflow] = step_result
                logger.info(f"{log_prefix_seq}: Successfully completed. Result: {str(step_result)[:100]}...")

            except Exception as e: 
                logger.error(f"{log_prefix_seq}: Definitive failure after retries: {e}", exc_info=False) 
                if not context.parameters.get("continue_on_failure", False):
                    context.status = StepExecutionStatus.FAILED.name 
                    context.error = f"Step '{step_id_in_workflow}' failed: {str(e)}"
                    logger.error(f"Workflow run {run_db_id} failing due to step '{step_id_in_workflow}'.")
                    return context 

        if context.status == StepExecutionStatus.RUNNING.name: 
            all_steps_processed_ok = True
            for node_id_check in graph.nodes:
                final_status = context.step_statuses.get(node_id_check)
                if final_status not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                    all_steps_processed_ok = False
                    context.status = StepExecutionStatus.FAILED.name 
                    context.error = context.error or f"Workflow finished with non-successful steps (e.g., step '{node_id_check}' status: {final_status})."
                    logger.warning(f"Workflow run {run_db_id} determined as FAILED overall due to step '{node_id_check}' (status: {final_status}).")
                    break
            if all_steps_processed_ok:
                 context.status = StepExecutionStatus.COMPLETED.name
                 logger.info(f"Workflow run {run_db_id} completed successfully (sequential execution).")
        return context

    async def _execute_parallel(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        logger.info(f"Executing workflow run {run_db_id} in parallel mode")

        if self.thread_executor is None:
            self.thread_executor = ThreadPoolExecutor(max_workers=self.max_workers, thread_name_prefix="WfThread")
        if self.process_executor is None:
            self.process_executor = ProcessPoolExecutor(max_workers=self.max_workers)

        completed_for_graph_logic: Set[str] = set()
        submitted_futures: Dict[str, asyncio.Future] = {} 

        while len(completed_for_graph_logic) < len(graph.nodes):
            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"Run {run_db_id}: Workflow cancelled. Halting parallel step submission.")
                for f_node_id, f in list(submitted_futures.items()): 
                    if not f.done(): f.cancel()
                break 
            
            if context.status == StepExecutionStatus.FAILED.name and not context.parameters.get("continue_on_failure", False):
                logger.info(f"Run {run_db_id}: Workflow failed and not continuing on failure. Halting.")
                for f_node_id, f in list(submitted_futures.items()):
                    if not f.done(): f.cancel()
                break

            ready_to_submit_nodes = []
            for node_id in graph.nodes:
                if node_id in completed_for_graph_logic or node_id in submitted_futures:
                    continue 

                dependencies = set(graph.predecessors(node_id))
                if dependencies.issubset(completed_for_graph_logic): 
                    dep_unsuccessful = False
                    for dep_id in dependencies:
                        dep_final_status = context.step_statuses.get(dep_id)
                        if dep_final_status not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                            log_prefix_par = f"ParallelRunner (Run: {run_db_id}, Step: {node_id})"
                            logger.info(f"{log_prefix_par}: Skipping due to unsuccessful dependency '{dep_id}' (status: {dep_final_status}).")
                            context.step_statuses[node_id] = StepExecutionStatus.SKIPPED
                            async with get_db_session() as session:
                                step_dc_skip = graph.nodes[node_id]["step_dataclass"]
                                await self._create_or_update_step_run(
                                    session, run_db_id, step_dc_skip, StatusEnum.SKIPPED,
                                    message=f"Skipped due to unsuccessful dependency: {dep_id} (status: {dep_final_status.name if dep_final_status else 'Unknown'}).",
                                    completed_at=datetime.now(timezone.utc)
                                )
                                await session.commit()
                            completed_for_graph_logic.add(node_id) 
                            dep_unsuccessful = True
                            break
                    if not dep_unsuccessful:
                        ready_to_submit_nodes.append(node_id)
            
            for node_id in ready_to_submit_nodes:
                if context.status == StepExecutionStatus.CANCELLED.name or \
                   (context.status == StepExecutionStatus.FAILED.name and not context.parameters.get("continue_on_failure", False)):
                    break

                step_dataclass = graph.nodes[node_id]["step_dataclass"]
                loop = asyncio.get_running_loop()
                future = loop.create_future()
                submitted_futures[node_id] = future

                context_repr = context if not step_dataclass.use_process else context.to_dict()
                executor = self.process_executor if step_dataclass.use_process else self.thread_executor

                if executor is None: 
                    logger.critical(f"Run {run_db_id}: Executor for step {node_id} (process: {step_dataclass.use_process}) is None. Bug!")
                    exc = RuntimeError(f"Executor not available for step {node_id}")
                    if not future.done(): future.set_exception(exc)
                    continue

                executor.submit(self._run_step_task_for_executor, run_db_id, step_dataclass, context_repr, future, loop)
                
                context.step_statuses[node_id] = StepExecutionStatus.QUEUED 
                context.add_log(f"Step '{node_id}' (Run: {run_db_id}): Submitted to executor.")

            if not submitted_futures:
                if len(completed_for_graph_logic) < len(graph.nodes): await asyncio.sleep(0.1)
                continue

            done_futures_set, _ = await asyncio.wait(submitted_futures.values(), return_when=asyncio.FIRST_COMPLETED, timeout=0.5)

            for fut in done_futures_set:
                processed_node_id = None
                for nid, f_val in list(submitted_futures.items()): 
                    if f_val == fut:
                        processed_node_id = nid
                        break
                
                if not processed_node_id: continue 

                try:
                    step_actual_result = fut.result() 
                    if context.step_statuses.get(processed_node_id) == StepExecutionStatus.COMPLETED:
                         context.results[processed_node_id] = step_actual_result
                         logger.info(f"Step '{processed_node_id}' (Run: {run_db_id}): Future resolved successfully.")
                    else: 
                        logger.warning(f"Step '{processed_node_id}' future completed, but context status is {context.step_statuses.get(processed_node_id)} (expected COMPLETED). Result: {str(step_actual_result)[:100]}")

                except asyncio.CancelledError:
                    logger.info(f"Step '{processed_node_id}' (Run: {run_db_id}): Future was cancelled.")
                    if context.step_statuses.get(processed_node_id) not in [StepExecutionStatus.CANCELLED, StepExecutionStatus.COMPLETED, StepExecutionStatus.FAILED, StepExecutionStatus.TIMEOUT]:
                         context.step_statuses[processed_node_id] = StepExecutionStatus.CANCELLED
                    async with get_db_session() as session:
                        await self._create_or_update_step_run(
                            session, run_db_id, graph.nodes[processed_node_id]["step_dataclass"], StatusEnum.CANCELLED,
                            message=f"Step '{processed_node_id}' cancelled via future.",
                            completed_at=datetime.now(timezone.utc)
                        )
                        await session.commit()

                except Exception as e_fut: 
                    logger.error(f"Step '{processed_node_id}' (Run: {run_db_id}): Future resolved with failure: {e_fut}", exc_info=False)
                    step_fail_status_in_ctx = context.step_statuses.get(processed_node_id)
                    if step_fail_status_in_ctx not in [StepExecutionStatus.FAILED, StepExecutionStatus.TIMEOUT]:
                        logger.warning(f"Step '{processed_node_id}' future failed, but context status is {step_fail_status_in_ctx}. Forcing FAILED.")
                        context.step_statuses[processed_node_id] = StepExecutionStatus.FAILED
                    
                    if not context.parameters.get("continue_on_failure", False):
                        logger.info(f"Run {run_db_id}: Failing due to step '{processed_node_id}' and not continuing on failure.")
                        context.status = StepExecutionStatus.FAILED.name 
                        context.error = context.error or f"Step '{processed_node_id}' failed: {str(e_fut)}"
                        for f_node_id_cancel, f_cancel in list(submitted_futures.items()):
                            if not f_cancel.done() and f_node_id_cancel != processed_node_id : 
                                f_cancel.cancel()
                
                completed_for_graph_logic.add(processed_node_id)
                del submitted_futures[processed_node_id] 

        if context.status == StepExecutionStatus.RUNNING.name:
            if len(completed_for_graph_logic) == len(graph.nodes):
                all_ok = True
                for sid_check in graph.nodes:
                    s_stat = context.step_statuses.get(sid_check)
                    if s_stat not in [StepExecutionStatus.COMPLETED, StepExecutionStatus.SKIPPED]:
                        all_ok = False; break
                if all_ok: context.status = StepExecutionStatus.COMPLETED.name
                else:
                    context.status = StepExecutionStatus.FAILED.name
                    context.error = context.error or "Workflow finished with non-successful steps (parallel)."
            else: 
                if context.status == StepExecutionStatus.RUNNING.name: 
                    context.status = StepExecutionStatus.FAILED.name
                    context.error = context.error or "Workflow did not complete all steps (parallel)."
        
        logger.info(f"Parallel execution for run {run_db_id} finished with overall status: {context.status}")
        return context

    async def cancel_workflow(self, run_db_id: uuid.UUID) -> bool:
        """
        Cancel a running workflow

        Args:
            run_db_id: ID of the workflow run

        Returns:
            True if workflow was cancelled, False otherwise
        """
        if run_db_id not in self.active_workflows:
            return False

        logger.info(f"Cancelling workflow run {run_db_id}")

        # Mark workflow as cancelled
        context = self.active_workflows[run_db_id]["context"]
        context.status = "CANCELLED"
        context.end_time = datetime.now()

        # Mark all running steps as cancelled
        for step_id in self.active_steps.get(run_db_id, set()):
            if step_id in self.active_workflows[run_db_id]["steps"]:
                step = self.active_workflows[run_db_id]["steps"][step_id]
                step.status = StepExecutionStatus.CANCELLED
                step.completed_at = datetime.now()
                context.step_statuses[step_id] = StepExecutionStatus.CANCELLED

        # Kill processes if necessary
        for key, pid in list(self.step_pid_map.items()):
            if key.startswith(f"{run_db_id}_"):
                try:
                    os.kill(pid, signal.SIGTERM)
                except ProcessLookupError:
                    pass

        # Update database
        await self._update_run_record(run_db_id, context)

        # Remove from active workflows
        if run_db_id in self.active_workflows:
            del self.active_workflows[run_db_id]
        if run_db_id in self.active_steps:
            del self.active_steps[run_db_id]

        return True

    async def get_workflow_status(self, run_db_id: uuid.UUID) -> Dict[str, Any]:
        """
        Get status of a workflow run from the database.

        Args:
            run_db_id: Database ID of the workflow run.

        Returns:
            Dictionary with workflow status information.
        """
        async with get_db_session() as session:  # type: AsyncSession
            run_model = await session.get(WorkflowRunModel, run_db_id)
            if not run_model:
                raise ValueError(f"Workflow run with DB ID '{run_db_id}' not found.")

            workflow_model = await session.get(WorkflowModel, run_model.workflow_id)
            if not workflow_model:
                logger.error(f"Workflow definition {run_model.workflow_id} for run {run_db_id} not found!")
                total_defined_steps = 0
            else:
                total_defined_steps = workflow_model.step_count or 0
                if total_defined_steps == 0:
                    count_stmt = sa.select(sa.func.count(WorkflowStepDefinitionModel.id)).where(
                        WorkflowStepDefinitionModel.workflow_id == workflow_model.id
                    )
                    total_defined_steps = (await session.execute(count_stmt)).scalar_one() or 0

            step_runs_stmt = sa.select(WorkflowStepRunModel).where(WorkflowStepRunModel.workflow_run_id == run_db_id)
            step_runs_result = await session.execute(step_runs_stmt)
            step_run_models = step_runs_result.scalars().all()

            step_statuses_details: Dict[str, Dict[str, Any]] = {}
            completed_or_skipped_count = 0
            active_step_info_list = []

            latest_step_runs_map: Dict[str, WorkflowStepRunModel] = {}
            for sr in step_run_models:
                if (
                    sr.step_id_in_workflow not in latest_step_runs_map
                    or latest_step_runs_map[sr.step_id_in_workflow].created_at < sr.created_at
                ):
                    latest_step_runs_map[sr.step_id_in_workflow] = sr

            for step_id_in_workflow, sr_model in latest_step_runs_map.items():
                status_enum = sr_model.status
                step_statuses_details[step_id_in_workflow] = {
                    "status": status_enum.name,
                    "started_at": sr_model.started_at.isoformat() if sr_model.started_at else None,
                    "completed_at": sr_model.completed_at.isoformat() if sr_model.completed_at else None,
                    "duration_seconds": sr_model.duration_seconds,
                    "retry_attempt": sr_model.retry_attempt,
                    "error": sr_model.error_message,
                }
                if (
                    status_enum.is_terminal and status_enum != StatusEnum.FAILED and status_enum != StatusEnum.TIMEOUT
                ):  # COMPLETED, SKIPPED, CANCELLED
                    completed_or_skipped_count += 1

                if status_enum == StatusEnum.RUNNING:
                    step_def_name = step_id_in_workflow
                    if workflow_model:
                        step_def_model = await session.get(WorkflowStepDefinitionModel, sr_model.step_definition_id)
                        if step_def_model:
                            step_def_name = step_def_model.name

                    active_step_info_list.append(
                        {
                            "id": step_id_in_workflow,
                            "name": step_def_name,
                            "status": status_enum.name,
                            "progress": 0.5,  # Placeholder, true step progress is complex
                            "duration": (
                                (datetime.now(timezone.utc) - sr_model.started_at).total_seconds()
                                if sr_model.started_at
                                else 0
                            ),
                        }
                    )

            progress = (completed_or_skipped_count / total_defined_steps) * 100 if total_defined_steps > 0 else 0
            if run_model.status == StatusEnum.COMPLETED:
                progress = 100.0
            elif run_model.status.is_terminal and not run_model.status.is_successful:
                # For failed/cancelled runs, progress reflects completed/skipped steps before termination
                pass

            return {
                "run_id": str(run_model.id),
                "workflow_id": str(run_model.workflow_id),
                "workflow_name": workflow_model.name if workflow_model else "Unknown",
                "status": run_model.status.name,
                "progress": progress,
                "total_steps": total_defined_steps,
                "completed_steps": completed_or_skipped_count,
                "current_steps": active_step_info_list,
                "step_statuses": step_statuses_details,
                "started_at": run_model.started_at.isoformat() if run_model.started_at else None,
                "completed_at": run_model.completed_at.isoformat() if run_model.completed_at else None,
                "duration_seconds": run_model.duration_seconds,
                "error": run_model.error_message,
            }

    async def get_workflow_logs(
        self, run_db_id: uuid.UUID, step_id_in_workflow: Optional[str] = None
    ) -> Union[List[str], str, None]:
        """
        Get logs for a workflow run or a specific step within that run from the database.

        Args:
            run_db_id: Database ID of the workflow run.
            step_id_in_workflow: Optional user-defined ID of a specific step.

        Returns:
            A list of log strings for a specific step (collated from all attempts if applicable, or latest attempt's logs),
            the logs_summary string for the overall workflow run if no step_id is provided,
            or None if no logs are found.
        """
        async with get_db_session() as session:  # type: AsyncSession
            if step_id_in_workflow:
                # Get logs for a specific step.
                stmt = (
                    sa.select(WorkflowStepRunModel.logs, WorkflowStepRunModel.created_at)
                    .where(WorkflowStepRunModel.workflow_run_id == run_db_id)
                    .where(WorkflowStepRunModel.step_id_in_workflow == step_id_in_workflow)
                    .order_by(WorkflowStepRunModel.created_at)
                )

                result = await session.execute(stmt)
                log_parts = result.all()

                if not log_parts:
                    return None

                collated_logs = []
                for part in log_parts:
                    if part.logs:
                        attempt_log_header = f"--- Logs from attempt starting around {part.created_at.isoformat()} ---"
                        collated_logs.append(attempt_log_header)
                        collated_logs.extend(part.logs.split("\n"))
                return collated_logs if collated_logs else None
            else:
                # Get overall workflow run logs summary
                run_model = await session.get(WorkflowRunModel, run_db_id)
                if not run_model:
                    raise ValueError(f"Workflow run with DB ID '{run_db_id}' not found.")
                return run_model.logs_summary

    async def list_workflows(
        self, name_filter: Optional[str] = None, enabled_only: bool = True, limit: int = 100, offset: int = 0
    ) -> List[Dict[str, Any]]:
        """
        List workflow definitions from the database.

        Args:
            name_filter: Optional filter by workflow name (substring match).
            enabled_only: If True, only list enabled workflows.
            limit: Maximum number of workflows to return.
            offset: Offset for pagination.

        Returns:
            A list of dictionaries, where each dictionary represents a workflow definition.
        """
        async with get_db_session() as session:  # type: AsyncSession
            stmt = sa.select(WorkflowModel)
            if name_filter:
                stmt = stmt.where(WorkflowModel.name.ilike(f"%{name_filter}%"))  # Case-insensitive substring match
            if enabled_only:
                # E712: Avoid equality comparisons to `True`
                stmt = stmt.where(WorkflowModel.enabled)

            stmt = stmt.order_by(WorkflowModel.name, WorkflowModel.version.desc()).limit(limit).offset(offset)

            result = await session.execute(stmt)
            workflow_db_models = result.scalars().all()

            workflows_list = []
            for wf_model in workflow_db_models:
                workflows_list.append(
                    {
                        "id": str(wf_model.id),
                        "name": wf_model.name,
                        "description": wf_model.description,
                        "version": wf_model.version,
                        "step_count": wf_model.step_count,
                        "enabled": wf_model.enabled,
                        "default_parameters": wf_model.default_parameters,
                        "tags": wf_model.tags,
                        "owner_id": wf_model.owner_id,
                        "created_at": wf_model.created_at.isoformat(),
                        "updated_at": wf_model.updated_at.isoformat(),
                        "custom_metadata": wf_model.custom_metadata,
                    }
                )
            return workflows_list

    async def get_all_step_runs_for_workflow_run(
        self, session: AsyncSession, run_db_id: uuid.UUID
    ) -> List[WorkflowStepRunModel]:
        """Helper to fetch all step runs for a given workflow run ID."""
        stmt = sa.select(WorkflowStepRunModel).where(WorkflowStepRunModel.workflow_run_id == run_db_id)
        result = await session.execute(stmt)
        return list(result.scalars().all())

    async def get_workflow_metrics(self, run_db_id: uuid.UUID) -> Dict[str, Any]:
        """
        Get detailed metrics for a workflow run from the database.

        Args:
            run_db_id: ID of the workflow run.

        Returns:
            Dictionary with workflow metrics.
        """
        async with get_db_session() as session:  # type: AsyncSession
            run_model = await session.get(WorkflowRunModel, run_db_id)
            if not run_model:
                raise ValueError(f"Workflow run DB ID '{run_db_id}' not found.")

            workflow_model = await session.get(WorkflowModel, run_model.workflow_id)
            if not workflow_model:
                logger.warning(
                    f"Workflow definition {run_model.workflow_id} for run {run_db_id} not found. Metrics might be incomplete."
                )

            step_run_models = await self.get_all_step_runs_for_workflow_run(session, run_db_id)

            latest_step_runs_map: Dict[str, WorkflowStepRunModel] = {}
            for sr in step_run_models:
                current_step_id_in_workflow = sr.step_id_in_workflow
                new_timestamp = sr.created_at

                if new_timestamp is None:  # Should ideally not happen with TimestampMixin
                    logger.warning(
                        f"StepRun {sr.id} for step '{current_step_id_in_workflow}' has None created_at timestamp. Skipping from latest_step_runs_map logic."
                    )
                    continue

                if current_step_id_in_workflow not in latest_step_runs_map:
                    latest_step_runs_map[current_step_id_in_workflow] = sr
                else:
                    existing_sr = latest_step_runs_map[current_step_id_in_workflow]
                    # Defensive check for existing_sr.created_at, though it should also exist
                    if existing_sr.created_at is None or new_timestamp > existing_sr.created_at:
                        latest_step_runs_map[current_step_id_in_workflow] = sr

            step_metrics_data: Dict[str, Dict[str, Any]] = {}
            for step_id_in_workflow, sr_model in latest_step_runs_map.items():
                step_def_name = step_id_in_workflow  # Default to step_id_in_workflow

                if workflow_model:
                    step_def_model_for_name = None
                    if sr_model.step_definition_id:
                        step_def_model_for_name = await session.get(
                            WorkflowStepDefinitionModel, sr_model.step_definition_id
                        )

                    if not step_def_model_for_name:
                        # Fallback if step_definition_id is not on run or not found
                        stmt_fallback = sa.select(WorkflowStepDefinitionModel).where(
                            WorkflowStepDefinitionModel.workflow_id == workflow_model.id,
                            WorkflowStepDefinitionModel.step_id_in_workflow == step_id_in_workflow,
                        )
                        result_fallback = await session.execute(stmt_fallback)
                        step_def_model_for_name = result_fallback.scalar_one_or_none()

                    if step_def_model_for_name:
                        step_def_name = step_def_model_for_name.name
                    else:
                        logger.warning(
                            f"Could not find StepDefinitionModel for step_id_in_workflow '{step_id_in_workflow}' "
                            f"in workflow '{workflow_model.name}' (ID: {workflow_model.id}) for run '{run_db_id}'. "
                            f"Using step_id_in_workflow ('{step_id_in_workflow}') as name."
                        )

                step_metrics_data[step_id_in_workflow] = {
                    "name": step_def_name,
                    "status": sr_model.status.name if sr_model.status else "UNKNOWN",
                    "duration_seconds": sr_model.duration_seconds,
                    "started_at": sr_model.started_at.isoformat() if sr_model.started_at else None,
                    "completed_at": sr_model.completed_at.isoformat() if sr_model.completed_at else None,
                    "retry_attempts": sr_model.retry_attempt if sr_model.retry_attempt is not None else 0,
                    "error": sr_model.error_message,
                    "host_machine": sr_model.host_machine,
                    "step_run_db_id": str(sr_model.id),
                }

            critical_path_steps = []
            critical_path_duration = 0.0
            if workflow_model:
                # Pass session to get_step_definition_models
                step_definitions_db = await self.get_step_definition_models(session, workflow_model.id)
                if step_definitions_db:
                    step_dataclasses_for_graph = [
                        WorkflowStepDefinition(
                            id=s_db.step_id_in_workflow,  # Critical: use step_id_in_workflow for graph node ID
                            name=s_db.name,
                            description=s_db.description or "",
                            function_identifier=s_db.function_identifier or "",
                            parameters=s_db.default_parameters or {},
                            dependencies=s_db.dependencies or [],
                            timeout_seconds=s_db.timeout_seconds,
                            retry_count=s_db.retry_count or 0,
                            priority=s_db.priority or 0,
                            estimated_memory_mb=s_db.estimated_memory_mb or 1000,
                            use_gpu=s_db.use_gpu or False,
                            use_process=getattr(s_db, "use_process", False),  # Default if not on model
                        )
                        for s_db in step_definitions_db
                    ]
                    try:
                        graph = self._build_workflow_graph(step_dataclasses_for_graph)

                        metrics_for_crit_path_calc = {}
                        for sid, data in step_metrics_data.items():
                            duration = data.get("duration_seconds")
                            if duration is not None:
                                try:
                                    metrics_for_crit_path_calc[sid] = {"execution_time": float(duration)}
                                except (ValueError, TypeError):
                                    logger.warning(
                                        f"Could not convert duration '{duration}' to float for step '{sid}' in critical path calculation for run {run_db_id}."
                                    )

                        if graph.nodes and metrics_for_crit_path_calc:
                            critical_path_steps = self._compute_critical_path(graph, metrics_for_crit_path_calc)
                            for step_id_cp in critical_path_steps:
                                if (
                                    step_id_cp in step_metrics_data
                                    and step_metrics_data[step_id_cp].get("duration_seconds") is not None
                                ):
                                    critical_path_duration += float(step_metrics_data[step_id_cp]["duration_seconds"])
                        elif not graph.nodes:
                            logger.warning(f"Graph has no nodes for run {run_db_id}, cannot compute critical path.")
                        elif not metrics_for_crit_path_calc:
                            logger.info(
                                f"No step durations available for run {run_db_id}, critical path duration will be 0."
                            )

                    except ValueError as e:
                        logger.error(f"Could not build graph for critical path calculation for run {run_db_id}: {e}")
                    except Exception as e:  # Catch other potential errors during critical path
                        logger.error(
                            f"Unexpected error during critical path calculation for run {run_db_id}: {e}",
                            exc_info=True,
                        )

            total_steps_defined = 0
            if workflow_model:
                total_steps_defined = workflow_model.step_count if workflow_model.step_count is not None else 0
                if total_steps_defined == 0:  # If count was 0 or None, try to count from definitions
                    count_stmt = sa.select(sa.func.count(WorkflowStepDefinitionModel.id)).where(
                        WorkflowStepDefinitionModel.workflow_id == workflow_model.id
                    )
                    count_result = await session.execute(count_stmt)
                    total_steps_defined = count_result.scalar_one_or_none() or 0

            completed_steps_count = sum(
                1 for sr_model in latest_step_runs_map.values() if sr_model.status == StatusEnum.COMPLETED
            )
            failed_steps_count = sum(
                1
                for sr_model in latest_step_runs_map.values()
                if sr_model.status and sr_model.status.has_failed_or_timed_out
            )
            running_steps_count = sum(
                1 for sr_model in latest_step_runs_map.values() if sr_model.status == StatusEnum.RUNNING
            )
            pending_steps_count = sum(
                1
                for sr_model in latest_step_runs_map.values()
                if sr_model.status == StatusEnum.PENDING or sr_model.status == StatusEnum.QUEUED
            )

            resource_usage_metrics = (
                run_model.custom_metadata.get("resource_usage", {}) if run_model.custom_metadata else {}
            )

            progress_percentage = 0
            if total_steps_defined > 0:
                # Consider a step "progressed" if it's completed or skipped.
                # For a more nuanced progress, one might weigh steps or consider running steps partially.
                progressed_count = sum(
                    1
                    for sr_model in latest_step_runs_map.values()
                    if sr_model.status in (StatusEnum.COMPLETED, StatusEnum.SKIPPED)
                )
                progress_percentage = (progressed_count / total_steps_defined) * 100

            # Placeholder for estimated cost
            total_estimated_cost = (
                run_model.custom_metadata.get("estimated_cost", 0.0) if run_model.custom_metadata else 0.0
            )
            # Actual calculation would need more data (e.g. cost per hour per step type * duration)

            return {
                "run_id": str(run_model.id),
                "workflow_id": str(run_model.workflow_id),
                "workflow_name": workflow_model.name if workflow_model else "Unknown",
                "workflow_version": workflow_model.version if workflow_model else "Unknown",
                "status": run_model.status.name if run_model.status else "UNKNOWN",
                "started_at": run_model.started_at.isoformat() if run_model.started_at else None,
                "completed_at": run_model.completed_at.isoformat() if run_model.completed_at else None,
                "duration_seconds": run_model.duration_seconds,
                "total_steps_defined": total_steps_defined,
                "completed_steps": completed_steps_count,
                "failed_steps": failed_steps_count,
                "running_steps": running_steps_count,
                "pending_steps": pending_steps_count,  # Includes queued
                "progress_percentage": round(progress_percentage, 2),
                "critical_path_step_ids": critical_path_steps,
                "critical_path_duration_seconds": (
                    round(critical_path_duration, 3) if critical_path_duration is not None else None
                ),
                "step_metrics": step_metrics_data,  # Detailed metrics for each step's latest run
                "resource_usage_summary": resource_usage_metrics,  # Overall from run_model custom_metadata
                "error_message": run_model.error_message,
                "triggered_by": run_model.triggered_by,
                "custom_metadata": run_model.custom_metadata,  # Other custom metadata from the run
                "estimated_cost": total_estimated_cost,  # Ensured F841 is resolved by using the variable
            }

    def _compute_critical_path(self, graph: nx.DiGraph, step_metrics: Dict[str, Dict[str, Any]]) -> List[str]:
        """
        Compute the critical path of a workflow

        Args:
            graph: Workflow graph
            step_metrics: Dictionary of step metrics

        Returns:
            List of step IDs forming the critical path
        """
        # Create a new graph with edge weights based on step durations
        G = nx.DiGraph()

        for node in graph.nodes():
            G.add_node(node)

        for u, v in graph.edges():
            # Use step execution time as weight, default to 0 if not available
            weight = step_metrics.get(v, {}).get("execution_time", 0) or 0
            G.add_edge(u, v, weight=weight)

        # Find the critical path (longest path through the DAG)
        # First, topologically sort the graph
        try:
            topo_sort = list(nx.topological_sort(G))

            # Calculate longest path to each node
            dist = {node: 0 for node in G.nodes()}
            prev = {node: None for node in G.nodes()}

            for node in topo_sort:
                for successor in G.successors(node):
                    weight = G.edges[node, successor]["weight"]
                    if dist[node] + weight > dist[successor]:
                        dist[successor] = dist[node] + weight
                        prev[successor] = node

            # Find the node with the maximum distance
            max_dist_node = max(topo_sort, key=lambda node: dist[node])

            # Reconstruct the path
            path = []
            current = max_dist_node
            while current is not None:
                path.append(current)
                current = prev[current]

            return list(reversed(path))

        except nx.NetworkXUnfeasible:
            # Graph has cycles
            return []

    async def retry_step(self, run_db_id: uuid.UUID, step_id: str) -> bool:
        """
        Retry a failed workflow step

        Args:
            run_db_id: ID of the workflow run
            step_id: ID of the step to retry

        Returns:
            True if retry was scheduled, False otherwise
        """
        if run_db_id not in self.active_workflows:
            # Look up completed workflow in database
            async with get_db_session() as session:
                run_record = await session.get(WorkflowRunModel, run_db_id)
                if not run_record:
                    raise ValueError(f"Workflow run {run_db_id} not found")

                # Cannot retry steps in completed workflows (would require reloading)
                return False

        # Get step from active workflow
        if step_id not in self.active_workflows[run_db_id]["steps"]:
            raise ValueError(f"Step {step_id} not found in workflow run {run_db_id}")

        step = self.active_workflows[run_db_id]["steps"][step_id]

        # Check if step can be retried
        if step.status not in (StepExecutionStatus.FAILED, StepExecutionStatus.TIMEOUT):
            logger.warning(f"Cannot retry step {step_id} with status {step.status.name}")
            return False

        # Reset step status
        step.status = StepExecutionStatus.PENDING
        step.error = None
        step.started_at = None
        step.completed_at = None

        # Update context
        context = self.active_workflows[run_db_id]["context"]
        context.step_statuses[step_id] = StepExecutionStatus.PENDING

        # Update database
        async with get_db_session() as session:
            step_runs = await session.execute(
                "SELECT * FROM workflow_step_run WHERE workflow_run_id = :run_id AND step_id = :step_id",
                {"run_id": run_db_id, "step_id": step_id},
            )
            step_run = step_runs.fetchone()

            if step_run:
                step_run.status = "PENDING"
                step_run.error = None
                step_run.started_at = None
                step_run.completed_at = None
                step_run.retries = step_run.retries or 0

    def _setup_directories(self, workflow_dir: str):
        """Ensure required directories exist"""
        self.workflow_dir = Path(workflow_dir)
        self.workflow_dir.mkdir(parents=True, exist_ok=True)
        (self.workflow_dir / "logs").mkdir(exist_ok=True)
        (self.workflow_dir / "artifacts").mkdir(exist_ok=True)
        
    def _validate_gpu_devices(self, gpu_devices: List[int]):
        """Validate GPU device IDs"""
        if gpu_devices:
            try:
                import torch
                available_devices = list(range(torch.cuda.device_count()))
                invalid_devices = set(gpu_devices) - set(available_devices)
                if invalid_devices:
                    raise ValueError(f"Invalid GPU devices specified: {invalid_devices}")
            except ImportError:
                logger.warning("PyTorch not installed, cannot validate GPU devices")

    def _get_current_resources(self):
        """Get current system resource usage for health reporting"""
        try:
            return {
                'cpu': psutil.cpu_percent(interval=0.1),
                'memory': psutil.virtual_memory()._asdict(),
                'disk': psutil.disk_usage('/')._asdict(),
                'active_workflows': getattr(self, 'active_workflows', {})
            }
        except Exception as e:
            logger.warning(f"Error getting current resources: {str(e)}")
            return {'error': str(e)}

    def _calculate_performance_metrics(self):
        """Calculate performance metrics for health reporting"""
        return {
            'total_workflows': self.performance_stats['total_workflows'],
            'successful_workflows': self.performance_stats['successful_workflows'],
            'failed_workflows': self.performance_stats['failed_workflows'],
            'avg_duration': self.performance_stats['avg_duration']
        }

    async def _execute_distributed(
        self,
        run_db_id: uuid.UUID,
        graph: nx.DiGraph,
        context: WorkflowContext,
        steps_map: Dict[str, WorkflowStepDefinition],
    ) -> WorkflowContext:
        """
        Execute workflow steps in distributed mode using DistributedTaskManager
        
        Args:
            run_db_id: ID of the workflow run
            graph: Directed acyclic graph of steps
            context: Workflow context
            steps_map: Map of step_id_in_workflow to WorkflowStepDefinition dataclass
            
        Returns:
            Updated workflow context
        """
        if not self.distributed_manager:
            raise ValueError("DistributedTaskManager not configured for distributed execution")
            
        logger.info(f"Executing workflow run {run_db_id} in distributed mode")
        
        # Initialize tracking structures
        completed_steps: Set[str] = set()
        submitted_steps: Set[str] = set()
        running_steps: Set[str] = set()
        task_id_to_step_id: Dict[str, str] = {}
        
        # Create a serializable context for distribution
        serialized_context = context.to_dict()
        
        while len(completed_steps) < len(graph.nodes):
            # Check for cancellation
            if context.status == StepExecutionStatus.CANCELLED.name:
                logger.info(f"Workflow run {run_db_id} cancelled. Halting distributed execution.")
                
                # Cancel any running distributed tasks
                for step_id in running_steps:
                    for task_id, s_id in task_id_to_step_id.items():
                        if s_id == step_id:
                            await self.distributed_manager.cancel_task(task_id)
                            
                # Mark remaining steps as cancelled
                for node_id in graph.nodes:
                    if node_id not in completed_steps and node_id not in submitted_steps:
                        context.step_statuses[node_id] = StepExecutionStatus.CANCELLED
                        async with get_db_session() as session:
                            step_dataclass = graph.nodes[node_id]["step_dataclass"]
                            await self._create_or_update_step_run(
                                session,
                                run_db_id,
                                step_dataclass,
                                StatusEnum.CANCELLED,
                                message="Workflow cancelled by user",
                                completed_at=datetime.now(timezone.utc),
                            )
                break
                
            # Find steps ready to submit
            ready_to_submit_nodes = []
            for node_id in graph.nodes:
                if node_id in completed_steps or node_id in submitted_steps or node_id in running_steps:
                    continue
                    
                dependencies = set(graph.predecessors(node_id))
                if dependencies.issubset(completed_steps):
                    # Check if any dependency failed
                    dep_failed_or_skipped = False
                    for dep_id in dependencies:
                        if context.step_statuses.get(dep_id) in [
                            StepExecutionStatus.FAILED, StepExecutionStatus.SKIPPED
                        ]:
                            context.step_statuses[node_id] = StepExecutionStatus.SKIPPED
                            logger.info(
                                f"Skipping step {node_id} due to failed/skipped dependency {dep_id} in run {run_db_id}"
                            )
                            async with get_db_session() as session:
                                step_dataclass = graph.nodes[node_id]["step_dataclass"]
                                await self._create_or_update_step_run(
                                    session,
                                    run_db_id,
                                    step_dataclass,
                                    StatusEnum.SKIPPED,
                                    message=f"Skipped due to failed/skipped dependency: {dep_id}",
                                    completed_at=datetime.now(timezone.utc),
                                )
                            completed_steps.add(node_id)
                            dep_failed_or_skipped = True
                            break
                    if not dep_failed_or_skipped:
                        ready_to_submit_nodes.append(node_id)
            
            # Submit ready steps to distributed manager
            for node_id in ready_to_submit_nodes:
                if context.status == StepExecutionStatus.CANCELLED.name:
                    break
                    
                step_dataclass = graph.nodes[node_id]["step_dataclass"]
                
                # Prepare task for distributed execution
                task_payload = {
                    "function_identifier": step_dataclass.function_identifier,
                    "parameters": step_dataclass.parameters,
                    "context": serialized_context,
                    "step_id": node_id,
                    "run_id": str(run_db_id),
                    "timeout_seconds": step_dataclass.timeout_seconds,
                }
                
                # Set resource requirements
                resource_requirements = {
                    "memory_mb": step_dataclass.estimated_memory_mb,
                    "use_gpu": step_dataclass.use_gpu,
                    "priority": step_dataclass.priority,
                }
                
                # Submit task to distributed manager
                try:
                    task_id = await self.distributed_manager.submit_task(
                        task_payload, 
                        resource_requirements=resource_requirements
                    )
                    
                    # Record task submission
                    task_id_to_step_id[task_id] = node_id
                    submitted_steps.add(node_id)
                    running_steps.add(node_id)
                    
                    # Update status in context and database
                    context.step_statuses[node_id] = StepExecutionStatus.QUEUED
                    context.add_log(f"Submitted step {node_id} to distributed execution with task_id {task_id}")
                    
                    async with get_db_session() as session:
                        await self._create_or_update_step_run(
                            session,
                            run_db_id,
                            step_dataclass,
                            StatusEnum.QUEUED,
                            parameters_used=step_dataclass.parameters,
                            message=f"Submitted to distributed worker as task {task_id}",
                        )
                except Exception as e:
                    logger.error(f"Failed to submit step {node_id} to distributed execution: {str(e)}")
                    context.step_statuses[node_id] = StepExecutionStatus.FAILED
                    completed_steps.add(node_id)
                    
                    async with get_db_session() as session:
                        await self._create_or_update_step_run(
                            session,
                            run_db_id,
                            step_dataclass,
                            StatusEnum.FAILED,
                            error_message=f"Failed to submit to distributed worker: {str(e)}",
                            completed_at=datetime.now(timezone.utc),
                        )
            
            # Check for completed tasks
            if running_steps:
                # Get status of all running tasks
                task_ids = [task_id for task_id, step_id in task_id_to_step_id.items() 
                           if step_id in running_steps]
                            
                task_statuses = await self.distributed_manager.get_task_statuses(task_ids)
                
                # Process completed tasks
                for task_id, status in task_statuses.items():
                    if status["status"] in ["completed", "failed", "cancelled", "timeout"]:
                        step_id = task_id_to_step_id[task_id]
                        
                        if step_id in running_steps:
                            running_steps.remove(step_id)
                            completed_steps.add(step_id)
                            
                            # Get step result
                            try:
                                task_result = await self.distributed_manager.get_task_result(task_id)
                                
                                if status["status"] == "completed":
                                    # Task completed successfully
                                    context.step_statuses[step_id] = StepExecutionStatus.COMPLETED
                                    context.results[step_id] = task_result["result"]
                                    
                                    async with get_db_session() as session:
                                        step_dataclass = graph.nodes[step_id]["step_dataclass"]
                                        await self._create_or_update_step_run(
                                            session,
                                            run_db_id,
                                            step_dataclass,
                                            StatusEnum.COMPLETED,
                                            result=task_result["result"],
                                            completed_at=datetime.now(timezone.utc),
                                            duration_seconds=status["duration_seconds"],
                                        )
                                else:
                                    # Task failed
                                    error_status = StatusEnum.FAILED
                                    if status["status"] == "timeout":
                                        error_status = StatusEnum.TIMEOUT
                                    elif status["status"] == "cancelled":
                                        error_status = StatusEnum.CANCELLED
                                        
                                    context.step_statuses[step_id] = StepExecutionStatus.from_db_status(error_status)
                                    
                                    async with get_db_session() as session:
                                        step_dataclass = graph.nodes[step_id]["step_dataclass"]
                                        await self._create_or_update_step_run(
                                            session,
                                            run_db_id,
                                            step_dataclass,
                                            error_status,
                                            error_message=task_result.get("error", "Unknown error"),
                                            error_traceback=task_result.get("traceback"),
                                            completed_at=datetime.now(timezone.utc),
                                            duration_seconds=status["duration_seconds"],
                                        )
                                        
                                    # Check if we should fail the workflow
                                    if not context.parameters.get("continue_on_failure", False):
                                        context.status = StepExecutionStatus.FAILED.name
                                        context.error = f"Step {step_id} failed: {task_result.get('error', 'Unknown error')}"
                            except Exception as e:
                                logger.error(f"Error processing task result for step {step_id}: {str(e)}")
                                context.step_statuses[step_id] = StepExecutionStatus.FAILED
                                
                                async with get_db_session() as session:
                                    step_dataclass = graph.nodes[step_id]["step_dataclass"]
                                    await self._create_or_update_step_run(
                                        session,
                                        run_db_id,
                                        step_dataclass,
                                        StatusEnum.FAILED,
                                        error_message=f"Error processing task result: {str(e)}",
                                        completed_at=datetime.now(timezone.utc),
                                    )
            
            # Check if we're done or in error state
            if len(completed_steps) == len(graph.nodes):
                break
                
            if context.status == StepExecutionStatus.FAILED.name and not context.parameters.get("continue_on_failure", False):
                # Cancel any running tasks
                for step_id in running_steps:
                    for task_id, s_id in task_id_to_step_id.items():
                        if s_id == step_id:
                            await self.distributed_manager.cancel_task(task_id)
                break
                
            # Avoid busy waiting
            await asyncio.sleep(0.5)
        
        # Determine final workflow status
        if context.status == StepExecutionStatus.RUNNING.name:
            if len(completed_steps) == len(graph.nodes):
                # Check if all steps completed successfully
                all_successful = True
                for step_id in graph.nodes:
                    if context.step_statuses.get(step_id) not in [
                        StepExecutionStatus.COMPLETED, 
                        StepExecutionStatus.SKIPPED
                    ]:
                        all_successful = False
                        break
                
                if all_successful:
                    context.status = StepExecutionStatus.COMPLETED.name
                else:
                    context.status = StepExecutionStatus.FAILED.name
                    if not context.error:
                        context.error = "Some steps failed or were skipped"
            else:
                # Not all steps completed
                context.status = StepExecutionStatus.FAILED.name
                if not context.error:
                    context.error = "Workflow execution incomplete"
        
        return context

    async def _update_run_record(self, run_db_id: uuid.UUID, context: WorkflowContext):
        """Update workflow run record in database"""
        async with get_db_session() as session:
            run_model = await session.get(WorkflowRunModel, run_db_id)
            if run_model:
                run_model.status = StatusEnum[context.status] if hasattr(StatusEnum, context.status) else StatusEnum.FAILED
                run_model.results = context.results
                run_model.artifacts = context.artifacts
                run_model.logs_summary = "\n".join(context.logs[-100:])
                run_model.completed_at = context.end_time or datetime.now(timezone.utc)
                run_model.error_message = context.error
                
                if run_model.started_at:
                    run_model.duration_seconds = (run_model.completed_at - run_model.started_at).total_seconds()
                
                await session.commit()
                logger.info(f"Updated workflow run {run_db_id} with status {run_model.status.name}")
            else:
                logger.error(f"Could not find WorkflowRun {run_db_id} in DB for update")

    # Add this property to avoid KeyError
    @property
    def active_workflows(self):
        """Return active workflows with initialization if needed"""
        if not hasattr(self, '_active_workflows'):
            self._active_workflows = {}
        return self._active_workflows
        
    @property
    def active_steps(self):
        """Return active steps with initialization if needed"""
        if not hasattr(self, '_active_steps'):
            self._active_steps = {}
        return self._active_steps

    # Cache for workflow execution results
    def _get_workflow_cache_key(self, workflow_id: Union[str, uuid.UUID], parameters: Dict[str, Any]) -> str:
        """
        Generate a unique cache key for a workflow based on its ID and parameters.
        This allows for efficient reuse of previous execution results.
        """
        import hashlib
        import json
        
        # Convert parameters to a stable string representation
        params_str = json.dumps(parameters, sort_keys=True)
        
        # Create a hash of the workflow ID and parameters
        key = f"{workflow_id}:{params_str}"
        hash_key = hashlib.md5(key.encode()).hexdigest()
        
        return hash_key
        
    async def get_cached_workflow_result(self, workflow_id: Union[str, uuid.UUID], parameters: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Check if a previous workflow execution with the same parameters exists and return its results.
        
        Args:
            workflow_id: ID of the workflow definition
            parameters: Workflow parameters
            
        Returns:
            Cached workflow results or None if not found
        """
        # Generate cache key
        cache_key = self._get_workflow_cache_key(workflow_id, parameters)
        
        # Check database for previous execution with same parameters
        async with get_db_session() as session:
            # Find workflow definition
            if isinstance(workflow_id, str):
                # Get by name
                stmt = sa.select(WorkflowModel).where(WorkflowModel.name == workflow_id).order_by(WorkflowModel.version.desc())
                result = await session.execute(stmt)
                workflow_model = result.scalar_one_or_none()
                if not workflow_model:
                    return None
                workflow_db_id = workflow_model.id
            else:
                # Get by UUID
                workflow_db_id = workflow_id
            
            # Find previous successful runs with matching parameters hash
            stmt = (
                sa.select(WorkflowRunModel)
                .where(
                    WorkflowRunModel.workflow_id == workflow_db_id,
                    WorkflowRunModel.status == StatusEnum.COMPLETED,
                    WorkflowRunModel.custom_metadata.has_key("cache_key"),  # Check if the key exists in JSON
                    WorkflowRunModel.custom_metadata["cache_key"].as_string() == cache_key
                )
                .order_by(WorkflowRunModel.completed_at.desc())  # Get most recent
                .limit(1)
            )
            
            try:
                result = await session.execute(stmt)
                cached_run = result.scalar_one_or_none()
                
                if cached_run and cached_run.results:
                    logger.info(f"Found cached workflow result for {workflow_id} with key {cache_key}")
                    return {
                        "run_id": str(cached_run.id),
                        "results": cached_run.results,
                        "completed_at": cached_run.completed_at.isoformat() if cached_run.completed_at else None,
                        "duration_seconds": cached_run.duration_seconds
                    }
            except Exception as e:
                logger.warning(f"Error checking workflow cache: {str(e)}")
                
        return None
        
    async def cache_workflow_result(self, run_db_id: uuid.UUID, workflow_id: Union[str, uuid.UUID], parameters: Dict[str, Any]) -> None:
        """
        Mark a workflow run as cacheable for future reference.
        
        Args:
            run_db_id: ID of the workflow run
            workflow_id: ID of the workflow definition
            parameters: Workflow parameters used
        """
        # Generate cache key
        cache_key = self._get_workflow_cache_key(workflow_id, parameters)
        
        # Update custom_metadata to include cache key
        async with get_db_session() as session:
            run_model = await session.get(WorkflowRunModel, run_db_id)
            if run_model and run_model.status == StatusEnum.COMPLETED:
                if not run_model.custom_metadata:
                    run_model.custom_metadata = {}
                    
                # Add cache key to metadata
                run_model.custom_metadata["cache_key"] = cache_key
                run_model.custom_metadata["cacheable"] = True
                
                # Save changes
                await session.commit()
                logger.info(f"Cached workflow result for run {run_db_id} with key {cache_key}")

    async def generate_workflow_visualization(self, workflow_db_id: uuid.UUID, output_format: str = "html") -> str:
        """
        Generate a visualization of the workflow DAG
        
        Args:
            workflow_db_id: Database ID of the workflow to visualize
            output_format: Format of visualization (html, svg, json)
            
        Returns:
            Visualization in the requested format
        """
        # Get workflow definition
        async with get_db_session() as session:
            workflow_model = await session.get(WorkflowModel, workflow_db_id)
            if not workflow_model:
                raise ValueError(f"Workflow definition with ID {workflow_db_id} not found")
                
            # Get workflow steps
            step_definitions = await self.get_step_definition_models(workflow_db_id, session)
            
            # Convert to dataclasses for graph building
            steps_dataclasses = [
                WorkflowStepDefinition(
                    id=step.step_id_in_workflow,
                    name=step.name,
                    description=step.description or "",
                    function_identifier=step.function_identifier or "",
                    parameters=step.default_parameters or {},
                    dependencies=step.dependencies or [],
                    timeout_seconds=step.timeout_seconds,
                    retry_count=step.retry_count or 0,
                    priority=step.priority or 0,
                    estimated_memory_mb=step.estimated_memory_mb or 1000,
                    use_gpu=step.use_gpu or False,
                )
                for step in step_definitions
            ]
            
            # Build graph
            try:
                graph = self._build_workflow_graph(steps_dataclasses)
            except ValueError as e:
                raise ValueError(f"Failed to build workflow graph: {e}")
                
            # Generate visualization
            if output_format == "html":
                return self._generate_html_visualization(workflow_model, graph)
            elif output_format == "svg":
                return self._generate_svg_visualization(workflow_model, graph)
            elif output_format == "json":
                return self._generate_json_visualization(workflow_model, graph)
            else:
                raise ValueError(f"Unsupported output format: {output_format}")
                
    def _generate_html_visualization(self, workflow_model: WorkflowModel, graph: nx.DiGraph) -> str:
        """Generate HTML visualization using PyVis library"""
        try:
            from pyvis.network import Network
            import tempfile
            
            # Create a PyVis network
            net = Network(height="800px", width="100%", directed=True, notebook=False)
            
            # Add nodes with metadata
            for node_id in graph.nodes:
                step_dataclass = graph.nodes[node_id]["step_dataclass"]
                node_title = f"""
                <b>{step_dataclass.name}</b><br>
                <i>{step_dataclass.description}</i><br>
                Function: {step_dataclass.function_identifier}<br>
                Retry Count: {step_dataclass.retry_count}<br>
                Memory: {step_dataclass.estimated_memory_mb}MB<br>
                Priority: {step_dataclass.priority}<br>
                GPU: {'Yes' if step_dataclass.use_gpu else 'No'}
                """
                
                # Set node color based on GPU usage
                color = "#1E90FF" if step_dataclass.use_gpu else "#3CB371"
                
                net.add_node(
                    node_id, 
                    label=step_dataclass.name,
                    title=node_title, 
                    color=color,
                    shape="box" if step_dataclass.use_gpu else "ellipse"
                )
                
            # Add edges with metadata
            for edge in graph.edges:
                net.add_edge(edge[0], edge[1], arrows="to")
                
            # Set physics options for better layout
            net.set_options("""
            {
                "physics": {
                    "hierarchicalRepulsion": {
                        "centralGravity": 0.0,
                        "springLength": 200,
                        "springConstant": 0.01,
                        "nodeDistance": 120,
                        "damping": 0.09
                    },
                    "solver": "hierarchicalRepulsion"
                },
                "layout": {
                    "hierarchical": {
                        "enabled": true,
                        "direction": "LR",
                        "sortMethod": "directed"
                    }
                },
                "interaction": {
                    "hover": true,
                    "navigationButtons": true,
                    "keyboard": true
                }
            }
            """)
            
            # Generate HTML
            with tempfile.NamedTemporaryFile(suffix='.html', delete=False) as tmp:
                net.save_graph(tmp.name)
                with open(tmp.name, 'r') as f:
                    html_content = f.read()
                    
                import os
                os.unlink(tmp.name)
                
                # Add workflow metadata to the HTML
                workflow_info = f"""
                <div style="padding: 20px; background-color: #f8f9fa; margin-bottom: 20px; border-radius: 5px;">
                    <h2>{workflow_model.name} v{workflow_model.version}</h2>
                    <p>{workflow_model.description}</p>
                    <p><b>Steps:</b> {len(graph.nodes)}</p>
                    <p><b>Dependencies:</b> {len(graph.edges)}</p>
                </div>
                """
                
                # Insert workflow info before the network div
                html_content = html_content.replace('<body>', f'<body>{workflow_info}')
                
                return html_content
                
        except ImportError:
            logger.warning("PyVis library not available. Falling back to basic visualization.")
            return self._generate_basic_html_visualization(workflow_model, graph)
            
    def _generate_basic_html_visualization(self, workflow_model: WorkflowModel, graph: nx.DiGraph) -> str:
        """Generate basic HTML visualization without external libraries"""
        nodes_html = ""
        for node_id in graph.nodes:
            step = graph.nodes[node_id]["step_dataclass"]
            nodes_html += f"""
            <div class="step-box" id="{node_id}">
                <div class="step-header">{step.name}</div>
                <div class="step-content">
                    <p>{step.description}</p>
                    <p><b>Function:</b> {step.function_identifier}</p>
                </div>
            </div>
            """
            
        # Generate basic HTML with CSS for DAG layout
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>{workflow_model.name} v{workflow_model.version}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .workflow-info {{ padding: 20px; background-color: #f8f9fa; margin-bottom: 20px; border-radius: 5px; }}
                .step-box {{ border: 1px solid #ddd; border-radius: 5px; margin: 10px; padding: 10px; max-width: 300px; }}
                .step-header {{ font-weight: bold; margin-bottom: 10px; }}
                .step-content {{ font-size: 0.9em; }}
            </style>
        </head>
        <body>
            <div class="workflow-info">
                <h2>{workflow_model.name} v{workflow_model.version}</h2>
                <p>{workflow_model.description}</p>
                <p><b>Steps:</b> {len(graph.nodes)}</p>
                <p><b>Dependencies:</b> {len(graph.edges)}</p>
            </div>
            <div class="workflow-steps">
                {nodes_html}
            </div>
        </body>
        </html>
        """
        
        return html
        
    def _generate_svg_visualization(self, workflow_model: WorkflowModel, graph: nx.DiGraph) -> str:
        """Generate SVG visualization"""
        try:
            import matplotlib.pyplot as plt
            import io
            
            # Create a figure
            plt.figure(figsize=(12, 8))
            
            # Generate positions using a hierarchical layout
            pos = nx.drawing.nx_agraph.graphviz_layout(graph, prog="dot")
            
            # Get node metadata for coloring
            node_colors = []
            for node in graph.nodes:
                step = graph.nodes[node]["step_dataclass"]
                node_colors.append("#1E90FF" if step.use_gpu else "#3CB371")
            
            # Draw the graph
            nx.draw(
                graph,
                pos,
                with_labels=True,
                node_color=node_colors,
                node_size=1500,
                font_size=10,
                font_weight="bold",
                arrowsize=20,
                edge_color="#666666",
            )
            
            # Add title
            plt.title(f"{workflow_model.name} v{workflow_model.version}", fontsize=16)
            
            # Save to SVG
            svg_io = io.StringIO()
            plt.savefig(svg_io, format="svg")
            plt.close()
            
            return svg_io.getvalue()
            
        except ImportError:
            logger.warning("Matplotlib or pygraphviz library not available. Returning empty SVG.")
            return f'<svg xmlns="http://www.w3.org/2000/svg" width="800" height="600"><text x="10" y="20">Visualization libraries not available</text></svg>'
            
    def _generate_json_visualization(self, workflow_model: WorkflowModel, graph: nx.DiGraph) -> str:
        """Generate JSON representation of the workflow for custom visualization"""
        import json
        
        # Prepare nodes data
        nodes_data = []
        for node_id in graph.nodes:
            step = graph.nodes[node_id]["step_dataclass"]
            nodes_data.append({
                "id": node_id,
                "name": step.name,
                "description": step.description,
                "function": step.function_identifier,
                "parameters": step.parameters,
                "use_gpu": step.use_gpu,
                "retry_count": step.retry_count,
                "priority": step.priority,
                "memory_mb": step.estimated_memory_mb,
            })
            
        # Prepare edges data
        edges_data = []
        for edge in graph.edges:
            edges_data.append({
                "source": edge[0],
                "target": edge[1],
            })
            
        # Prepare workflow data
        workflow_data = {
            "id": str(workflow_model.id),
            "name": workflow_model.name,
            "version": workflow_model.version,
            "description": workflow_model.description,
            "nodes": nodes_data,
            "edges": edges_data,
        }
        
        return json.dumps(workflow_data, indent=2)
        
    async def generate_workflow_monitor_dashboard(self, run_db_id: uuid.UUID) -> str:
        """
        Generate an HTML dashboard for monitoring a workflow run
        
        Args:
            run_db_id: ID of the workflow run to monitor
            
        Returns:
            HTML dashboard
        """
        try:
            # Get workflow status
            workflow_status = await self.get_workflow_status(run_db_id)
            
            # Get workflow metrics
            workflow_metrics = await self.get_workflow_metrics(run_db_id)
            
            # Get workflow logs
            workflow_logs = await self.get_workflow_logs(run_db_id)
            
            # Generate HTML dashboard
            html = (
                f"""<!DOCTYPE html>
                <html>
                <head>
                    <title>Workflow Monitor - {workflow_status["workflow_name"]}</title>
                    <meta http-equiv="refresh" content="10">
                    <style>
                        body {{ font-family: Arial, sans-serif; margin: 0; padding: 20px; background-color: #f5f5f5; }}
                        .dashboard {{ display: flex; flex-wrap: wrap; gap: 20px; }}
                        .panel {{ background-color: white; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); padding: 20px; }}
                        .header {{ width: calc(100% - 40px); }}
                        .status {{ width: 250px; }}
                        .metrics {{ width: calc(50% - 50px); }}
                        .steps {{ width: calc(50% - 50px); }}
                        .logs {{ width: calc(100% - 40px); }}
                        .progress-bar {{ width: 100%; height: 20px; background-color: #e0e0e0; border-radius: 10px; margin-top: 10px; }}
                        .progress-fill {{ height: 100%; background-color: #4caf50; border-radius: 10px; }}
                        .step-status {{ margin-bottom: 10px; padding: 10px; border-radius: 5px; }}
                        .step-running {{ background-color: #fff9c4; }}
                        .step-completed {{ background-color: #e8f5e9; }}
                        .step-failed {{ background-color: #ffebee; }}
                        .step-skipped {{ background-color: #e0e0e0; }}
                        table {{ width: 100%; border-collapse: collapse; }}
                        th, td {{ padding: 8px; text-align: left; border-bottom: 1px solid #ddd; }}
                        th {{ background-color: #f5f5f5; }}
                        .logs-container {{ max-height: 400px; overflow-y: auto; background-color: #263238; color: #eeffff; padding: 15px; border-radius: 5px; font-family: monospace; }}
                    </style>
                </head>
                <body>
                    <div class="dashboard">
                        <div class="panel header">
                            <h1>{workflow_status["workflow_name"]} - Run {workflow_status["run_id"]}</h1>
                            <div style="display: flex; justify-content: space-between; align-items: center;">
                                <div>
                                    <p><strong>Status:</strong> {workflow_status["status"]}</p>
                                    <p><strong>Started:</strong> {workflow_status["started_at"] or "Not started"}</p>
                                    <p><strong>Completed:</strong> {workflow_status["completed_at"] or "In progress"}</p>
                                </div>
                                <div>
                                    <h3>Progress: {workflow_status["progress"]:.1f}%</h3>
                                    <div class="progress-bar">
                                        <div class="progress-fill" style="width: {workflow_status["progress"]}%;"></div>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="panel status">
                            <h2>Status</h2>
                            <table>
                                <tr>
                                    <td>Total Steps</td>
                                    <td>{workflow_status["total_steps"]}</td>
                                </tr>
                                <tr>
                                    <td>Completed</td>
                                    <td>{workflow_status["completed_steps"]}</td>
                                </tr>
                                <tr>
                                    <td>Duration</td>
                                    <td>{workflow_status["duration_seconds"] or "Running..."}</td>
                                </tr>
                                <tr>
                                    <td>Error</td>
                                    <td>{workflow_status["error"] or "None"}</td>
                                </tr>
                            </table>
                        </div>
                        
                        <div class="panel metrics">
                            <h2>Metrics</h2>
                            <table>
                                <tr>
                                    <th>Metric</th>
                                    <th>Value</th>
                                </tr>
                                <tr>
                                    <td>Critical Path Duration</td>
                                    <td>{workflow_metrics.get("critical_path_duration_seconds", "N/A")} seconds</td>
                                </tr>
                                <tr>
                                    <td>Failed Steps</td>
                                    <td>{workflow_metrics.get("failed_steps", 0)}</td>
                                </tr>
                                <tr>
                                    <td>Running Steps</td>
                                    <td>{workflow_metrics.get("running_steps", 0)}</td>
                                </tr>
                                <tr>
                                    <td>Pending Steps</td>
                                    <td>{workflow_metrics.get("pending_steps", 0)}</td>
                                </tr>
                            </table>
                        </div>
                        
                        <div class="panel steps">
                            <h2>Steps</h2>
                            <div style="max-height: 300px; overflow-y: auto;">""")
                        
            # Add step status sections
            step_statuses = workflow_status["step_statuses"]
            for step_id, status in step_statuses.items():
                css_class = "step-running"
                if status["status"] == "COMPLETED":
                    css_class = "step-completed"
                elif status["status"] == "FAILED" or status["status"] == "TIMEOUT":
                    css_class = "step-failed"
                elif status["status"] == "SKIPPED" or status["status"] == "CANCELLED":
                    css_class = "step-skipped"
                    
                duration = status.get("duration_seconds", "N/A")
                if duration and isinstance(duration, (int, float)):
                    duration = f"{duration:.2f} seconds"
                    
                html += f"""
                <div class="step-status {css_class}">
                    <strong>{step_id}</strong>: {status["status"]}
                    <div>Started: {status["started_at"] or "N/A"}</div>
                    <div>Duration: {duration}</div>
                    {f'<div style="color: red;">Error: {status["error"]}</div>' if status["error"] else ''}
                </div>
                """
                
            # Complete the HTML
            logs_display_content = (workflow_logs or "No logs available").replace('\n', '<br>')
            logs_display_content = logs_display_content.replace(r'\n', '<br>') # Handle literal \n
            html += f"""
                        </div>
                    </div>
                    
                    <div class="panel logs">
                        <h2>Logs</h2>
                        <div class="logs-container">
                            {logs_display_content}
                        </div>
                    </div>
                </div>
                
                <script>
                    // Auto-scroll logs to bottom
                    window.onload = function() {{
                        var logsContainer = document.querySelector('.logs-container');
                        logsContainer.scrollTop = logsContainer.scrollHeight;
                    }};
                </script>
            </body>
            </html>
            """
            
        except Exception as e:
            # Return error page
            error_message = str(e).replace("\\", "")
            return (
                "<!DOCTYPE html>\n"
                "<html>\n"
                "<head>\n"
                "    <title>Workflow Monitor Error</title>\n"
                "</head>\n"
                "<body>\n"
                "    <h1>Error Generating Dashboard</h1>\n"
                "    <p>" + error_message + "</p>\n"
                "</body>\n"
                "</html>"
            )

    # Performance optimization for handling large workflows
    async def optimize_workflow_execution_plan(self, workflow_db_id: uuid.UUID) -> Dict[str, Any]:
        """
        Analyze workflow definition and optimize execution plan for better performance.
        
        This method analyzes step dependencies, resource requirements, and historical
        execution times to generate an optimized execution plan.
        
        Args:
            workflow_db_id: ID of the workflow definition
            
        Returns:
            Dictionary with optimization suggestions and estimated performance improvements
        """
        async with get_db_session() as session:
            # Get workflow definition
            workflow_model = await session.get(WorkflowModel, workflow_db_id)
            if not workflow_model:
                raise ValueError(f"Workflow definition with ID {workflow_db_id} not found")
                
            # Get step definitions
            step_definitions = await self.get_step_definition_models(workflow_db_id, session)
            
            # Convert to dataclasses for graph building
            steps_dataclasses = [
                WorkflowStepDefinition(
                    id=step.step_id_in_workflow,
                    name=step.name,
                    description=step.description or "",
                    function_identifier=step.function_identifier or "",
                    parameters=step.default_parameters or {},
                    dependencies=step.dependencies or [],
                    timeout_seconds=step.timeout_seconds,
                    retry_count=step.retry_count or 0,
                    priority=step.priority or 0,
                    estimated_memory_mb=step.estimated_memory_mb or 1000,
                    use_gpu=step.use_gpu or False,
                )
                for step in step_definitions
            ]
            
            # Build graph
            try:
                graph = self._build_workflow_graph(steps_dataclasses)
            except ValueError as e:
                raise ValueError(f"Failed to build workflow graph: {e}")
            
            # Get historical execution data
            stmt = sa.select(WorkflowRunModel).where(
                WorkflowRunModel.workflow_id == workflow_db_id,
                WorkflowRunModel.status == StatusEnum.COMPLETED
            ).order_by(WorkflowRunModel.completed_at.desc()).limit(10)
            
            result = await session.execute(stmt)
            historical_runs = result.scalars().all()
            
            # Collect historical step execution times
            historical_step_data = {}
            for run in historical_runs:
                step_runs_stmt = sa.select(WorkflowStepRunModel).where(
                    WorkflowStepRunModel.workflow_run_id == run.id
                )
                step_runs_result = await session.execute(step_runs_stmt)
                step_runs = step_runs_result.scalars().all()
                
                for step_run in step_runs:
                    if step_run.step_id_in_workflow not in historical_step_data:
                        historical_step_data[step_run.step_id_in_workflow] = []
                    
                    if step_run.duration_seconds:
                        historical_step_data[step_run.step_id_in_workflow].append({
                            "duration": step_run.duration_seconds,
                            "memory_used": step_run.custom_metadata.get("memory_mb") if step_run.custom_metadata else None,
                            "cpu_used": step_run.custom_metadata.get("cpu_percent") if step_run.custom_metadata else None
                        })
            
            # Calculate average durations
            avg_durations = {}
            for step_id, data in historical_step_data.items():
                if data:
                    durations = [entry["duration"] for entry in data if isinstance(entry["duration"], (int, float))]
                    if durations:
                        avg_durations[step_id] = sum(durations) / len(durations)
            
            # Identify parallelization opportunities
            parallelizable_groups = []
            current_level = set()
            visited = set()
            
            # Group steps by levels (steps that can run in parallel)
            while len(visited) < len(graph.nodes):
                current_level = {node for node in graph.nodes 
                               if node not in visited and 
                               all(pred in visited for pred in graph.predecessors(node))}
                
                if not current_level:
                    break
                    
                visited.update(current_level)
                if len(current_level) > 1:
                    parallelizable_groups.append(list(current_level))
            
            # Resource analysis for GPU and memory
            gpu_steps = [step_def.step_id_in_workflow for step_def in steps_dataclasses if step_def.use_gpu]
            high_memory_steps = [step_def.step_id_in_workflow for step_def in steps_dataclasses 
                                if step_def.estimated_memory_mb > 0.5 * self.available_memory_mb]
            
            # Identify bottlenecks
            bottlenecks = []
            if historical_runs:
                # Calculate critical path
                if avg_durations:
                    critical_path = self._compute_critical_path(graph, 
                                                              {step_id: {"execution_time": duration} 
                                                               for step_id, duration in avg_durations.items()})
                    
                    # Find top 3 longest steps in critical path
                    critical_steps = [step for step in critical_path if step in avg_durations]
                    critical_steps.sort(key=lambda s: avg_durations.get(s, 0), reverse=True)
                    bottlenecks = critical_steps[:3]
            
            # Generate optimization recommendations
            recommendations = []
            if bottlenecks:
                for step_id in bottlenecks:
                    step_def = next((s for s in steps_dataclasses if s.id == step_id), None)
                    if step_def:
                        recommendations.append({
                            "step_id": step_id,
                            "step_name": step_def.name,
                            "avg_duration": avg_durations.get(step_id, "Unknown"),
                            "recommendation": "Consider optimizing this bottleneck step or splitting into smaller steps"
                        })
            
            # Suggest better GPU utilization
            if gpu_steps:
                gpu_conflicts = []
                for group in parallelizable_groups:
                    gpu_count_in_group = sum(1 for s in group if s in gpu_steps)
                    if gpu_count_in_group > len(self.gpu_devices or [0]):
                        gpu_conflicts.append(group)
                
                if gpu_conflicts:
                    recommendations.append({
                        "type": "gpu_conflict",
                        "description": f"Found {len(gpu_conflicts)} step groups with more GPU steps than available GPUs",
                        "recommendation": "Consider adjusting dependencies to balance GPU workloads"
                    })
            
            # Suggest better memory utilization
            if high_memory_steps:
                for group in parallelizable_groups:
                    high_mem_in_group = [s for s in group if s in high_memory_steps]
                    if len(high_mem_in_group) > 1:
                        recommendations.append({
                            "type": "memory_pressure",
                            "steps": high_mem_in_group,
                            "recommendation": "Consider running these high-memory steps sequentially to avoid memory pressure"
                        })
            
            # Calculate potential speed improvement from optimizations
            current_critical_path_time = sum(avg_durations.get(step, 0) for step in critical_path)
            potential_speedup = 0
            
            if current_critical_path_time > 0:
                # Estimate speedup from parallelization
                parallelization_gain = 0
                for group in parallelizable_groups:
                    if len(group) > 1:
                        group_times = [avg_durations.get(step, 0) for step in group]
                        sequential_time = sum(group_times)
                        parallel_time = max(group_times) if group_times else 0
                        parallelization_gain += (sequential_time - parallel_time)
                
                potential_speedup = (parallelization_gain / current_critical_path_time) * 100
            
            return {
                "workflow_id": str(workflow_db_id),
                "workflow_name": workflow_model.name,
                "total_steps": len(steps_dataclasses),
                "historical_runs_analyzed": len(historical_runs),
                "parallelizable_groups": parallelizable_groups,
                "critical_path": critical_path if 'critical_path' in locals() else [],
                "bottleneck_steps": bottlenecks,
                "recommendations": recommendations,
                "estimated_speedup_percent": round(potential_speedup, 2) if 'potential_speedup' in locals() else 0,
                "gpu_steps": gpu_steps,
                "high_memory_steps": high_memory_steps
            }

    # Advanced GPU management
    def _setup_gpu_scheduler(self):
        """
        Initialize advanced GPU scheduling system for optimal GPU allocation
        across workflow steps.
        """
        self.gpu_allocation = {gpu_id: None for gpu_id in self.gpu_devices} if self.gpu_devices else {}
        self.gpu_queue = []
        self.gpu_metrics = {gpu_id: {"utilization": [], "memory": [], "temperature": []} 
                          for gpu_id in self.gpu_devices} if self.gpu_devices else {}
        
        # Start GPU monitoring if GPUs are available
        if self.gpu_devices:
            self._start_gpu_monitor()
    
    def _start_gpu_monitor(self):
        """Start a background thread to monitor GPU metrics"""
        import threading
        
        def monitor_gpus():
            while True:
                try:
                    self._update_gpu_metrics()
                    time.sleep(5)  # Update every 5 seconds
                except Exception as e:
                    logger.error(f"Error in GPU monitor: {str(e)}")
                    time.sleep(10)  # Longer pause after error
        
        monitor_thread = threading.Thread(target=monitor_gpus, daemon=True)
        monitor_thread.start()
        logger.info(f"Started GPU monitoring thread for devices: {self.gpu_devices}")
    
    def _update_gpu_metrics(self):
        """Update GPU metrics using PyTorch or other available methods"""
        try:
            import torch
            if torch.cuda.is_available():
                for device_id in self.gpu_devices:
                    # Memory usage
                    torch.cuda.set_device(device_id)
                    allocated = torch.cuda.memory_allocated(device_id) / (1024 * 1024)
                    reserved = torch.cuda.memory_reserved(device_id) / (1024 * 1024)
                    
                    # Get device properties
                    props = torch.cuda.get_device_properties(device_id)
                    total = props.total_memory / (1024 * 1024)
                    
                    # Store metrics
                    utilization = allocated / total * 100 if total > 0 else 0
                    self.gpu_metrics[device_id]["utilization"].append(utilization)
                    self.gpu_metrics[device_id]["memory"].append(allocated)
                    
                    # Keep only recent history
                    max_history = 100
                    if len(self.gpu_metrics[device_id]["utilization"]) > max_history:
                        self.gpu_metrics[device_id]["utilization"] = self.gpu_metrics[device_id]["utilization"][-max_history:]
                        self.gpu_metrics[device_id]["memory"] = self.gpu_metrics[device_id]["memory"][-max_history:]
                    
                    # Report to metrics collector if available
                    if self.metrics_collector:
                        self.metrics_collector.gauge(f"gpu_{device_id}_utilization", utilization)
                        self.metrics_collector.gauge(f"gpu_{device_id}_memory_used_mb", allocated)
                        self.metrics_collector.gauge(f"gpu_{device_id}_memory_reserved_mb", reserved)
        except ImportError:
            logger.debug("PyTorch not available for GPU metrics")
        except Exception as e:
            logger.error(f"Error updating GPU metrics: {str(e)}")
    
    async def allocate_gpu(self, step_id: str, memory_requirement: int = 0) -> Optional[int]:
        """
        Allocate a GPU for a workflow step with smart scheduling
        
        Args:
            step_id: ID of the workflow step
            memory_requirement: Memory required in MB
            
        Returns:
            GPU device ID or None if no GPU is available
        """
        if not self.gpu_devices:
            return None
        
        # Check for available GPUs
        available_gpus = []
        for gpu_id, allocated_to in self.gpu_allocation.items():
            if allocated_to is None:
                available_gpus.append(gpu_id)
        
        if not available_gpus:
            # No free GPUs, add to queue
            self.gpu_queue.append((step_id, memory_requirement))
            logger.info(f"No free GPUs available. Step {step_id} added to GPU queue.")
            return None
        
        # Find best GPU based on memory availability
        best_gpu = available_gpus[0]
        best_available_memory = 0
        
        for gpu_id in available_gpus:
            # Get available memory
            memory = self.available_gpu_memory.get(gpu_id, 0)
            if memory > best_available_memory:
                best_available_memory = memory
                best_gpu = gpu_id
        
        # Check if memory requirement is met
        if memory_requirement > best_available_memory and best_available_memory > 0:
            logger.warning(
                f"Step {step_id} requires {memory_requirement}MB but best GPU has only {best_available_memory}MB. "
                f"Allocating anyway, but performance may be affected."
            )
        
        # Allocate GPU
        self.gpu_allocation[best_gpu] = step_id
        logger.info(f"Allocated GPU {best_gpu} to step {step_id}")
        
        return best_gpu
    
    async def release_gpu(self, gpu_id: int):
        """
        Release a GPU and assign to next step in queue if available
        
        Args:
            gpu_id: ID of the GPU to release
        """
        if gpu_id not in self.gpu_allocation:
            logger.warning(f"Attempted to release unallocated GPU {gpu_id}")
            return
        
        current_step = self.gpu_allocation[gpu_id]
        logger.info(f"Released GPU {gpu_id} from step {current_step}")
        
        # Mark as available
        self.gpu_allocation[gpu_id] = None
        
        # Check queue
        if self.gpu_queue:
            next_step, memory_req = self.gpu_queue.pop(0)
            self.gpu_allocation[gpu_id] = next_step
            logger.info(f"Allocated GPU {gpu_id} to queued step {next_step}")

    # Enhanced telemetry and observability
    def setup_prometheus_metrics(self, prometheus_registry=None):
        """
        Configure Prometheus metrics for workflow monitoring
        
        Args:
            prometheus_registry: Optional Prometheus registry
        """
        try:
            import prometheus_client
            
            # Use provided registry or create a new one
            registry = prometheus_registry or prometheus_client.CollectorRegistry()
            
            # Define metrics
            self.prom_active_workflows = prometheus_client.Gauge(
                'workflow_manager_active_workflows',
                'Number of active workflows',
                registry=registry
            )
            
            self.prom_workflow_durations = prometheus_client.Histogram(
                'workflow_manager_workflow_duration_seconds',
                'Workflow execution durations in seconds',
                ['workflow_name', 'status'],
                registry=registry
            )
            
            self.prom_step_durations = prometheus_client.Histogram(
                'workflow_manager_step_duration_seconds',
                'Step execution durations in seconds',
                ['workflow_name', 'step_name', 'status'],
                registry=registry
            )
            
            self.prom_workflow_memory = prometheus_client.Gauge(
                'workflow_manager_workflow_memory_mb',
                'Workflow memory usage in MB',
                ['workflow_id'],
                registry=registry
            )
            
            self.prom_error_count = prometheus_client.Counter(
                'workflow_manager_errors_total',
                'Total number of workflow errors',
                ['workflow_name', 'error_type'],
                registry=registry
            )
            
            # Store registry
            self.prometheus_registry = registry
            logger.info("Prometheus metrics configured for workflow manager")
            
            # Start metrics server if requested
            if hasattr(self, 'prometheus_port'):
                prometheus_client.start_http_server(
                    self.prometheus_port, 
                    registry=registry
                )
                logger.info(f"Prometheus metrics server started on port {self.prometheus_port}")
                
            return registry
            
        except ImportError:
            logger.warning("Prometheus client not available. Metrics will not be exposed.")
            return None

    # Integration with external monitoring systems
    async def send_metrics_to_datadog(self, metrics: Dict[str, Any], tags: List[str] = None):
        """
        Send workflow metrics to Datadog
        
        Args:
            metrics: Dictionary of metrics to send
            tags: Optional tags to attach to metrics
        """
        try:
            import datadog
            from datadog import api
            
            # Check if Datadog is configured
            if not hasattr(self, 'datadog_config'):
                logger.warning("Datadog not configured. Metrics will not be sent.")
                return
            
            # Ensure Datadog is initialized
            datadog.initialize(**self.datadog_config)
            
            # Send metrics
            for metric_name, value in metrics.items():
                if isinstance(value, (int, float)):
                    api.Metric.send(
                        metric=f"workflow.{metric_name}",
                        points=value,
                        tags=tags or []
                    )
            
            logger.debug(f"Sent {len(metrics)} metrics to Datadog with tags: {tags}")
            
        except ImportError:
            logger.warning("Datadog client library not available. Metrics will not be sent.")
        except Exception as e:
            logger.error(f"Error sending metrics to Datadog: {str(e)}")
    
    def configure_datadog(self, api_key: str, app_key: str = None, api_host: str = None):
        """
        Configure Datadog integration
        
        Args:
            api_key: Datadog API key
            app_key: Optional Datadog application key
            api_host: Optional Datadog API host
        """
        self.datadog_config = {
            "api_key": api_key,
            "app_key": app_key,
            "api_host": api_host
        }
        logger.info("Datadog integration configured")
    
    async def send_alert(self, alert_type: str, message: str, workflow_id: str = None, run_id: str = None, severity: str = "warning"):
        """
        Send alerts to configured notification systems
        
        Args:
            alert_type: Type of alert
            message: Alert message
            workflow_id: Optional workflow ID
            run_id: Optional run ID
            severity: Alert severity (info, warning, error, critical)
        """
        alert_data = {
            "type": alert_type,
            "message": message,
            "workflow_id": workflow_id,
            "run_id": run_id,
            "severity": severity,
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
        
        # Log the alert
        log_method = getattr(logger, severity) if hasattr(logger, severity) else logger.warning
        log_method(f"ALERT: {message} (type: {alert_type}, workflow: {workflow_id}, run: {run_id})")
        
        # Send to configured alert endpoints
        if hasattr(self, 'alert_handlers'):
            for handler in self.alert_handlers:
                try:
                    await handler(alert_data)
                except Exception as e:
                    logger.error(f"Error sending alert to handler: {str(e)}")
    
    def add_alert_handler(self, handler_func):
        """
        Add a handler function for alerts
        
        Args:
            handler_func: Async function that accepts alert data dictionary
        """
        if not hasattr(self, 'alert_handlers'):
            self.alert_handlers = []
        
        self.alert_handlers.append(handler_func)
        logger.info(f"Added alert handler: {handler_func.__name__}")
    
    # Workflow history and insights
    async def get_workflow_history(self, workflow_id: Union[str, uuid.UUID], limit: int = 20, offset: int = 0) -> List[Dict[str, Any]]:
        """
        Get historical runs of a workflow with detailed statistics
        
        Args:
            workflow_id: Name or ID of the workflow
            limit: Maximum number of runs to return
            offset: Offset for pagination
            
        Returns:
            List of workflow run history entries
        """
        async with get_db_session() as session:
            # Get workflow ID
            if isinstance(workflow_id, str):
                # Find by name
                stmt = sa.select(WorkflowModel).where(WorkflowModel.name == workflow_id)
                result = await session.execute(stmt)
                workflow_model = result.scalar_one_or_none()
                if not workflow_model:
                    raise ValueError(f"Workflow '{workflow_id}' not found")
                workflow_db_id = workflow_model.id
            else:
                workflow_db_id = workflow_id
            
            # Get run history
            runs_stmt = (
                sa.select(WorkflowRunModel)
                .where(WorkflowRunModel.workflow_id == workflow_db_id)
                .order_by(WorkflowRunModel.created_at.desc())
                .limit(limit)
                .offset(offset)
            )
            
            runs_result = await session.execute(runs_stmt)
            runs = runs_result.scalars().all()
            
            # Calculate statistics
            history = []
            for run in runs:
                # Get step runs
                steps_stmt = sa.select(WorkflowStepRunModel).where(
                    WorkflowStepRunModel.workflow_run_id == run.id
                )
                steps_result = await session.execute(steps_stmt)
                steps = steps_result.scalars().all()
                
                # Group steps by status
                step_status_counts = {}
                for step in steps:
                    status = step.status.name if step.status else "UNKNOWN"
                    step_status_counts[status] = step_status_counts.get(status, 0) + 1
                
                # Calculate run statistics
                total_steps = len(steps)
                completed_steps = sum(1 for step in steps if step.status == StatusEnum.COMPLETED)
                failed_steps = sum(1 for step in steps if step.status in [
                    StatusEnum.FAILED, StatusEnum.TIMEOUT
                ])
                
                # Calculate average step duration
                step_durations = [step.duration_seconds for step in steps if step.duration_seconds]
                avg_step_duration = sum(step_durations) / len(step_durations) if step_durations else None
                
                history.append({
                    "run_id": str(run.id),
                    "status": run.status.name if run.status else "UNKNOWN",
                    "parameters": run.parameters,
                    "created_at": run.created_at.isoformat() if run.created_at else None,
                    "started_at": run.started_at.isoformat() if run.started_at else None,
                    "completed_at": run.completed_at.isoformat() if run.completed_at else None,
                    "duration_seconds": run.duration_seconds,
                    "total_steps": total_steps,
                    "completed_steps": completed_steps,
                    "failed_steps": failed_steps,
                    "step_status_counts": step_status_counts,
                    "avg_step_duration": avg_step_duration,
                    "error_message": run.error_message,
                    "triggered_by": run.triggered_by
                })
            
            return history
    
    async def get_workflow_performance_trends(self, workflow_id: Union[str, uuid.UUID], timeframe_days: int = 30) -> Dict[str, Any]:
        """
        Get performance trend analysis for a workflow over time
        
        Args:
            workflow_id: Name or ID of the workflow
            timeframe_days: Number of days to look back
            
        Returns:
            Dictionary with performance trend analysis
        """
        async with get_db_session() as session:
            # Get workflow ID
            if isinstance(workflow_id, str):
                # Find by name
                stmt = sa.select(WorkflowModel).where(WorkflowModel.name == workflow_id)
                result = await session.execute(stmt)
                workflow_model = result.scalar_one_or_none()
                if not workflow_model:
                    raise ValueError(f"Workflow '{workflow_id}' not found")
                workflow_db_id = workflow_model.id
                workflow_name = workflow_model.name
            else:
                workflow_db_id = workflow_id
                # Get name
                wf_model = await session.get(WorkflowModel, workflow_db_id)
                workflow_name = wf_model.name if wf_model else str(workflow_db_id)
            
            # Calculate timeframe
            cutoff_date = datetime.now(timezone.utc) - timedelta(days=timeframe_days)
            
            # Get runs within timeframe
            runs_stmt = (
                sa.select(WorkflowRunModel)
                .where(
                    WorkflowRunModel.workflow_id == workflow_db_id,
                    WorkflowRunModel.created_at >= cutoff_date
                )
                .order_by(WorkflowRunModel.created_at.asc())
            )
            
            runs_result = await session.execute(runs_stmt)
            runs = runs_result.scalars().all()
            
            if not runs:
                return {
                    "workflow_id": str(workflow_db_id),
                    "workflow_name": workflow_name,
                    "timeframe_days": timeframe_days,
                    "run_count": 0,
                    "message": "No workflow runs found in the specified timeframe"
                }
            
            # Group runs by date
            runs_by_date = {}
            for run in runs:
                date_key = run.created_at.date().isoformat()
                if date_key not in runs_by_date:
                    runs_by_date[date_key] = []
                runs_by_date[date_key].append(run)
            
            # Calculate statistics by date
            daily_stats = []
            for date_key, date_runs in runs_by_date.items():
                successful = sum(1 for run in date_runs if run.status == StatusEnum.COMPLETED)
                failed = sum(1 for run in date_runs if run.status == StatusEnum.FAILED)
                durations = [run.duration_seconds for run in date_runs 
                           if run.duration_seconds and run.status == StatusEnum.COMPLETED]
                
                avg_duration = sum(durations) / len(durations) if durations else None
                success_rate = (successful / len(date_runs)) * 100 if date_runs else 0
                
                daily_stats.append({
                    "date": date_key,
                    "run_count": len(date_runs),
                    "successful": successful,
                    "failed": failed,
                    "avg_duration_seconds": avg_duration,
                    "success_rate_percent": success_rate
                })
            
            # Calculate overall statistics
            all_durations = [run.duration_seconds for run in runs 
                           if run.duration_seconds and run.status == StatusEnum.COMPLETED]
            
            successful_runs = sum(1 for run in runs if run.status == StatusEnum.COMPLETED)
            failed_runs = sum(1 for run in runs if run.status == StatusEnum.FAILED)
            
            overall_success_rate = (successful_runs / len(runs)) * 100 if runs else 0
            overall_avg_duration = sum(all_durations) / len(all_durations) if all_durations else None
            
            # Calculate trend
            if len(daily_stats) >= 2:
                # Duration trend
                if all([stats.get("avg_duration_seconds") for stats in daily_stats[-2:]]):
                    latest_duration = daily_stats[-1]["avg_duration_seconds"]
                    previous_duration = daily_stats[-2]["avg_duration_seconds"]
                    duration_change_percent = ((latest_duration - previous_duration) / previous_duration) * 100
                else:
                    duration_change_percent = None
                
                # Success rate trend
                latest_success_rate = daily_stats[-1]["success_rate_percent"]
                previous_success_rate = daily_stats[-2]["success_rate_percent"]
                success_rate_change = latest_success_rate - previous_success_rate
            else:
                duration_change_percent = None
                success_rate_change = None
            
            return {
                "workflow_id": str(workflow_db_id),
                "workflow_name": workflow_name,
                "timeframe_days": timeframe_days,
                "run_count": len(runs),
                "daily_stats": daily_stats,
                "overall_stats": {
                    "success_rate_percent": overall_success_rate,
                    "avg_duration_seconds": overall_avg_duration,
                    "successful_runs": successful_runs,
                    "failed_runs": failed_runs
                },
                "trends": {
                    "duration_change_percent": duration_change_percent,
                    "success_rate_change": success_rate_change
                }
            }

    async def run_workflow_tests(self, workflow_id: Union[str, uuid.UUID], test_parameters: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Run a comprehensive set of tests for a workflow to ensure it works correctly
        
        Args:
            workflow_id: ID or name of the workflow to test
            test_parameters: List of parameter sets to test with
            
        Returns:
            Test report with results
        """
        if not test_parameters:
            test_parameters = [{}]  # Default empty parameters
            
        async with get_db_session() as session:
            # Get workflow definition
            if isinstance(workflow_id, str):
                stmt = sa.select(WorkflowModel).where(WorkflowModel.name == workflow_id)
                result = await session.execute(stmt)
                workflow_model = result.scalar_one_or_none()
                if not workflow_model:
                    raise ValueError(f"Workflow '{workflow_id}' not found")
                workflow_db_id = workflow_model.id
                workflow_name = workflow_model.name
            else:
                workflow_db_id = workflow_id
                workflow_model = await session.get(WorkflowModel, workflow_db_id)
                if not workflow_model:
                    raise ValueError(f"Workflow with ID {workflow_id} not found")
                workflow_name = workflow_model.name
            
            # Start test suite
            start_time = time.time()
            test_results = []
            
            for i, params in enumerate(test_parameters):
                test_name = f"Test {i+1}" if len(test_parameters) > 1 else "Default Test"
                
                # Create a test report entry
                test_report = {
                    "test_name": test_name,
                    "parameters": params,
                    "status": "PENDING",
                    "steps_results": {},
                    "errors": [],
                    "run_id": None,
                    "duration_seconds": None
                }
                
                try:
                    # Create workflow run
                    run_id = await self.create_workflow_run(
                        workflow_db_id,
                        parameters=params,
                        triggered_by="workflow_test",
                        custom_metadata={"test_run": True, "test_name": test_name}
                    )
                    
                    test_report["run_id"] = str(run_id)
                    
                    # Execute workflow with caching disabled for tests
                    test_start = time.time()
                    context = await self.execute_workflow(run_id, use_cache=False)
                    test_duration = time.time() - test_start
                    
                    # Record results
                    test_report["status"] = context.status
                    test_report["duration_seconds"] = test_duration
                    
                    # Check each step's status
                    for step_id, status in context.step_statuses.items():
                        step_result = {
                            "status": status.name if isinstance(status, StepExecutionStatus) else status,
                            "duration": context.step_times.get(step_id, {}).get("duration")
                        }
                        
                        if status == StepExecutionStatus.FAILED or (isinstance(status, str) and status == "FAILED"):
                            step_result["error"] = context.step_times.get(step_id, {}).get("error", "Unknown error")
                            test_report["errors"].append(f"Step {step_id} failed: {step_result['error']}")
                            
                        test_report["steps_results"][step_id] = step_result
                    
                    # Overall test status
                    if context.status == StepExecutionStatus.COMPLETED.name:
                        test_report["status"] = "PASSED"
                    else:
                        test_report["status"] = "FAILED"
                        if context.error:
                            test_report["errors"].append(context.error)
                            
                except Exception as e:
                    test_report["status"] = "ERROR"
                    test_report["errors"].append(f"Test execution error: {str(e)}")
                    logger.error(f"Error running test for workflow {workflow_name}: {str(e)}", exc_info=True)
                
                test_results.append(test_report)
            
            # Generate overall test report
            total_duration = time.time() - start_time
            passed_tests = sum(1 for t in test_results if t["status"] == "PASSED")
            
            return {
                "workflow_id": str(workflow_db_id),
                "workflow_name": workflow_name,
                "test_count": len(test_results),
                "tests_passed": passed_tests,
                "tests_failed": len(test_results) - passed_tests,
                "success_rate": (passed_tests / len(test_results)) * 100 if test_results else 0,
                "total_duration_seconds": total_duration,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "test_results": test_results
            }
    
    async def benchmark_workflow(self, workflow_id: Union[str, uuid.UUID], parameters: Dict[str, Any] = None,
                               iterations: int = 3) -> Dict[str, Any]:
        """
        Benchmark a workflow's performance over multiple iterations
        
        Args:
            workflow_id: ID or name of the workflow to benchmark
            parameters: Parameters to use for the workflow
            iterations: Number of iterations to run
            
        Returns:
            Benchmark report with performance statistics
        """
        if parameters is None:
            parameters = {}
            
        async with get_db_session() as session:
            # Get workflow definition
            if isinstance(workflow_id, str):
                stmt = sa.select(WorkflowModel).where(WorkflowModel.name == workflow_id)
                result = await session.execute(stmt)
                workflow_model = result.scalar_one_or_none()
                if not workflow_model:
                    raise ValueError(f"Workflow '{workflow_id}' not found")
                workflow_db_id = workflow_model.id
                workflow_name = workflow_model.name
            else:
                workflow_db_id = workflow_id
                workflow_model = await session.get(WorkflowModel, workflow_db_id)
                if not workflow_model:
                    raise ValueError(f"Workflow with ID {workflow_id} not found")
                workflow_name = workflow_model.name
                
            # Start benchmark
            start_time = time.time()
            run_results = []
            
            for i in range(iterations):
                iteration_name = f"Iteration {i+1}/{iterations}"
                logger.info(f"Running {iteration_name} for workflow {workflow_name}")
                
                # Create a results entry
                run_result = {
                    "iteration": i + 1,
                    "status": "PENDING",
                    "run_id": None,
                    "duration_seconds": None,
                    "step_durations": {},
                    "memory_usage_mb": None,
                    "error": None
                }
                
                try:
                    # Create workflow run with modified parameters to prevent caching
                    benchmark_params = parameters.copy() if parameters else {}
                    benchmark_params["_benchmark_iteration"] = i
                    
                    run_id = await self.create_workflow_run(
                        workflow_db_id,
                        parameters=benchmark_params,
                        triggered_by="workflow_benchmark",
                        custom_metadata={"benchmark_run": True, "iteration": i + 1}
                    )
                    
                    run_result["run_id"] = str(run_id)
                    
                    # Execute workflow with caching disabled
                    run_start = time.time()
                    context = await self.execute_workflow(run_id, use_cache=False)
                    run_duration = time.time() - run_start
                    
                    # Record results
                    run_result["status"] = context.status
                    run_result["duration_seconds"] = run_duration
                    
                    # Record step durations
                    for step_id, step_time in context.step_times.items():
                        if "duration" in step_time:
                            run_result["step_durations"][step_id] = step_time["duration"]
                    
                    # Record memory usage
                    if context.resource_usage.get("memory_mb"):
                        run_result["memory_usage_mb"] = max(context.resource_usage["memory_mb"])
                        
                    # Record error if failed
                    if context.status != StepExecutionStatus.COMPLETED.name:
                        run_result["error"] = context.error
                        
                except Exception as e:
                    run_result["status"] = "ERROR"
                    run_result["error"] = str(e)
                    logger.error(f"Error in benchmark iteration {i+1} for workflow {workflow_name}: {str(e)}", exc_info=True)
                
                run_results.append(run_result)
                
                # Small delay between iterations
                await asyncio.sleep(0.5)
            
            # Calculate statistics
            successful_runs = [r for r in run_results if r["status"] == StepExecutionStatus.COMPLETED.name]
            durations = [r["duration_seconds"] for r in successful_runs if r["duration_seconds"] is not None]
            
            if durations:
                avg_duration = sum(durations) / len(durations)
                min_duration = min(durations)
                max_duration = max(durations)
                std_deviation = (sum((d - avg_duration) ** 2 for d in durations) / len(durations)) ** 0.5
            else:
                avg_duration = min_duration = max_duration = std_deviation = None
                
            # Step performance analysis
            step_performance = {}
            for run in successful_runs:
                for step_id, duration in run["step_durations"].items():
                    if step_id not in step_performance:
                        step_performance[step_id] = []
                    step_performance[step_id].append(duration)
            
            step_stats = {}
            for step_id, durations in step_performance.items():
                if durations:
                    step_stats[step_id] = {
                        "avg_duration": sum(durations) / len(durations),
                        "min_duration": min(durations),
                        "max_duration": max(durations),
                        "std_deviation": (sum((d - sum(durations)/len(durations)) ** 2 for d in durations) / len(durations)) ** 0.5,
                        "percent_of_total": (sum(durations) / len(durations) / avg_duration) * 100 if avg_duration else None
                    }
            
            # Generate benchmark report
            return {
                "workflow_id": str(workflow_db_id),
                "workflow_name": workflow_name,
                "iterations": iterations,
                "successful_iterations": len(successful_runs),
                "total_duration_seconds": time.time() - start_time,
                "performance_stats": {
                    "avg_duration_seconds": avg_duration,
                    "min_duration_seconds": min_duration,
                    "max_duration_seconds": max_duration,
                    "std_deviation_seconds": std_deviation,
                    "consistency_percent": 100 - ((std_deviation / avg_duration) * 100) if avg_duration and std_deviation else None
                },
                "step_performance": step_stats,
                "timestamp": datetime.now(timezone.utc).isoformat(),
                "detailed_results": run_results
            }
            
    async def validate_workflow_integrity(self, workflow_id: Union[str, uuid.UUID]) -> Dict[str, Any]:
        """
        Perform a comprehensive integrity check on a workflow definition
        
        Args:
            workflow_id: ID or name of the workflow to validate
            
        Returns:
            Validation report with any issues found
        """
        async with get_db_session() as session:
            # Get workflow definition
            if isinstance(workflow_id, str):
                stmt = sa.select(WorkflowModel).where(WorkflowModel.name == workflow_id)
                result = await session.execute(stmt)
                workflow_model = result.scalar_one_or_none()
                if not workflow_model:
                    raise ValueError(f"Workflow '{workflow_id}' not found")
                workflow_db_id = workflow_model.id
                workflow_name = workflow_model.name
            else:
                workflow_db_id = workflow_id
                workflow_model = await session.get(WorkflowModel, workflow_db_id)
                if not workflow_model:
                    raise ValueError(f"Workflow with ID {workflow_id} not found")
                workflow_name = workflow_model.name
                
            # Get step definitions
            step_definitions = await self.get_step_definition_models(workflow_db_id, session)
            
            # Convert to dataclasses for validation
            steps_dataclasses = [
                WorkflowStepDefinition(
                    id=step.step_id_in_workflow,
                    name=step.name,
                    description=step.description or "",
                    function_identifier=step.function_identifier or "",
                    parameters=step.default_parameters or {},
                    dependencies=step.dependencies or [],
                    timeout_seconds=step.timeout_seconds,
                    retry_count=step.retry_count or 0,
                    priority=step.priority or 0,
                    estimated_memory_mb=step.estimated_memory_mb or 1000,
                    use_gpu=step.use_gpu or False,
                )
                for step in step_definitions
            ]
            
            # Initialize validation report
            validation_report = {
                "workflow_id": str(workflow_db_id),
                "workflow_name": workflow_name,
                "step_count": len(steps_dataclasses),
                "is_valid": True,
                "issues": [],
                "warnings": [],
                "advice": []
            }
            
            # Check for duplicate step IDs
            step_ids = [step.id for step in steps_dataclasses]
            duplicate_ids = set([id for id in step_ids if step_ids.count(id) > 1])
            if duplicate_ids:
                validation_report["is_valid"] = False
                validation_report["issues"].append({
                    "type": "duplicate_step_ids",
                    "description": f"Duplicate step IDs found: {', '.join(duplicate_ids)}",
                    "severity": "high"
                })
            
            # Check for missing function identifiers
            for step in steps_dataclasses:
                if not step.function_identifier:
                    validation_report["is_valid"] = False
                    validation_report["issues"].append({
                        "type": "missing_function_identifier",
                        "description": f"Step '{step.id}' has no function identifier",
                        "severity": "high"
                    })
            
            # Try to build graph to check for cycles and missing dependencies
            try:
                graph = self._build_workflow_graph(steps_dataclasses)
                
                # Check for dangling steps (no incoming or outgoing edges)
                for node in graph.nodes:
                    if not list(graph.predecessors(node)) and not list(graph.successors(node)):
                        validation_report["warnings"].append({
                            "type": "isolated_step",
                            "description": f"Step '{node}' is isolated (no dependencies or dependents)",
                            "severity": "medium"
                        })
                
                # Check for long paths
                longest_path = nx.dag_longest_path(graph)
                if len(longest_path) > 10:
                    validation_report["warnings"].append({
                        "type": "long_critical_path",
                        "description": f"Critical path is very long ({len(longest_path)} steps). Consider restructuring for better parallelism.",
                        "severity": "low",
                        "path": longest_path
                    })
                    
                # Identify potential parallelization opportunities
                parallelizable_groups = []
                current_level = set()
                visited = set()
                
                # Group steps by levels (steps that can run in parallel)
                while len(visited) < len(graph.nodes):
                    current_level = {node for node in graph.nodes 
                                  if node not in visited and 
                                  all(pred in visited for pred in graph.predecessors(node))}
                    
                    if not current_level:
                        break
                        
                    visited.update(current_level)
                    if len(current_level) > 1:
                        parallelizable_groups.append(list(current_level))
                
                if not parallelizable_groups:
                    validation_report["warnings"].append({
                        "type": "no_parallelism",
                        "description": "Workflow is completely sequential. Consider restructuring for better parallelism.",
                        "severity": "medium"
                    })
                
            except ValueError as e:
                validation_report["is_valid"] = False
                validation_report["issues"].append({
                    "type": "graph_construction_error",
                    "description": f"Error building workflow graph: {str(e)}",
                    "severity": "high"
                })
            
            # Validate function resolution
            if self._function_resolver:
                for step in steps_dataclasses:
                    try:
                        func = self._resolve_function(step.function_identifier)
                    except Exception as e:
                        validation_report["is_valid"] = False
                        validation_report["issues"].append({
                            "type": "function_resolution_error",
                            "description": f"Cannot resolve function '{step.function_identifier}' for step '{step.id}': {str(e)}",
                            "severity": "high"
                        })
            else:
                validation_report["warnings"].append({
                    "type": "no_function_resolver",
                    "description": "Function resolver not set. Cannot validate function identifiers.",
                    "severity": "medium"
                })
            
            # Check for GPU usage imbalance
            gpu_steps = [step for step in steps_dataclasses if step.use_gpu]
            if gpu_steps and self.gpu_devices:
                if len(gpu_steps) > len(self.gpu_devices) * 2:
                    validation_report["warnings"].append({
                        "type": "gpu_oversubscription",
                        "description": f"Workflow has {len(gpu_steps)} GPU steps but only {len(self.gpu_devices)} GPUs available. Consider distributing GPU tasks better.",
                        "severity": "medium"
                    })
            
            # Generate optimization recommendations
            if getattr(self, 'use_optimization', False) and validation_report["is_valid"]:
                try:
                    optimization_report = await self.optimize_workflow_execution_plan(workflow_db_id)
                    if optimization_report["recommendations"]:
                        for rec in optimization_report["recommendations"]:
                            validation_report["advice"].append({
                                "type": "optimization",
                                "description": rec.get("recommendation", "Consider optimization opportunities"),
                                "details": rec
                            })
                except Exception as e:
                    validation_report["warnings"].append({
                        "type": "optimization_error",
                        "description": f"Could not generate optimization recommendations: {str(e)}",
                        "severity": "low"
                    })
            
            return validation_report

    async def export_workflow_definition(self, workflow_id: Union[str, uuid.UUID], 
                                      format: str = "json") -> Union[str, Dict[str, Any]]:
        """
        Export a workflow definition in various formats for portability
        
        Args:
            workflow_id: ID or name of the workflow to export
            format: Export format ("json", "yaml", "python")
            
        Returns:
            Exported workflow definition in the requested format
        """
        async with get_db_session() as session:
            # Get workflow definition
            if isinstance(workflow_id, str):
                stmt = sa.select(WorkflowModel).where(WorkflowModel.name == workflow_id)
                result = await session.execute(stmt)
                workflow_model = result.scalar_one_or_none()
                if not workflow_model:
                    raise ValueError(f"Workflow '{workflow_id}' not found")
            else:
                workflow_model = await session.get(WorkflowModel, workflow_id)
                if not workflow_model:
                    raise ValueError(f"Workflow with ID {workflow_id} not found")
                
            # Get step definitions
            step_definitions = await self.get_step_definition_models(workflow_model.id, session)
            
            # Create portable definition
            portable_definition = {
                "name": workflow_model.name,
                "version": workflow_model.version,
                "description": workflow_model.description,
                "default_parameters": workflow_model.default_parameters,
                "tags": workflow_model.tags,
                "steps": []
            }
            
            for step in step_definitions:
                portable_step = {
                    "id": step.step_id_in_workflow,
                    "name": step.name,
                    "description": step.description,
                    "function_identifier": step.function_identifier,
                    "parameters": step.default_parameters,
                    "dependencies": step.dependencies,
                    "timeout_seconds": step.timeout_seconds,
                    "retry_count": step.retry_count,
                    "priority": step.priority,
                    "estimated_memory_mb": step.estimated_memory_mb,
                    "use_gpu": step.use_gpu
                }
                portable_definition["steps"].append(portable_step)
            
            # Generate output in requested format
            if format.lower() == "json":
                import json
                return json.dumps(portable_definition, indent=2)
            
            elif format.lower() == "yaml":
                try:
                    import yaml
                    return yaml.dump(portable_definition, sort_keys=False)
                except ImportError:
                    # Fall back to JSON if YAML not available
                    import json
                    logger.warning("YAML module not available. Falling back to JSON format.")
                    return json.dumps(portable_definition, indent=2)
            
            elif format.lower() == "python":
                # Generate Python code that can recreate this workflow
                steps_code = []
                for step in portable_definition["steps"]:
                    # Convert Python dict to formatted string representation
                    param_str = "{\n"
                    for k, v in (step["parameters"] or {}).items():
                        param_str += f"        '{k}': {repr(v)},\n"
                    param_str += "    }"
                    
                    dep_str = repr(step["dependencies"]) if step["dependencies"] else "[]"
                    
                    step_code = f"""    WorkflowStepDefinition(
        id="{step["id"]}",
        name="{step["name"]}",
        description="{step["description"] or ""}",
        function_identifier="{step["function_identifier"]}",
        parameters={param_str},
        dependencies={dep_str},
        timeout_seconds={step["timeout_seconds"] or "None"},
        retry_count={step["retry_count"] or 0},
        priority={step["priority"] or 0},
        estimated_memory_mb={step["estimated_memory_mb"] or 1000},
        use_gpu={step["use_gpu"] or False}
    )"""
                    steps_code.append(step_code)
                
                header = "# Generated workflow definition for " + portable_definition["name"] + " v" + portable_definition["version"] 
                imports = """
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from workflow_manager import WorkflowManager, WorkflowStepDefinition, ExecutionMode

# Initialize the workflow manager
manager = WorkflowManager(
    execution_mode=ExecutionMode.PARALLEL,
    max_workers=None,  # Use default (CPU count)
    workflow_dir="./workflow_artifacts"
)

# Define workflow steps
steps = [
"""
                steps_definition = ",\n".join(steps_code)

                register_workflow = """
]

# Register the workflow
async def register_workflow():
    workflow_id = await manager.register_workflow_definition(
        name=\"""" + portable_definition["name"] + """\",
        steps_definitions=steps,
        version=\"""" + portable_definition["version"] + """\",
        description=\"""" + (portable_definition["description"] or "") + """\",
        default_parameters=""" + repr(portable_definition["default_parameters"] or {}) + """,
        tags=""" + repr(portable_definition["tags"] or []) + """,
        enabled=True
    )
    print(f"Registered workflow with ID: {workflow_id}")
    return workflow_id
"""

                run_workflow = """
# To run this workflow, use:
# async def run_workflow():
#     run_id = await manager.create_workflow_run(
#         workflow_identifier=\"""" + portable_definition["name"] + """\",
#         parameters={},  # Add your parameters here
#         version=\"""" + portable_definition["version"] + """\"
#     )
#     context = await manager.execute_workflow(run_id)
#     print(f"Workflow completed with status: {context.status}")
#     return context
"""
                python_code = header + imports + steps_definition + register_workflow + run_workflow
                return python_code
            
            else:
                raise ValueError(f"Unsupported export format: {format}. Supported formats: json, yaml, python")
    
    async def import_workflow_definition(self, definition: Union[str, Dict[str, Any]], 
                                     format: str = "json") -> uuid.UUID:
        """
        Import a workflow definition from various formats
        
        Args:
            definition: Workflow definition in string or dict format
            format: Format of the definition ("json", "yaml", "python" - not supported for direct import)
            
        Returns:
            UUID of the imported workflow
        """
        # Parse definition if needed
        if isinstance(definition, str):
            if format.lower() == "json":
                import json
                try:
                    definition = json.loads(definition)
                except json.JSONDecodeError as e:
                    raise ValueError(f"Invalid JSON format: {str(e)}")
            
            elif format.lower() == "yaml":
                try:
                    import yaml
                    definition = yaml.safe_load(definition)
                except ImportError:
                    raise ValueError("YAML module not available. Install PyYAML to import YAML definitions.")
                except yaml.YAMLError as e:
                    raise ValueError(f"Invalid YAML format: {str(e)}")
            
            elif format.lower() == "python":
                raise ValueError("Python format cannot be directly imported. Execute the Python code instead.")
            
            else:
                raise ValueError(f"Unsupported import format: {format}. Supported formats: json, yaml")
        
        # Validate definition structure
        required_fields = ["name", "version", "steps"]
        for field in required_fields:
            if field not in definition:
                raise ValueError(f"Missing required field: {field}")
        
        # Convert to WorkflowStepDefinition instances
        steps = []
        for step_def in definition["steps"]:
            required_step_fields = ["id", "name", "function_identifier"]
            for field in required_step_fields:
                if field not in step_def:
                    raise ValueError(f"Step definition missing required field: {field}")
            
            steps.append(WorkflowStepDefinition(
                id=step_def["id"],
                name=step_def["name"],
                description=step_def.get("description", ""),
                function_identifier=step_def["function_identifier"],
                parameters=step_def.get("parameters", {}),
                dependencies=step_def.get("dependencies", []),
                timeout_seconds=step_def.get("timeout_seconds"),
                retry_count=step_def.get("retry_count", 0),
                retry_delay_seconds=step_def.get("retry_delay_seconds", 60),
                priority=step_def.get("priority", 0),
                estimated_memory_mb=step_def.get("estimated_memory_mb", 1000),
                use_gpu=step_def.get("use_gpu", False),
                use_process=step_def.get("use_process", False)
            ))
        
        # Register workflow
        workflow_id = await self.register_workflow_definition(
            name=definition["name"],
            steps_definitions=steps,
            version=definition["version"],
            description=definition.get("description"),
            default_parameters=definition.get("default_parameters"),
            tags=definition.get("tags"),
            owner_id=definition.get("owner_id"),
            enabled=definition.get("enabled", True)
        )
        
        logger.info(f"Imported workflow '{definition['name']}' v{definition['version']} with ID: {workflow_id}")
        return workflow_id
    
    # Final optimizations and utilities
    
    async def optimize_automatic_retries(self, run_db_id: uuid.UUID) -> bool:
        """
        Analyze step failure patterns and automatically adjust retry settings
        for optimal reliability
        
        Args:
            run_db_id: ID of the failed workflow run to analyze
            
        Returns:
            True if optimizations were applied successfully
        """
        async with get_db_session() as session:
            # Get run info
            run_model = await session.get(WorkflowRunModel, run_db_id)
            if not run_model:
                raise ValueError(f"Workflow run with ID {run_db_id} not found")
                
            if run_model.status != StatusEnum.FAILED:
                logger.info(f"Workflow run {run_db_id} didn't fail. No retry optimization needed.")
                return False
                
            # Get workflow definition
            workflow_model = await session.get(WorkflowModel, run_model.workflow_id)
            if not workflow_model:
                raise ValueError(f"Workflow definition {run_model.workflow_id} not found")
                
            # Get failed steps
            steps_stmt = sa.select(WorkflowStepRunModel).where(
                WorkflowStepRunModel.workflow_run_id == run_db_id,
                WorkflowStepRunModel.status == StatusEnum.FAILED
            )
            steps_result = await session.execute(steps_stmt)
            failed_steps = steps_result.scalars().all()
            
            if not failed_steps:
                logger.info(f"No failed steps found for run {run_db_id}")
                return False
                
            # Get step definitions
            step_definitions = await self.get_step_definition_models(workflow_model.id, session)
            
            # Map step IDs to definitions
            step_def_map = {step.step_id_in_workflow: step for step in step_definitions}
            
            # Analyze failure patterns and update retry settings
            optimizations_applied = False
            for failed_step in failed_steps:
                step_def = step_def_map.get(failed_step.step_id_in_workflow)
                if not step_def:
                    continue
                    
                # Check error message for patterns that suggest retrying might help
                should_increase_retries = False
                error_msg = failed_step.error_message or ""
                
                # Network/connection errors typically benefit from retries
                if any(pattern in error_msg.lower() for pattern in 
                      ["timeout", "connection", "network", "temporarily unavailable", 
                       "resource", "memory", "throttl", "overload", "busy", "unavailable"]):
                    should_increase_retries = True
                
                # If retrying would help, update the step definition
                if should_increase_retries:
                    # Check current retry count and increase it
                    current_retries = step_def.retry_count or 0
                    if current_retries < 5:  # Cap at reasonable number
                        new_retry_count = current_retries + 1
                        
                        # Update step definition
                        step_def.retry_count = new_retry_count
                        
                        # Calculate a good retry delay based on the failure type
                        if "throttl" in error_msg.lower() or "rate limit" in error_msg.lower():
                            # Longer delays for rate limiting issues
                            step_def.retry_delay_seconds = 120
                        elif "timeout" in error_msg.lower():
                            # Medium delays for timeouts
                            step_def.retry_delay_seconds = 60
                        else:
                            # Standard exponential backoff for other issues
                            step_def.retry_delay_seconds = 30
                            
                        logger.info(
                            f"Optimizing retries for step '{failed_step.step_id_in_workflow}': "
                            f"count={new_retry_count}, delay={step_def.retry_delay_seconds}s"
                        )
                        optimizations_applied = True
            
            if optimizations_applied:
                # Update the workflow definition with new retry settings
                await session.commit()
                logger.info(f"Applied automatic retry optimizations for workflow '{workflow_model.name}'")
                return True
            else:
                logger.info(f"No retry optimizations applied for workflow '{workflow_model.name}'")
                return False

    async def _are_resources_available_for_step(self, step_dataclass: WorkflowStepDefinition) -> bool:
        """
        Check if sufficient system and executor resources are available for a step.
        """
        # Check system-wide CPU
        current_cpu_percent = psutil.cpu_percent(interval=None) # Non-blocking
        if current_cpu_percent > self.system_cpu_threshold_percent:
            logger.debug(f"Resource check: System CPU usage {current_cpu_percent}% exceeds threshold {self.system_cpu_threshold_percent}%.")
            return False

        # Check system-wide Memory
        current_memory_info = psutil.virtual_memory()
        if current_memory_info.percent > self.system_memory_threshold_percent:
            logger.debug(f"Resource check: System memory usage {current_memory_info.percent}% exceeds threshold {self.system_memory_threshold_percent}%.")
            return False
        
        # Check executor capacity (simple count of active tasks)
        # This is an approximation. _active_executor_tasks needs to be maintained.
        active_thread_tasks = 0
        active_process_tasks = 0
        
        # This part needs careful implementation of how _active_executor_tasks is managed
        # For now, let's assume a simplified check if executors are "full"
        # A more robust way is to check self.thread_executor._work_queue.qsize() if available and safe,
        # or track active futures explicitly.
        # For now, let's assume a simpler check against max_workers.

        # Count active tasks by checking futures associated with executors
        # This requires _active_executor_tasks to be populated when tasks are submitted
        # and cleaned up when they complete. This is done in _execute_parallel.
        
        # Simplified check: if total active tasks >= max_workers (could be refined per executor type)
        if len(self._active_executor_tasks) >= self.max_workers:
             logger.debug(f"Resource check: Executor capacity reached ({len(self._active_executor_tasks)} active / {self.max_workers} max).")
             return False


        # Check GPU resources if needed
        if step_dataclass.use_gpu:
            if not self.gpu_devices:
                logger.warning(f"Step {step_dataclass.id} requires GPU, but no GPU devices configured in manager.")
                return False # Cannot run GPU task if no GPUs configured
            
            # Attempt to find an available GPU (this is a simplified check, allocate_gpu is more complex)
            # This check should be quick and not actually allocate.
            # A better way would be for allocate_gpu to return a status or for a separate query method.
            has_available_gpu = False
            for gpu_id in self.gpu_devices:
                if self.gpu_allocation.get(gpu_id) is None: # GPU is free
                    # Check if it has enough memory (if self.available_gpu_memory is up-to-date)
                    if step_dataclass.estimated_memory_mb <= self.available_gpu_memory.get(gpu_id, 0):
                        has_available_gpu = True
                        break
            if not has_available_gpu:
                logger.debug(f"Resource check: Step {step_dataclass.id} requires GPU, but no suitable GPU available or not enough memory.")
                return False
        
        logger.debug(f"Resource check: Resources deemed available for step {step_dataclass.id}.")
        return True

    async def _process_resource_limited_queues(self):
        """
        Periodically checks queues of steps waiting for resources and submits them
        if resources become available.
        """
        logger.info("Starting resource-limited queue processor.")
        try:
            while True:
                await asyncio.sleep(self.resource_check_interval_seconds)
                logger.debug("_process_resource_limited_queues: Checking queues...")

                processed_in_cycle = 0
                # Iterate over a copy of items if modification occurs during iteration (though PriorityQueue.get should be safe)
                for run_id, queue in list(self._resource_limited_queues.items()):
                    if queue.empty():
                        if run_id in self._resource_limited_queues: # Cleanup empty queues
                             del self._resource_limited_queues[run_id]
                        continue

                    # Try to process one item from each non-empty queue to ensure fairness across runs
                    try:
                        # PriorityQueue stores (priority, timestamp, step_id, step_data...), so item[0] is priority.
                        # We need all parts of the stored tuple.
                        priority, timestamp, step_id_to_process, step_dataclass_to_process, \
                        context_repr_to_process, future_to_process, loop_to_process = queue.get_nowait()
                        
                        logger.debug(f"Checking resource availability for queued step {step_id_to_process} (Run: {run_id}, Prio: {priority})")

                        if await self._are_resources_available_for_step(step_dataclass_to_process):
                            logger.info(f"Resources available for queued step {step_id_to_process} (Run: {run_id}). Submitting.")
                            
                            chosen_executor = self.process_executor if step_dataclass_to_process.use_process else self.thread_executor
                            if chosen_executor:
                                chosen_executor.submit(
                                    self._run_step_task_for_executor,
                                    run_id, # This is run_db_id
                                    step_dataclass_to_process,
                                    context_repr_to_process,
                                    future_to_process,
                                    loop_to_process
                                )
                                # Add to active executor tasks for resource tracking
                                self._active_executor_tasks[future_to_process] = (run_id, step_id_to_process)
                                processed_in_cycle +=1
                            else:
                                # Should not happen if executors are initialized
                                logger.error(f"Executor not available for step {step_id_to_process} from resource queue. Re-queuing.")
                                queue.put_nowait((priority, timestamp, step_id_to_process, step_dataclass_to_process, 
                                                  context_repr_to_process, future_to_process, loop_to_process))

                        else:
                            logger.debug(f"Resources still unavailable for step {step_id_to_process} (Run: {run_id}). Re-queuing.")
                            # Put it back in the queue if resources are not yet available
                            queue.put_nowait((priority, timestamp, step_id_to_process, step_dataclass_to_process, 
                                              context_repr_to_process, future_to_process, loop_to_process))
                    
                    except asyncio.QueueEmpty:
                        # Queue became empty during processing for this run_id, which is fine.
                        if run_id in self._resource_limited_queues and self._resource_limited_queues[run_id].empty():
                             del self._resource_limited_queues[run_id]
                        continue
                    except Exception as e:
                        logger.error(f"Error processing resource-limited queue for run {run_id}: {e}", exc_info=True)
                
                if processed_in_cycle > 0:
                    logger.info(f"Processed {processed_in_cycle} tasks from resource-limited queues in this cycle.")

        except asyncio.CancelledError:
            logger.info("Resource-limited queue processor task cancelled.")
        except Exception as e:
            logger.critical(f"Resource-limited queue processor task failed critically: {e}", exc_info=True)
            # Consider if this task should attempt to restart or if the manager should enter a degraded state.
            # For now, it will terminate.

    def _compute_critical_path(self, graph: nx.DiGraph, step_metrics: Dict[str, Dict[str, Any]]) -> List[str]:
        """
        Compute the critical path of a workflow

        Args:
            graph: Workflow graph
            step_metrics: Dictionary of step metrics

        Returns:
            List of step IDs forming the critical path
        """
        # Create a new graph with edge weights based on step durations
        G = nx.DiGraph()

        for node in graph.nodes():
            G.add_node(node)

        for u, v in graph.edges():
            # Use step execution time as weight, default to 0 if not available
            weight = step_metrics.get(v, {}).get("execution_time", 0) or 0
            G.add_edge(u, v, weight=weight)

        # Find the critical path (longest path through the DAG)
        # First, topologically sort the graph
        try:
            topo_sort = list(nx.topological_sort(G))

            # Calculate longest path to each node
            dist = {node: 0 for node in G.nodes()}
            prev = {node: None for node in G.nodes()}

            for node in topo_sort:
                for successor in G.successors(node):
                    weight = G.edges[node, successor]["weight"]
                    if dist[node] + weight > dist[successor]:
                        dist[successor] = dist[node] + weight
                        prev[successor] = node

            # Find the node with the maximum distance
            max_dist_node = max(topo_sort, key=lambda node: dist[node])

            # Reconstruct the path
            path = []
            current = max_dist_node
            while current is not None:
                path.append(current)
                current = prev[current]

            return list(reversed(path))

        except nx.NetworkXUnfeasible:
            # Graph has cycles
            return []
